"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[5680],{67680:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var a=n(35721),i=n(74848),s=n(28453);const r={layout:"posts",title:"Follow up REST API performance",date:new Date("2025-07-02T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},o=void 0,h={authorsImageUrls:[void 0]},c=[{value:"Investigating REST API performance",id:"investigating-rest-api-performance",level:2},{value:"Day 1: Investigation REST API Performance",id:"day-1-investigation-rest-api-performance",level:2},{value:"REST API Metrics",id:"rest-api-metrics",level:3},{value:"Investigating Worker errors",id:"investigating-worker-errors",level:3},{value:"Profiling the System",id:"profiling-the-system",level:3},{value:"Path pattern matching",id:"path-pattern-matching",level:4},{value:"Gateway - Broker request latency",id:"gateway---broker-request-latency",level:4},{value:"Request handling execution logic",id:"request-handling-execution-logic",level:5},{value:"Day 2: Profiling and Experimenting",id:"day-2-profiling-and-experimenting",level:2},{value:"Virtual threads",id:"virtual-threads",level:3},{value:"Direct handling",id:"direct-handling",level:3},{value:"Spring PathPattern parser for MVC",id:"spring-pathpattern-parser-for-mvc",level:3},{value:"Combination of direct handle and PathPattern",id:"combination-of-direct-handle-and-pathpattern",level:3},{value:"Day 3: Observing load tests and further experimenting",id:"day-3-observing-load-tests-and-further-experimenting",level:2}];function l(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"investigating-rest-api-performance",children:"Investigating REST API performance"}),"\n",(0,i.jsx)(t.p,{children:"At this point in time, we don't have one root cause identified. As it is often the case with such performance issues, it is the combination of several things."}),"\n",(0,i.jsx)(t.p,{children:"What we have done and validated so far:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Investigated existing REST api metrics + breakdown metrics to have a better overview of where time is spent"}),"\n",(0,i.jsx)(t.li,{children:"Investigated worker failing with OOM - due to performance issue and using a queue to store the response futures"}),"\n",(0,i.jsx)(t.li,{children:"Take JFR recordings and profile the system"}),"\n",(0,i.jsx)(t.li,{children:"Increase CPU resources to understand whether it is resource contention - it is."}),"\n",(0,i.jsxs)(t.li,{children:["Improve Spring request filtering and execution","\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Use virtual threads for Spring"}),"\n",(0,i.jsx)(t.li,{children:"Use PathPattern instead of legacy AntPathPattern"}),"\n",(0,i.jsx)(t.li,{children:"Use direct response handling instead of asynchronous"}),"\n",(0,i.jsx)(t.li,{children:"Combine some of them"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.li,{children:"Observe, profile, and investigate performance further - with different commits from main"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"From what we can observe is that some load tests can run stable for quite a while, until they break down. It seems to be often related to restarts/rescheduling. They reach a point where the CPU throttling increases, and at this point the performance breaks down."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"all-namespaces-throughput",src:n(42733).A+"",width:"2561",height:"860"})}),"\n",(0,i.jsx)(t.h2,{id:"day-1-investigation-rest-api-performance",children:"Day 1: Investigation REST API Performance"}),"\n",(0,i.jsx)(t.p,{children:"This blog post aims to summarize the investigation of the REST API performance, and give some hints and collections of what to improve."}),"\n",(0,i.jsx)(t.h3,{id:"rest-api-metrics",children:"REST API Metrics"}),"\n",(0,i.jsx)(t.p,{children:"One remark from the last experiments was that we do not have good insights for the REST API. Actually, we have the necessary metrics already exposed, but not yet available in our Dashboard."}),"\n",(0,i.jsxs)(t.p,{children:["This is currently prepared with ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/pull/33907",children:"#33907"}),". Based on this, I was able to further investigate the REST API performance."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"rest-api",src:n(14043).A+"",width:"2539",height:"607"})}),"\n",(0,i.jsx)(t.p,{children:"What we can see is that our requests take on average more than 50ms to complete. This is causing our throughput to go down, we are not able to create 150 PI/s even."}),"\n",(0,i.jsx)(t.p,{children:"Looking at a different Benchmark using gRPC, we can see that requests take 5-10ms to complete, and have a stable throughput"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.img,{alt:"grpc-latency",src:n(84127).A+"",width:"2526",height:"333"}),"\n",(0,i.jsx)(t.img,{alt:"grpc",src:n(12110).A+"",width:"1273",height:"486"})]}),"\n",(0,i.jsx)(t.p,{children:"Due to the slower workers (on completion), we can see error reports of the workers not being able to accept further job pushes. This has been mentioned in the previous blog post as well.  This, in consequence, means the worker sends FAIL commands for such jobs, to give them back. It has a cascading effect, as jobs are sent back and forth and impacting the general process instance execution latency (which grows up to 60s compared to 0.2 s)."}),"\n",(0,i.jsx)(t.h3,{id:"investigating-worker-errors",children:"Investigating Worker errors"}),"\n",(0,i.jsx)(t.p,{children:"In our previous experiments, we have seen the following exceptions"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"13:25:14.684 [pool-4-thread-3] WARN  io.camunda.client.job.worker - Worker benchmark failed to handle job with key 4503599628992806 of type benchmark-task, sending fail command to broker\njava.lang.IllegalStateException: Queue full\n\tat java.base/java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:?]\n\tat java.base/java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:329) ~[?:?]\n\tat io.camunda.zeebe.Worker.lambda$handleJob$1(Worker.java:122) ~[classes/:?]\n\tat io.camunda.client.impl.worker.JobRunnableFactoryImpl.executeJob(JobRunnableFactoryImpl.java:45) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\n\tat io.camunda.client.impl.worker.JobRunnableFactoryImpl.lambda$create$0(JobRunnableFactoryImpl.java:40) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\n\tat io.camunda.client.impl.worker.BlockingExecutor.lambda$execute$0(BlockingExecutor.java:50) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]\n\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?\n"})}),"\n",(0,i.jsxs)(t.p,{children:["This is actually coming from the Worker (benchmark) application, as it is collecting all ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/blob/main/zeebe/benchmarks/project/src/main/java/io/camunda/zeebe/Worker.java#L54",children:"the request futures in a blocking queue"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"As the performance is lower of handling requests, we collect more futures in the worker, causing to fill the queue. This in the end causes also to fail more jobs - causing even more work."}),"\n",(0,i.jsx)(t.p,{children:"This allows explains why our workers have a higher memory consumption - we had to increase the worker memory to have a stable worker."}),"\n",(0,i.jsx)(t.h3,{id:"profiling-the-system",children:"Profiling the System"}),"\n",(0,i.jsxs)(t.p,{children:["With the previous results, we were encouraged to do some profiling. For the start we used ",(0,i.jsx)(t.a,{href:"https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170",children:"JFR"})," for some basic profiling."]}),"\n",(0,i.jsx)(t.p,{children:"You can do this by:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-shell",children:'  kubectl exec -it "$1" -- jcmd 1 JFR.start duration=100s filename=/usr/local/camunda/data/flight-$(date +%d%m%y-%H%M).jfr\n'})}),"\n",(0,i.jsxs)(t.p,{children:["If the flight recording is done, you can copy the recording (via ",(0,i.jsx)(t.code,{children:"kubectl cp"}),") and open it with Intellij (JMC didn't work for me)"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"first-profile",src:n(64402).A+"",width:"2012",height:"626"})}),"\n",(0,i.jsx)(t.p,{children:"We see that the Spring filter chaining is dominating the profile, which is not unexpected as every request has gone through this chain. As this is a CPU based sampling profile, it is likely to be part of the profile. Still, it was something interesting to note and investigate."}),"\n",(0,i.jsx)(t.h4,{id:"path-pattern-matching",children:"Path pattern matching"}),"\n",(0,i.jsxs)(t.p,{children:["Some research showed that it might be interesting to look into other path pattern matchers, as we use the (legacy) ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/blob/main/dist/src/main/resources/application.properties#L17",children:"ant path matcher"})," with ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/blob/main/authentication/src/main/java/io/camunda/authentication/config/WebSecurityConfig.java#L86",children:"regex"}),"."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Resources:"})}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["PathPattern - ",(0,i.jsx)(t.a,{href:"https://spring.io/blog/2020/06/30/url-matching-with-pathpattern-in-spring-mvc#pathpattern",children:"https://spring.io/blog/2020/06/30/url-matching-with-pathpattern-in-spring-mvc#pathpattern"})]}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://github.com/spring-projects/spring-framework/issues/31098#issuecomment-1891737375",children:"Results of using PathPattern and related discussion on GH"})}),"\n"]}),"\n",(0,i.jsx)(t.h4,{id:"gateway---broker-request-latency",children:"Gateway - Broker request latency"}),"\n",(0,i.jsx)(t.p,{children:"As we have such a high request-response latency, we have to find out where the time is spent. Ideally, we would have some sort of tracing (which we didn't have yet), or we would look at metrics that cover sub-parts of the system and the request-response cycle."}),"\n",(0,i.jsx)(t.p,{children:"The REST API request-response latency metric, we can take it as the complete round trip, accepting the request on the gateway edge, converting it to a Broker request, sending it to the Broker, the Broker processes, sends the response back, etc."}),"\n",(0,i.jsxs)(t.p,{children:["Luckily, we have a metric that is covering the part of sending the Broker request (from the other side of the Gateway) to the Broker and wait for the response. See related ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/blob/main/zeebe/broker-client/src/main/java/io/camunda/zeebe/broker/client/impl/BrokerRequestManager.java#L153",children:"code here"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"The difference shows us that there is not a small overhead, meaning that actually the Gateway to Broker request-response is slower with REST as well, which is unexpected."}),"\n",(0,i.jsx)(t.p,{children:"This can either be because different data is sent, or a different API is used, or some other execution mechanics, etc."}),"\n",(0,i.jsx)(t.p,{children:"Using the same cluster and enabling the REST API later, we can see the immediate effect on performance."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"rest-enabled",src:n(10224).A+"",width:"2512",height:"652"})}),"\n",(0,i.jsx)(t.h5,{id:"request-handling-execution-logic",children:"Request handling execution logic"}),"\n",(0,i.jsx)(t.p,{children:"A difference we have spotted with REST API and gRPC is the usage of the BrokerClient."}),"\n",(0,i.jsxs)(t.p,{children:["While we use on the gRPC side the ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/blob/main/zeebe/gateway-grpc/src/main/java/io/camunda/zeebe/gateway/EndpointManager.java#L457",children:"BrokerClient with retries"})," and direct response handling, on the REST API we use no retries and ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/blob/main/service/src/main/java/io/camunda/service/ApiServices.java#L55",children:"handle the response async with the ForkJoinPool"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["As our benchmark clusters have two CPUs, ",(0,i.jsx)(t.a,{href:"https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html",children:"meaning 1 Thread for the common ForkJoin thread pool"})," we expected some contention on the thread."]}),"\n",(0,i.jsxs)(t.p,{children:["For testing purposes, we increased the thread count by: ",(0,i.jsx)(t.code,{children:"-Djava.util.concurrent.ForkJoinPool.common.parallelism=8"})]}),"\n",(0,i.jsx)(t.p,{children:"In a profile we can see that more threads are used, but it doesn't change anything in the performance."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"profile-inc-fork-join",src:n(62470).A+"",width:"2537",height:"630"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"rest-gw-metrics-after-increaese-thread-pool",src:n(88334).A+"",width:"1907",height:"761"})}),"\n",(0,i.jsx)(t.p,{children:"The assumption was that we might not be able to handle the response in time with one thread, and this causes some contention also on the Gateway-Broker request-response cycle, but this is not the case."}),"\n",(0,i.jsx)(t.p,{children:"We seem to spend time somewhere else or have a general resource contention issue. What we can see is that we have to work with more CPU throttling, then without REST API usage."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"rest-api-cpu-throttling.png",src:n(94104).A+"",width:"2541",height:"950"})}),"\n",(0,i.jsx)(t.p,{children:"Increasing the CPU resolves the general performance problem, hinting even more that we might have some issues with threads competing with resources, etc."}),"\n",(0,i.jsx)(t.p,{children:"In the following screenshot, you see the test with 6 CPUs per Camunda application."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"six-cpus",src:n(4078).A+"",width:"1896",height:"933"})}),"\n",(0,i.jsx)(t.p,{children:"Compared to the previous run with 2 CPUs per Camunda application, where it had to fight with a lot of CPU throttling. The request-response latency was five times higher on average."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"two-cpus",src:n(12308).A+"",width:"1901",height:"925"})}),"\n",(0,i.jsx)(t.p,{children:"We have to further investigate this based on this knowledge."}),"\n",(0,i.jsx)(t.h2,{id:"day-2-profiling-and-experimenting",children:"Day 2: Profiling and Experimenting"}),"\n",(0,i.jsx)(t.p,{children:"Yesterday I was taking profiles with 100s, to reduce the noise. Still, we can see that the filter chain is taking ~40% of the complete profile."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"jfr-10-minutes-filter-chain.png",src:n(5547).A+"",width:"2540",height:"665"})}),"\n",(0,i.jsx)(t.p,{children:"When opening the JFR recording with JMC, we get some hints, related to context switches, CPU throttling (which we already know) and the inverted parallelism of GC (also mentioning high IO)."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.img,{alt:"locks-and-contention-context-switch.png",src:n(34449).A+"",width:"640",height:"107"}),"\n",(0,i.jsx)(t.img,{alt:"jfr-cpu-throttling-detection.png",src:n(60417).A+"",width:"658",height:"193"}),"\n",(0,i.jsx)(t.img,{alt:"gc-ineffeciency-high-io.png",src:n(86839).A+"",width:"636",height:"240"})]}),"\n",(0,i.jsx)(t.p,{children:"We have already seen in our metrics, for example, that we fight with high CPU throttling"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"rest-base-cpu",src:n(22437).A+"",width:"2540",height:"308"})}),"\n",(0,i.jsx)(t.p,{children:"To better analyze (and circumvent that we have no tracing), I added some more metrics to understand where time is spent. Furthermore, I created a temporary dashboard to break down where time is spent."}),"\n",(0,i.jsx)(t.p,{children:"When we look at the base with gRPC (taking our weekly benchmarks), we can see all latencies are low, and mostly under 5 ms."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"grpc-break-down.png",src:n(29575).A+"",width:"2526",height:"644"})}),"\n",(0,i.jsx)(t.p,{children:"As soon as we enable the REST API, we can see the latencies go up. The most significant increase we see is in the job activations."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"rest-break-down",src:n(55561).A+"",width:"2533",height:"796"})}),"\n",(0,i.jsx)(t.p,{children:"Fascinating is that the write to process latency, the time from acceptance by the CommandAPI until the processor processes this command, also increases."}),"\n",(0,i.jsx)(t.h3,{id:"virtual-threads",children:"Virtual threads"}),"\n",(0,i.jsxs)(t.p,{children:["To remove some thoughts about potential IO and CPU contention, I experimented with virtual threads, which we can ",(0,i.jsx)(t.a,{href:"https://www.baeldung.com/spring-6-virtual-threads",children:"easily enable for Spring"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"I set the following system property on the statefulset."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-shell",children:"-Dspring.threads.virtual.enabled=true\n"})}),"\n",(0,i.jsx)(t.p,{children:"Taking a new profile, we can see that all the http threads are gone, but still the filtering is prominent."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"jfr-virtual-threads.png",src:n(52912).A+"",width:"2522",height:"714"})}),"\n",(0,i.jsx)(t.p,{children:"Checking our metrics break-down again we see there is no benefit here."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"virtual-threads-break-down.png",src:n(35386).A+"",width:"2532",height:"849"})}),"\n",(0,i.jsx)(t.h3,{id:"direct-handling",children:"Direct handling"}),"\n",(0,i.jsxs)(t.p,{children:["Investigating the code basis, we saw several times ",(0,i.jsx)(t.code,{children:"#handleAsync"})," without using an extra executor, causing to use of the ForkJoinPool (as mentioned the other day). One idea was to ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/commit/265d7164f5384be8c443c30b20e432582df09c24",children:"directly handle the future completions"}),", meaning the response handling, etc."]}),"\n",(0,i.jsx)(t.p,{children:"We didn't observe any benefits with this."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"direct-handling-breakdown.png",src:n(37793).A+"",width:"2530",height:"808"})}),"\n",(0,i.jsxs)(t.p,{children:["In the JFR recording, we can see that less Threads are used, but the Spring filter chain is also super prominent.\n",(0,i.jsx)(t.img,{alt:"direct-handling-v2-profile-too-much-filtering.png",src:n(97136).A+"",width:"2529",height:"692"})]}),"\n",(0,i.jsx)(t.h3,{id:"spring-pathpattern-parser-for-mvc",children:"Spring PathPattern parser for MVC"}),"\n",(0,i.jsxs)(t.p,{children:["At the end of the day I finally came to try the ",(0,i.jsx)(t.code,{children:"PathPattern"})," parser. As mentioned the other day, it is recommended to use it over the legacy ",(0,i.jsx)(t.code,{children:"AntPathMatcher"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["The migration was ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/commit/357522d8355a624a1c07e1fb889561254b0305ba",children:"rather simple"}),", we can replace the ",(0,i.jsx)(t.code,{children:"spring.mvc.pathmatch.matching-strategy=ant_path_matcher"})," with\n",(0,i.jsx)(t.code,{children:"spring.mvc.pathmatch.matching-strategy=path_pattern_parser"}),", we only had to fix some occurrences of regex combinations with ",(0,i.jsx)(t.code,{children:"**"}),", as it is only allowed to have ",(0,i.jsx)(t.code,{children:"**"})," at the end (no regex after)."]}),"\n",(0,i.jsxs)(t.p,{children:["See related branch ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/commits/ck-pattern-path-parse/",children:"ck-pattern-path-parse"}),"."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"path-pattern-breakdown",src:n(33464).A+"",width:"2512",height:"817"})}),"\n",(0,i.jsx)(t.p,{children:"We were able to reduce the latencies by half, which also allowed us to bring back our throughput."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"path-pattern-general.png",src:n(75845).A+"",width:"2528",height:"667"})}),"\n",(0,i.jsx)(t.p,{children:"I did a cross-check with the current SNAPSHOT, and weirdly the SNAPSHOT now behaved the same. I will run this for a while to see the results, as it might fail after certain period of time. As this might be also related to where the pods are scheduled (noisy neighbours etc.)"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.img,{alt:"rest-base-v2-breakdown.png",src:n(42341).A+"",width:"2543",height:"796"}),"\n",(0,i.jsx)(t.img,{alt:"rest-base-v2-general.png",src:n(28436).A+"",width:"2527",height:"679"})]}),"\n",(0,i.jsx)(t.h3,{id:"combination-of-direct-handle-and-pathpattern",children:"Combination of direct handle and PathPattern"}),"\n",(0,i.jsxs)(t.p,{children:["On top of the above, I ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/commits/ck-direct-handle/",children:"combined the direct handling and PathPattern usage"}),", and this gave us the best results."]}),"\n",(0,i.jsx)(t.p,{children:"The latencies are only two times higher than gRPC vs before 5 times (and more)."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"combination-of-all-breakdown.png",src:n(86983).A+"",width:"2502",height:"907"})}),"\n",(0,i.jsx)(t.p,{children:"The throttling of the CPU was reduced by half as well."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"combination-of-all-cpu.png",src:n(98168).A+"",width:"2538",height:"324"})}),"\n",(0,i.jsx)(t.p,{children:"This gives a great stable throughput again."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"combination-of-all-general.png",src:n(79490).A+"",width:"2531",height:"684"})}),"\n",(0,i.jsx)(t.h2,{id:"day-3-observing-load-tests-and-further-experimenting",children:"Day 3: Observing load tests and further experimenting"}),"\n",(0,i.jsx)(t.p,{children:"Yesterday, I have started several load tests for things I have tried out in code (like PathPattern or direct response handling), but also from different commits of the main branch (the current SNASPHOT, some commits that touch the rest gateway, and from begin of the week)."}),"\n",(0,i.jsx)(t.p,{children:"ALL of them were running at the beginning fine, but failed at some point. Some sooner, some later."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"all-namespaces-throughput.png",src:n(42733).A+"",width:"2561",height:"860"})}),"\n",(0,i.jsx)(t.p,{children:"Interesting was that on all JFR recordings (with and without PathPattern), I still saw the Spring filter chain take a big chunk of the proile."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.img,{alt:"rest-base-v3-jfr.png",src:n(34331).A+"",width:"1861",height:"710"}),"\n",(0,i.jsx)(t.img,{alt:"path-pattern-jfr.png",src:n(43179).A+"",width:"1886",height:"724"}),"\n",(0,i.jsx)(t.img,{alt:"combination-of-all-jfr.png",src:n(84856).A+"",width:"1863",height:"702"})]}),"\n",(0,i.jsx)(t.p,{children:"Based on the profile, we do not see much of a difference."}),"\n",(0,i.jsx)(t.p,{children:"Today, I will validate the following:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Is anti-affinity still enabled with our platform charts"}),"\n",(0,i.jsx)(t.li,{children:"Combination of virtual threads (to reduce the thread count and blocking behavior), with PathPattern (as this was the most stable test)"}),"\n",(0,i.jsx)(t.li,{children:"Maybe increasing the CPU limits again, to remove the K8 CPU throttling to better understand the system performance (until which the CPU consumption grows) and profile again"}),"\n",(0,i.jsx)(t.li,{children:"Investigate further - likely try a different profiler like asyncProfiler"}),"\n"]})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},42733:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/all-namespaces-throughput-1e62889524faaeec5cd7fb3052db4f04.png"},86983:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/combination-of-all-breakdown-3796dbecb61e2f55349b7e2b2fe9c58e.png"},98168:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/combination-of-all-cpu-76b39f387018b794f171bb676b8b3832.png"},79490:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/combination-of-all-general-285aa7b2d45b50d79f995ebffbbd697f.png"},84856:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/combination-of-all-jfr-96fe5ae899bbbab0a0739b88385e562e.png"},37793:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/direct-handling-breakdown-f5af66c79a5d1064e3d1e603651d8797.png"},97136:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/direct-handling-v2-profile-too-much-filtering-d3a5da919b5220949e925bbb9042dbb4.png"},64402:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/first-profile-6385c51bbf0d219a4a48da47d2505805.png"},86839:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/gc-ineffeciency-high-io-8e03650f088004e21e0e549bf8cf985c.png"},29575:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/grpc-break-down-5f9058a44a49e0fd5f38f87489eda3e7.png"},84127:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/grpc-latency-097288055f8e099cc22f7268d67b53a9.png"},12110:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/grpc-93fbafebfd4eebb459d95c8c713b67d7.png"},5547:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/jfr-10-minutes-filter-chain-c3a4c75586a6c3ce023bad52889c5590.png"},60417:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/jfr-cpu-throttling-detection-5cfaa70f6b7e3b06c6ac745b8d317413.png"},52912:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/jfr-virtual-threads-c97b70375d089d057d3edecdd0ad2ad5.png"},34449:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/locks-and-contention-context-switch-b3ebe8dc71f088fc16b4bf39f5defd2e.png"},33464:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/path-pattern-breakdown-9d9c15dc4c5d21d70472756d370ca533.png"},75845:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/path-pattern-general-a9e20dc4a52c74b0488488aa78fa6e62.png"},43179:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/path-pattern-jfr-b3978d0f445b946a903d52f8651efbca.png"},62470:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/profile-inc-fork-join-f567119ebd3858f2f5414425b7212228.png"},94104:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-api-cpu-throttling-f4210b87ca53b8fcab9741cf12e76a60.png"},14043:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-api-922105b50aa5f7c2c731f488a8999976.png"},22437:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-base-cpu-c50ce9a1e9a0bb6715386fe81d11e5d8.png"},42341:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-base-v2-breakdown-fe4ebdd50122d4c32e30517e8e66c2ce.png"},28436:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-base-v2-general-b19aece8c6549aa08c4caef4ba45bf02.png"},34331:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-base-v3-jfr-03b09d7ca4ff602d0f55716b5e13c64f.png"},55561:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-break-down-b8b23cb51fdc0b22bda557fd5f3d7836.png"},10224:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-enabled-845502a54c3df66f99d1e87d99480221.png"},88334:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/rest-gw-metrics-after-increaese-thread-pool-f43f9d58ea3cc7f791d9e8a6aa85e32f.png"},4078:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/six-cpus-fcbf79199e8adfeb2dd3f073ae0020fe.png"},12308:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/two-cpus-a003a95c94a18bd11326736ffd163332.png"},35386:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/virtual-threads-break-down-2d3df2e117f534eeb119f4614c377e90.png"},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>o});var a=n(96540);const i={},s=a.createContext(i);function r(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:t},e.children)}},35721:e=>{e.exports=JSON.parse('{"permalink":"/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-07-02-Follow-up-REST-API-performance/index.md","source":"@site/blog/2025-07-02-Follow-up-REST-API-performance/index.md","title":"Follow up REST API performance","description":"Investigating REST API performance","date":"2025-07-02T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":9.605,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Follow up REST API performance","date":"2025-07-02T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"nextItem":{"title":"Performance of REST API","permalink":"/zeebe-chaos/2025/06/30/Performance-of-REST-API"}}')}}]);