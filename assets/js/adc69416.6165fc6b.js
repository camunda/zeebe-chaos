"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[6477],{98112:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>n,metadata:()=>o,toc:()=>l});var i=s(74848),a=s(28453);const n={layout:"posts",title:"Hot backups impact on processing",date:new Date("2023-11-07T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},r="Chaos Day Summary",o={permalink:"/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-11-07-Hot-backups-impact-on-processing/index.md",source:"@site/blog/2023-11-07-Hot-backups-impact-on-processing/index.md",title:"Hot backups impact on processing",description:"Today, we want to experiment with hot backups in SaaS and a larger runtime state in Zeebe and how it impacts the ongoing processing in Zeebe (or not?). This is part of the investigation of a recently created bug issue we wanted to verify/reproduce #14696.",date:"2023-11-07T00:00:00.000Z",tags:[{inline:!0,label:"availability",permalink:"/zeebe-chaos/tags/availability"}],readingTime:3.82,hasTruncateMarker:!0,authors:[{name:"Christopher Kujawa",title:"Chaos Engineer @ Zeebe",url:"https://github.com/zelldon",page:{permalink:"/zeebe-chaos/authors/zell"},imageURL:"https://github.com/zelldon.png",key:"zell"}],frontMatter:{layout:"posts",title:"Hot backups impact on processing",date:"2023-11-07T00:00:00.000Z",categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},unlisted:!1,prevItem:{title:"Job push overloading",permalink:"/zeebe-chaos/2023/11/30/Job-push-overloading"},nextItem:{title:"Using Large Multi-Instance",permalink:"/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance"}},c={authorsImageUrls:[void 0]},l=[{value:"Chaos Experiment",id:"chaos-experiment",level:2},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"Step one",id:"step-one",level:4},{value:"Result",id:"result",level:5},{value:"Step two",id:"step-two",level:4},{value:"Result",id:"result-1",level:5},{value:"Step three",id:"step-three",level:4},{value:"Result",id:"result-2",level:5},{value:"Step four",id:"step-four",level:4},{value:"Result",id:"result-3",level:5},{value:"Step five",id:"step-five",level:4},{value:"Result",id:"result-4",level:5},{value:"Conclusion",id:"conclusion",level:3},{value:"Follow-ups",id:"follow-ups",level:4}];function h(e){const t={a:"a",blockquote:"blockquote",em:"em",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.p,{children:["Today, we want to experiment with hot backups in SaaS and a larger runtime state in Zeebe and how it impacts the ongoing processing in Zeebe (or not?). This is part of the investigation of a recently created bug issue we wanted to verify/reproduce ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/issues/14696",children:"#14696"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"TL;DR;"})," We were able to prove that hot backups are indeed not impacting overall processing throughput in Zeebe. We found that having a full Elasticsearch disk might impact or even fail your backups, which is intransparent to the user."]}),"\n",(0,i.jsx)(t.h2,{id:"chaos-experiment",children:"Chaos Experiment"}),"\n",(0,i.jsxs)(t.p,{children:["For the experiment, we have set up a Camunda SaaS cluster (G3-M configuration), and run the ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/tree/main/benchmarks/setup/cloud-default",children:"cloud benchmark"})," workload against it. During the experiment, we will run a stable load, which will cause to increase in the runtime state. We will create/initiate in different stages backups to verify the impact on processing depending on state size."]}),"\n",(0,i.jsx)(t.p,{children:"We kept the starter rate (creation of process instance 100 PI/s) but reduced the worker capacity and replicas."}),"\n",(0,i.jsx)(t.h3,{id:"expected",children:"Expected"}),"\n",(0,i.jsx)(t.p,{children:"Hot backups were built with the premise of not disrupting the processing throughput in Zeebe, which is why we define the following hypothesis:"}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Hypothesis"})}),"\n",(0,i.jsxs)(t.p,{children:["Creating hot backups should not impact Zeebe's ",(0,i.jsxs)(t.em,{children:["processing throughput ",(0,i.jsx)(t.em,{children:"no"})," matter how"]})," large the runtime state is in Zeebe."]}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"actual",children:"Actual"}),"\n",(0,i.jsx)(t.p,{children:"We created a cluster in the Camunda SaaS environment (in our internal stage)."}),"\n",(0,i.jsx)(t.h4,{id:"step-one",children:"Step one"}),"\n",(0,i.jsx)(t.p,{children:"We created a first backup to verify that it works without issues."}),"\n",(0,i.jsx)(t.h5,{id:"result",children:"Result"}),"\n",(0,i.jsx)(t.p,{children:"Success on stage one creating a backup with no actual state."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"first-backup",src:s(49539).A+"",width:"1890",height:"683"})}),"\n",(0,i.jsx)(t.h4,{id:"step-two",children:"Step two"}),"\n",(0,i.jsxs)(t.p,{children:["We started a stable load as mentioned ",(0,i.jsx)(t.a,{href:"#chaos-experiment",children:"above"}),". After reaching around ~100 MB runtime state at each partition we triggered a backup."]}),"\n",(0,i.jsx)(t.h5,{id:"result-1",children:"Result"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:s(82442).A+"",width:"1887",height:"498"})}),"\n",(0,i.jsxs)(t.p,{children:["The backup was successful and we were not able to observe any disruption in the processing throughput. We can see that during the backup is taken the exporting is paused (which is expected) and afterwards it is starting to export again. ","\u2705"]}),"\n",(0,i.jsx)(t.h4,{id:"step-three",children:"Step three"}),"\n",(0,i.jsx)(t.p,{children:"At a later stage, we tried to take a backup again with around ~300MB of runtime state in Zeebe."}),"\n",(0,i.jsx)(t.h5,{id:"result-2",children:"Result"}),"\n",(0,i.jsx)(t.p,{children:"Based on the output from Console the backup was successful and took around one hour."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"third-console",src:s(20007).A+"",width:"1364",height:"276"})}),"\n",(0,i.jsx)(t.p,{children:"Based on our internal metrics we can also see that there is no impact on the processing throughput"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"general-third",src:s(94462).A+"",width:"1888",height:"688"})}),"\n",(0,i.jsxs)(t.p,{children:["What is unclear to me is that it looks like we only took a backup of partition two. This needs to be further investigated, it might be also that the metrics are just confusing since it is resetting after the pod restarts.","\ud83d\udc1b"]}),"\n",(0,i.jsx)(t.h4,{id:"step-four",children:"Step four"}),"\n",(0,i.jsx)(t.p,{children:"Here we come into struggle after running the load on the cluster for quite some time we reached a runtime state size of ~1 gig. Furthermore, we filled our Elasticsearch disk tremendously."}),"\n",(0,i.jsx)(t.h5,{id:"result-3",children:"Result"}),"\n",(0,i.jsx)(t.p,{children:"At this time we were no longer able to successfully create backups. Here I tried it first without interacting with the cluster. It failed after 1.5 hours, which is potentially the timeout."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:s(64799).A+"",width:"1833",height:"271"})}),"\n",(0,i.jsx)(t.p,{children:"The backup failed, because of elastic was full."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"elastic-full",src:s(96376).A+"",width:"1889",height:"369"})}),"\n",(0,i.jsx)(t.p,{children:"I went ahead to remove some data from Elastic to keep experimenting."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.img,{alt:"elastic-indices",src:s(45367).A+"",width:"1240",height:"321"}),"\n",(0,i.jsx)(t.img,{alt:"delete-indices",src:s(16142).A+"",width:"1260",height:"407"})]}),"\n",(0,i.jsx)(t.h4,{id:"step-five",children:"Step five"}),"\n",(0,i.jsx)(t.p,{children:"After cleaning the Elasticsearch we retried taking a backup again. At this step, we already reached a runtime state of ~1.25G for each partition, which is quite huge."}),"\n",(0,i.jsx)(t.h5,{id:"result-4",children:"Result"}),"\n",(0,i.jsx)(t.p,{children:"The backup took quite a while and failed again."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"last-bk",src:s(72968).A+"",width:"1814",height:"398"})}),"\n",(0,i.jsx)(t.p,{children:"What we can see based on the times it is likely that it here timed out again. Taking a look at the metrics we see that a backup was processed in Zeebe."}),"\n",(0,i.jsx)(t.p,{children:"It had no impact on the ongoing processing throughput."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"fifth-bk",src:s(41078).A+"",width:"1895",height:"685"})}),"\n",(0,i.jsx)(t.p,{children:"At a later stage, we tried another backup with ~1.45G of runtime state even here we were not able to observe any issues related to the impact on processing throughput."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"sixth",src:s(4535).A+"",width:"1890",height:"688"})}),"\n",(0,i.jsx)(t.h3,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsxs)(t.p,{children:["\u2705"," We were able to prove that even on a large runtime state there is no observable impact on processing throughput in Zeebe."]}),"\n",(0,i.jsx)(t.p,{children:"Furthermore, we have seen that having a large Elastic state (almost full disk) will impact taking backups and is likely to fail them. We might need to iterate here, whether we want to tune the backup strategy, give elastic more space when taking backups, or adjust watermarks, etc."}),"\n",(0,i.jsx)(t.h4,{id:"follow-ups",children:"Follow-ups"}),"\n",(0,i.jsx)(t.p,{children:"We have to investigate the marking of failed backups, whether it is because of a timeout in the Operator or whether these backups are failed. It looks to me like they are marked as failed, even if they may succeed."}),"\n",(0,i.jsx)(t.p,{children:"Furthermore, the completed backups seem to be misleading and are reset which causes inconsistent views."})]})}function d(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},16142:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/deleting-indices2-c6c439bda96f933647a197156063af9e.png"},45367:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/deleting-indices3-2051dac5d38acbe4b05b930e152283e7.png"},96376:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/deleting-indices4-c2d15986b9499ae4ac28f1c264b1709e.png"},49539:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/first-backup-5d70b491b31ac0fd1eb11d052277570c.png"},41078:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/general-fifth-bk-162b830758b48c2b2a2d4359960c40e8.png"},82442:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/general-sec-bk-c0c03a786b5e701fd6ccc9bd75f9bd78.png"},4535:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/general-sixth-bk-8c901d74d9665263b9436406aaeac29d.png"},94462:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/general-third-bk-3ac8aaa53e75ac20345f7ab64c6a27e3.png"},72968:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/last-bk-failed-ec7ad090bc219c0a563e44cc123b3942.png"},20007:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/sec-bk-console-8af4a5c33d75d3628548f9d7b21c4a66.png"},64799:(e,t,s)=>{s.d(t,{A:()=>i});const i=s.p+"assets/images/third-bk-console-8c301c13085c61e11b5d5cafc6e245bb.png"},28453:(e,t,s)=>{s.d(t,{R:()=>r,x:()=>o});var i=s(96540);const a={},n=i.createContext(a);function r(e){const t=i.useContext(n);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(n.Provider,{value:t},e.children)}}}]);