"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[7603],{97721:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>n,toc:()=>o});var n=s(74721),i=s(74848),r=s(28453);const a={layout:"posts",title:"Dynamic Scaling: probing linear scalability",date:new Date("2025-07-11T00:00:00.000Z"),categories:["chaos_experiment","scalability","performance"],tags:["availability","scalability","performance"],authors:"sana"},l="Probing linear scalability by adding partitions dynamically",c={authorsImageUrls:[void 0]},o=[{value:"Hypothesis",id:"hypothesis",level:2},{value:"General Experiment setup",id:"general-experiment-setup",level:3},{value:"Initial cluster configuration",id:"initial-cluster-configuration",level:4},{value:"Target cluster configuration",id:"target-cluster-configuration",level:4},{value:"Experiment",id:"experiment",level:4},{value:"Expectation",id:"expectation",level:4},{value:"Steady state",id:"steady-state",level:2},{value:"First benchmark: 3 broker and 3 partitions",id:"first-benchmark-3-broker-and-3-partitions",level:3},{value:"Second part of the benchmark: scaling to 6 brokers and 6 partitions",id:"second-part-of-the-benchmark-scaling-to-6-brokers-and-6-partitions",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"hypothesis",children:"Hypothesis"}),"\n",(0,i.jsx)(t.p,{children:"The objective of this chaos day is to estimate the scalability of Zeebe when brokers and partitions are\nscaled together: we expect to be able to see the system scaling linearly with the number of brokers/partition\nin terms of throughput and back pressure, while maintaining predictable latency."}),"\n",(0,i.jsx)(t.h3,{id:"general-experiment-setup",children:"General Experiment setup"}),"\n",(0,i.jsxs)(t.p,{children:["To test this, we ran a benchmark using the latest alpha version of Camunda 8.8.0-alpha6, with the old\n",(0,i.jsx)(t.code,{children:"ElasticsearchExporter"})," disabled, and the new ",(0,i.jsx)(t.code,{children:"CamundaExporter"})," enabled. We also made sure Raft\nleadership was balanced before starting the test, meaning each broker is leader for exactly one partition,\nand we turned on partition scaling by adding the following environment variable:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.code,{children:"ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEPARTITIONSCALING=true"})}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Each broker also has a SSD-class volume with 32GB of disk space, limiting them to a few thousand IOPS.\nThe processing load was 150 processes per second, with a large payload of 32KiB each. Each process instance has\na single service task:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"one-task",src:s(28091).A+"",width:"999",height:"276"})}),"\n",(0,i.jsxs)(t.p,{children:["The processing load is generated by our own ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/tree/9e723b21b0e408fc2b97fd7d3f6b092af8e62dbe/benchmarks",children:"benchmarking application"}),"."]}),"\n",(0,i.jsx)(t.h4,{id:"initial-cluster-configuration",children:"Initial cluster configuration"}),"\n",(0,i.jsxs)(t.p,{children:["To test this hypothesis, we will start with a ",(0,i.jsx)(t.em,{children:"standard"})," configuration of the ",(0,i.jsx)(t.em,{children:"Camunda orchestration cluster"}),":"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"3 nodes"}),"\n",(0,i.jsx)(t.li,{children:"3 partitions"}),"\n",(0,i.jsx)(t.li,{children:"CPU limit: 2"}),"\n",(0,i.jsx)(t.li,{children:"Memory limit: 2 GB"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"We will increase the load through a load generator in fixed increments until we start to see the nodes showing constant non zero backpressure,\nwhich is a sign that the system has hit its throughput limits."}),"\n",(0,i.jsx)(t.h4,{id:"target-cluster-configuration",children:"Target cluster configuration"}),"\n",(0,i.jsxs)(t.p,{children:["Once that level of throughput is increased, we will scale broker & partitions ",(0,i.jsx)(t.strong,{children:"while the cluster is under load"})," to the new target value:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"6 nodes"}),"\n",(0,i.jsx)(t.li,{children:"6 partitions"}),"\n",(0,i.jsx)(t.li,{children:"CPU limit: 2"}),"\n",(0,i.jsx)(t.li,{children:"Memory limit: 2 GB"}),"\n"]}),"\n",(0,i.jsx)(t.h4,{id:"experiment",children:"Experiment"}),"\n",(0,i.jsx)(t.p,{children:"We expect that during the scaling operation the backpressure/latencies might worsen,\nbut only temporarily, as once the scaling operation has completed,\nthe additional load it generate is not present anymore."}),"\n",(0,i.jsx)(t.p,{children:"Then, we will execute the same procedure as above,\nuntil we hit 2x the critical throughput hit before."}),"\n",(0,i.jsx)(t.h4,{id:"expectation",children:"Expectation"}),"\n",(0,i.jsxs)(t.p,{children:["If the system scales linearly, we expect to see similar level of performance metrics\nfor similar values of the ratios ",(0,i.jsx)(t.code,{children:"PI (created/complete) per second / nr. of partition"}),"."]}),"\n",(0,i.jsx)(t.h2,{id:"steady-state",children:"Steady state"}),"\n",(0,i.jsxs)(t.p,{children:["The system is started with a throughput of 150 Process instances created per second.\nAs this is a ",(0,i.jsx)(t.strong,{children:"standard benchmark configuration"}),", nothing unexpected happens:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"The same number of process instances are completed as the ones created"}),"\n",(0,i.jsx)(t.li,{children:"The expected number of jobs is completed per unit of time"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"At this point, we have the following topology:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"initial-topology",src:s(77893).A+"",width:"1585",height:"192"})}),"\n",(0,i.jsx)(t.h3,{id:"first-benchmark-3-broker-and-3-partitions",children:"First benchmark: 3 broker and 3 partitions"}),"\n",(0,i.jsx)(t.p,{children:"Let's start increasing the load incrementally, by adding 30 Process instances/s for every step."}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Time"}),(0,i.jsx)(t.th,{children:"Brokers"}),(0,i.jsx)(t.th,{children:"Partitions"}),(0,i.jsx)(t.th,{children:"Throughput"}),(0,i.jsx)(t.th,{children:"CPU Usage"}),(0,i.jsx)(t.th,{children:"Throttling (CPU)"}),(0,i.jsx)(t.th,{children:"Backpressure"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"09:30"}),(0,i.jsx)(t.td,{children:"3"}),(0,i.jsx)(t.td,{children:"3"}),(0,i.jsx)(t.td,{children:"150 PI/s, 150 jobs/s"}),(0,i.jsx)(t.td,{children:"1.28 / 1.44 / 1.02"}),(0,i.jsx)(t.td,{children:"12% / 7% / 1%"}),(0,i.jsx)(t.td,{children:"0"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"09:49"}),(0,i.jsx)(t.td,{children:"3"}),(0,i.jsx)(t.td,{children:"3"}),(0,i.jsx)(t.td,{children:"180 PI/s, 180 jobs/s"}),(0,i.jsx)(t.td,{children:"1.34 / 1.54 / 1.12"}),(0,i.jsx)(t.td,{children:"20% / 17% / 2%"}),(0,i.jsx)(t.td,{children:"0"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"10:00"}),(0,i.jsx)(t.td,{children:"3"}),(0,i.jsx)(t.td,{children:"3"}),(0,i.jsx)(t.td,{children:"210 PI/s, 210 jobs/s"}),(0,i.jsx)(t.td,{children:"1.79 / 1.62 / 1.33"}),(0,i.jsx)(t.td,{children:"28% / 42% / 4%"}),(0,i.jsx)(t.td,{children:"0"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"10:12"}),(0,i.jsx)(t.td,{children:"3"}),(0,i.jsx)(t.td,{children:"3"}),(0,i.jsx)(t.td,{children:"240 PI/s, 240 jobs/s"}),(0,i.jsx)(t.td,{children:"1.77 / 1.95 / 1.62"}),(0,i.jsx)(t.td,{children:"45% / 90% / 26%"}),(0,i.jsx)(t.td,{children:"0/0.5%"})]})]})]}),"\n",(0,i.jsxs)(t.p,{children:["At 240 Process Instances spawned per second, the system starts to hit the limits:\n",(0,i.jsx)(t.img,{alt:"CPU usage @ 240 PI/s",src:s(28661).A+"",width:"1702",height:"1127"}),"\n",(0,i.jsx)(t.img,{alt:"CPU throttling@ 240 PI/s",src:s(1563).A+"",width:"1702",height:"1127"})]}),"\n",(0,i.jsxs)(t.p,{children:["And the backpressure is not zero anymore:\n",(0,i.jsx)(t.img,{alt:"Backpressure @ 240 PI/s",src:s(11339).A+"",width:"1702",height:"1127"})]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["The CPU throttling reaches almost 90% on one node (this is probably caused by only one node being selected as ",(0,i.jsx)(t.strong,{children:"gateway"})," as previously noted)"]}),"\n",(0,i.jsx)(t.li,{children:"Backpressure is now constantly above zero, even if it's just 0.5%, it's a sign that we are reaching the throughput limits."}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"second-part-of-the-benchmark-scaling-to-6-brokers-and-6-partitions",children:"Second part of the benchmark: scaling to 6 brokers and 6 partitions"}),"\n",(0,i.jsx)(t.p,{children:"With 240 process instances per second being spawned, we send the commands to scale the cluster."}),"\n",(0,i.jsxs)(t.p,{children:["We first scale the ",(0,i.jsx)(t.code,{children:"zeebe"})," ",(0,i.jsx)(t.em,{children:"statefulset"})," to 6 brokers. As soon as the new brokers are running, even before they are healthy,\nwe can send the command to include them in the cluster and to increase the number of partition to 6."]}),"\n",(0,i.jsxs)(t.p,{children:["This can be done following the ",(0,i.jsx)(t.a,{href:"https://docs.camunda.io/docs/next/self-managed/components/orchestration-cluster/zeebe/operations/cluster-scaling/#2b-scaling-brokers-and-partitions",children:"guide in the official docs"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["Once the scaling has been completed, as can be seen from the ",(0,i.jsx)(t.strong,{children:"Cluster operation"})," section in the dashboard, we see the newly created\npartitions participate in the workload."]}),"\n",(0,i.jsx)(t.p,{children:"We now have the following topology:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"six-partitions-topology",src:s(12209).A+"",width:"1569",height:"298"})}),"\n",(0,i.jsx)(t.p,{children:"As we did before, let's start increasing the load incrementally as we did with the other cluster configuration."}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Time"}),(0,i.jsx)(t.th,{children:"Brokers"}),(0,i.jsx)(t.th,{children:"Partitions"}),(0,i.jsx)(t.th,{children:"Throughput"}),(0,i.jsx)(t.th,{children:"CPU Usage"}),(0,i.jsx)(t.th,{children:"Throttling (CPU)"}),(0,i.jsx)(t.th,{children:"Backpressure"}),(0,i.jsx)(t.th,{children:"Notes"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"10:27"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsx)(t.td,{children:"240 PI/s"}),(0,i.jsx)(t.td,{children:"0.92/1.26/0.74/0.94/0.93/0.93"}),(0,i.jsx)(t.td,{children:"2.8/6.0/0.3/2.8/3.4/3.18"}),(0,i.jsx)(t.td,{children:"0"}),(0,i.jsx)(t.td,{children:"After scale up"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"11:05"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsx)(t.td,{children:"300 PI/s"}),(0,i.jsx)(t.td,{children:"1.17/1.56/1.06/1.23/1.19/1.18"}),(0,i.jsx)(t.td,{children:"9%/29%/0.6%/9%/11%/10%"}),(0,i.jsx)(t.td,{children:"0"}),(0,i.jsx)(t.td,{children:"Stable"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"11:10"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsx)(t.td,{children:"360 PI/s"}),(0,i.jsx)(t.td,{children:"1.39/1.76/1.26/1.43/1.37/1.42"}),(0,i.jsx)(t.td,{children:"19%/42%/2%/16%/21%/22%"}),(0,i.jsx)(t.td,{children:"0"}),(0,i.jsx)(t.td,{children:"Stable"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"11:10"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsx)(t.td,{children:"6"}),(0,i.jsx)(t.td,{children:"420 PI/s"}),(0,i.jsx)(t.td,{children:"1.76/1.89/1.50/1.72/1.50/1.70"}),(0,i.jsx)(t.td,{children:"76%/84%/52%/71%/60%/65%"}),(0,i.jsx)(t.td,{children:"0 (spurts on 1 partition)"}),(0,i.jsx)(t.td,{children:"Pushing hard"})]})]})]}),"\n",(0,i.jsx)(t.p,{children:"However, at 11:32 one of the workers restarted, causing a spike in the processing due to jobs being yielded back to the engine, less jobs to be activated,\nand thus less to be completed. This caused a job backlog to build up in the engine. Once the worker restarted, the backlog was drained, leading to a spike in\njob completion requests: around 820 req/s, as opposed to the expected 420 req/s."}),"\n",(0,i.jsx)(t.p,{children:"Because of this extra load, the cluster started to consume even more CPU, resulting in heavy CPU throttling from the cloud provider."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.img,{alt:"CPU usage @ 420 PI/s",src:s(35650).A+"",width:"1649",height:"1128"}),"\n",(0,i.jsx)(t.img,{alt:"CPU throttling @ 420 PI/s",src:s(90954).A+"",width:"1649",height:"1128"})]}),"\n",(0,i.jsx)(t.p,{children:"On top of this, eventually a broker restarted (most likely as we run on spot VMs). In order to continue with our test, we scaled the load down to 60 PI/s\nto give the cluster the time to heal."}),"\n",(0,i.jsx)(t.p,{children:"Once the cluster was healthy again, we raised the throughput back to 480 PI/s to verify the scalability with twice as much throughput as the initial configuration."}),"\n",(0,i.jsx)(t.p,{children:"The cluster was able to sustain 480 process instances per second with similar levels of backpressure of the initial configuration:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Backpressure @ 480 PI/s",src:s(57756).A+"",width:"1705",height:"1135"})}),"\n",(0,i.jsx)(t.p,{children:"We can see below that CPU usage is high, and there is still some throttling, indicating we might be able to do more with a little bit of vertical scaling, or by scaling out and reducing the number of partitions per broker:"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.img,{alt:"CPU usage @ 480 PI/s",src:s(14448).A+"",width:"1702",height:"1127"}),"\n",(0,i.jsx)(t.img,{alt:"CPU throttling",src:s(41424).A+"",width:"1702",height:"1127"})]}),"\n",(0,i.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(t.p,{children:"We were able to verify that the cluster can scale almost linearly with new brokers and partitions, so long as the other components, like the secondary storage, workers, connectors, etc., are able to sustain a similar."}),"\n",(0,i.jsx)(t.p,{children:"In particular, making sure that the secondary storage is able to keep up with the throughput turned out to be crucial to keep the cluster stable in order to\navoid filling up the Zeebe disks, which would bring to a halt the cluster."}),"\n",(0,i.jsx)(t.p,{children:"We encountered a similar issue when one worker restarts: initially it creates a backlog of unhandled jobs, which turns into a massive increase in requests per second when the worker comes back, as it starts activating jobs faster than the cluster can complete them."}),"\n",(0,i.jsx)(t.p,{children:"Finally, with this specific test, it would be interesting to explore the limits of vertical scalability, as we often saw CPU throttling being a major blocker for processing. This would make for an interesting future experiment."})]})}function d(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},11339:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/config_1_240_backpressure-dafc5cbae6bd06f5ed054fa4a1b87377.png"},28661:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/config_1_240_cpu-8520ff99e75fea10d3add1bf720b21ab.png"},1563:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/config_1_240_cpu_throttling-0f8daf817e83828dffb1f5468577e753.png"},35650:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/config_2_420_cpu-7179ead09ae826515cfc6a8aeafec600.png"},90954:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/config_2_420_cpu_throttling-f4f8d922f4cdf6859965acb1c6a3b78f.png"},57756:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/config_2_480_backpressure-50f97c503678bcf1a51c1c316029be52.png"},14448:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/config_2_480_cpu-33e89ad48a794be879a4977d4c5d1a5f.png"},41424:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/config_2_480_cpu_throttling-dc7f857c52e898fb159f70761f142499.png"},77893:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/initial-topology-37fddc92ed46f6b20ae0cfb7ff0cac27.png"},28091:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/one_task-f083f237e568d87cc17eef056cb45d73.png"},12209:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/six-partitions-topology-c06f2c4b77537e86ff32e533466494ed.png"},28453:(e,t,s)=>{s.d(t,{R:()=>a,x:()=>l});var n=s(96540);const i={},r=n.createContext(i);function a(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),n.createElement(r.Provider,{value:t},e.children)}},74721:e=>{e.exports=JSON.parse('{"permalink":"/zeebe-chaos/2025/07/11/linear-dynamic-scaling","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-07-11-linear-dynamic-scaling/index.md","source":"@site/blog/2025-07-11-linear-dynamic-scaling/index.md","title":"Dynamic Scaling: probing linear scalability","description":"Hypothesis","date":"2025-07-11T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"scalability","permalink":"/zeebe-chaos/tags/scalability"},{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":6.415,"hasTruncateMarker":false,"authors":[{"name":"Carlo Sana","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/entangled90","imageURL":"https://github.com/entangled90.png","key":"sana","page":null}],"frontMatter":{"layout":"posts","title":"Dynamic Scaling: probing linear scalability","date":"2025-07-11T00:00:00.000Z","categories":["chaos_experiment","scalability","performance"],"tags":["availability","scalability","performance"],"authors":"sana"},"unlisted":false,"nextItem":{"title":"Follow up REST API performance","permalink":"/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance"}}')}}]);