"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[4053],{62514:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/07/11/linear-dynamic-scaling","metadata":{"permalink":"/zeebe-chaos/2025/07/11/linear-dynamic-scaling","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-07-11-linear-dynamic-scaling/index.md","source":"@site/blog/2025-07-11-linear-dynamic-scaling/index.md","title":"Dynamic Scaling: probing linear scalability","description":"Hypothesis","date":"2025-07-11T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"scalability","permalink":"/zeebe-chaos/tags/scalability"},{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":6.41,"hasTruncateMarker":false,"authors":[{"name":"Carlo Sana","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/entangled90","imageURL":"https://github.com/entangled90.png","key":"sana","page":null}],"frontMatter":{"layout":"posts","title":"Dynamic Scaling: probing linear scalability","date":"2025-07-11T00:00:00.000Z","categories":["chaos_experiment","scalability","performance"],"tags":["availability","scalability","performance"],"authors":"sana"},"unlisted":false,"nextItem":{"title":"Follow up REST API performance","permalink":"/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance"}},"content":"## Hypothesis\\n\\nThe objective of this chaos day is to estimate the scalability of Zeebe when brokers and partitions are\\nscaled together: we expect to be able to see the system scaling linearly with the number of brokers/partition\\nin terms of throughput and back pressure, while maintaining predictable latency.\\n\\n### General Experiment setup\\n\\nTo test this, we ran a benchmark using the latest alpha version of Camunda 8.8.0-alpha6, with the old\\n`ElasticsearchExporter` disabled, and the new `CamundaExporter` enabled. We also made sure Raft\\nleadership was balanced before starting the test, meaning each broker is leader for exactly one partition,\\nand we turned on partition scaling by adding the following environment variable:\\n\\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEPARTITIONSCALING=true`\\n\\nEach broker also has a SSD-class volume with 32GB of disk space, limiting them to a few thousand IOPS.\\nThe processing load was 150 processes per second, with a large payload of 32KiB each. Each process instance has\\na single service task:\\n\\n![one-task](./one_task.png)\\n\\nThe processing load is generated by our own [benchmarking application](https://github.com/camunda/camunda/tree/9e723b21b0e408fc2b97fd7d3f6b092af8e62dbe/benchmarks).\\n\\n#### Initial cluster configuration\\n\\nTo test this hypothesis, we will start with a *standard* configuration of the *Camunda orchestration cluster*:\\n\\n- 3 nodes\\n- 3 partitions\\n- CPU limit: 2\\n- Memory limit: 2 GB\\n\\nWe will increase the load through a load generator in fixed increments until we start to see the nodes showing constant non zero backpressure,\\nwhich is a sign that the system has hit its throughput limits.\\n\\n#### Target cluster configuration\\n\\nOnce that level of throughput is increased, we will scale broker & partitions **while the cluster is under load** to the new target value:\\n\\n- 6 nodes\\n- 6 partitions\\n- CPU limit: 2\\n- Memory limit: 2 GB\\n\\n#### Experiment\\n\\nWe expect that during the scaling operation the backpressure/latencies might worsen,\\nbut only temporarily, as once the scaling operation has completed,\\nthe additional load it generate is not present anymore.\\n\\nThen, we will execute the same procedure as above,\\nuntil we hit 2x the critical throughput hit before.\\n\\n#### Expectation\\n\\nIf the system scales linearly, we expect to see similar level of performance metrics\\nfor similar values of the ratios `PI (created/complete) per second / nr. of partition`.\\n\\n## Steady state\\n\\nThe system is started with a throughput of 150 Process instances created per second.\\nAs this is a **standard benchmark configuration**, nothing unexpected happens:\\n\\n- The same number of process instances are completed as the ones created\\n- The expected number of jobs is completed per unit of time\\n\\nAt this point, we have the following topology:\\n\\n![initial-topology](./initial-topology.png)\\n\\n### First benchmark: 3 broker and 3 partitions\\n\\nLet\'s start increasing the load incrementally, by adding 30 Process instances/s for every step.\\n\\n| Time   | Brokers | Partitions | Throughput       | CPU Usage               | Throttling (CPU)        | Backpressure     |\\n|--------|---------|------------|------------------|-------------------------|-------------------------|------------------|\\n| 09:30  | 3       | 3          | 150 PI/s, 150 jobs/s | 1.28 / 1.44 / 1.02  | 12% / 7% / 1%          | 0                 |\\n| 09:49  | 3       | 3          | 180 PI/s, 180 jobs/s | 1.34 / 1.54 / 1.12  | 20% / 17% / 2%         | 0                 |\\n| 10:00  | 3       | 3          | 210 PI/s, 210 jobs/s | 1.79 / 1.62 / 1.33  | 28% / 42% / 4%         | 0                 |\\n| 10:12  | 3       | 3          | 240 PI/s, 240 jobs/s | 1.77 / 1.95 / 1.62  | 45% / 90% / 26%        | 0/0.5%            |\\n\\nAt 240 Process Instances spawned per second, the system starts to hit the limits:\\n![CPU usage @ 240 PI/s](./config_1_240_cpu.png)\\n![CPU throttling@ 240 PI/s](./config_1_240_cpu_throttling.png)\\n\\nAnd the backpressure is not zero anymore:\\n![Backpressure @ 240 PI/s](./config_1_240_backpressure.png)\\n\\n- The CPU throttling reaches almost 90% on one node (this is probably caused by only one node being selected as **gateway** as previously noted)\\n- Backpressure is now constantly above zero, even if it\'s just 0.5%, it\'s a sign that we are reaching the throughput limits.\\n\\n### Second part of the benchmark: scaling to 6 brokers and 6 partitions\\n\\nWith 240 process instances per second being spawned, we send the commands to scale the cluster.\\n\\nWe first scale the `zeebe` *statefulset* to 6 brokers. As soon as the new brokers are running, even before they are healthy,\\nwe can send the command to include them in the cluster and to increase the number of partition to 6.\\n\\nThis can be done following the [guide in the official docs](https://docs.camunda.io/docs/next/self-managed/components/orchestration-cluster/zeebe/operations/cluster-scaling/#2b-scaling-brokers-and-partitions).\\n\\nOnce the scaling has been completed, as can be seen from the **Cluster operation** section in the dashboard, we see the newly created\\npartitions participate in the workload.\\n\\nWe now have the following topology:\\n\\n![six-partitions-topology](./six-partitions-topology.png)\\n\\nAs we did before, let\'s start increasing the load incrementally as we did with the other cluster configuration.\\n\\n| Time   | Brokers | Partitions | Throughput       | CPU Usage                                    | Throttling (CPU)                  | Backpressure         | Notes            |\\n|--------|---------|------------|------------------|----------------------------------------------|------------------------------------|----------------------|------------------|\\n| 10:27  | 6       | 6          | 240 PI/s         | 0.92/1.26/0.74/0.94/0.93/0.93                | 2.8/6.0/0.3/2.8/3.4/3.18     | 0                    | After scale up   |\\n| 11:05  | 6       | 6          | 300 PI/s         | 1.17/1.56/1.06/1.23/1.19/1.18                | 9%/29%/0.6%/9%/11%/10%     | 0                    | Stable           |\\n| 11:10  | 6       | 6          | 360 PI/s         | 1.39/1.76/1.26/1.43/1.37/1.42                | 19%/42%/2%/16%/21%/22%    | 0                    | Stable           |\\n| 11:10  | 6       | 6          | 420 PI/s         | 1.76/1.89/1.50/1.72/1.50/1.70                | 76%/84%/52%/71%/60%/65%     | 0 (spurts on 1 partition) | Pushing hard    |\\n\\nHowever, at 11:32 one of the workers restarted, causing a spike in the processing due to jobs being yielded back to the engine, less jobs to be activated,\\nand thus less to be completed. This caused a job backlog to build up in the engine. Once the worker restarted, the backlog was drained, leading to a spike in\\njob completion requests: around 820 req/s, as opposed to the expected 420 req/s.\\n\\nBecause of this extra load, the cluster started to consume even more CPU, resulting in heavy CPU throttling from the cloud provider.\\n\\n![CPU usage @ 420 PI/s](./config_2_420_cpu.png)\\n![CPU throttling @ 420 PI/s](./config_2_420_cpu_throttling.png)\\n\\nOn top of this, eventually a broker restarted (most likely as we run on spot VMs). In order to continue with our test, we scaled the load down to 60 PI/s\\nto give the cluster the time to heal.\\n\\nOnce the cluster was healthy again, we raised the throughput back to 480 PI/s to verify the scalability with twice as much throughput as the initial configuration.\\n\\nThe cluster was able to sustain 480 process instances per second with similar levels of backpressure of the initial configuration:\\n\\n![Backpressure @ 480 PI/s](./config_2_480_backpressure.png)\\n\\nWe can see below that CPU usage is high, and there is still some throttling, indicating we might be able to do more with a little bit of vertical scaling, or by scaling out and reducing the number of partitions per broker:\\n\\n![CPU usage @ 480 PI/s](./config_2_480_cpu.png)\\n![CPU throttling](./config_2_480_cpu_throttling.png)\\n\\n## Conclusion\\n\\nWe were able to verify that the cluster can scale almost linearly with new brokers and partitions, so long as the other components, like the secondary storage, workers, connectors, etc., are able to sustain a similar.\\n\\nIn particular, making sure that the secondary storage is able to keep up with the throughput turned out to be crucial to keep the cluster stablein order to\\navoid filling up the Zeebe disks, which would bring to a halt the cluster.\\n\\nWe encountered a similar issue when one worker restarts: initially it creates a backlog of unhandled jobs, which turns into a massive increase in requests per second when the worker comes back, as it starts activating jobs faster than the cluster can complete them.\\n\\nFinally, with this specific test, it would be interesting to explore the limits of vertical scalability, as we often saw CPU throttling being a major blocker for processing. This would make for an interesting future experiment."},{"id":"/2025/07/02/Follow-up-REST-API-performance","metadata":{"permalink":"/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-07-02-Follow-up-REST-API-performance/index.md","source":"@site/blog/2025-07-02-Follow-up-REST-API-performance/index.md","title":"Follow up REST API performance","description":"Investigating REST API performance","date":"2025-07-02T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":19.785,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Follow up REST API performance","date":"2025-07-02T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Dynamic Scaling: probing linear scalability","permalink":"/zeebe-chaos/2025/07/11/linear-dynamic-scaling"},"nextItem":{"title":"Performance of REST API","permalink":"/zeebe-chaos/2025/06/30/Performance-of-REST-API"}},"content":"## Investigating REST API performance\\n\\nThis post collates the experiments, findings, and lessons learned during the REST API performance investigation.\\n\\nThere wasn\'t one explicit root cause identified. As it is often the case with such performance issues, it is the combination of several things.\\n\\n**Quint essence:** REST API is more CPU intense/heavy than gRPC. You can read more about this in the [conclusion part](#conclusion). We have discovered ~10 issues we have to follow up with, where at least 2-3 might have a significant impact in the performance. Details can be found in the [Discovered issues](#discovered-issues) section\\n\\n\x3c!--truncate--\x3e\\n \\n_Short summary of what we have done and validated_\\n  \\n * Investigations\\n    * Investigated existing REST api metrics\\n        * Breakdown metrics to have a better overview of where time is spent (created a tmp dashboard)\\n    * Investigated worker failing with OOM\\n    * Investigated deployments and anti-affinities\\n    * Investigated command distribution\\n    * Investigated JFR recordings and profiles\\n      * Take JFR recordings and profile the system\\n      * Make use of the async profiler\\n * Experiments\\n    * Increase CPU resources to understand whether it is resource contention - it is.\\n    * Improve Spring request filtering and execution\\n      * Use virtual threads for Spring\\n      * Use PathPattern instead of legacy AntPathPattern\\n      * Use direct response handling instead of asynchronous\\n      * Combine some of them\\n    * Experiment with different setups to better distribute the load\\n\\n\\n\x3c!--truncate--\x3e\\n\\n## Day 1: Investigation REST API Performance\\n\\nThis blog post aims to summarize the investigation of the REST API performance and give some hints and suggestions on what to improve.\\n\\n### REST API Metrics\\n\\nOne remark from the last experiments was that we do not have good insights for the REST API. Actually, we have the necessary metrics already exposed, but not yet available in our Dashboard.\\n\\nThis is currently prepared with [#33907](https://github.com/camunda/camunda/pull/33907). Based on this, I was able to further investigate the REST API performance.\\n\\n![rest-api](rest-api.png)\\n\\nWhat we can see is that our requests take on average more than 50ms to complete. This is causing our throughput to go down, we are not able to create 150 PI/s even.\\n\\nLooking at a different Benchmark using gRPC, we can see that requests take 5-10ms to complete, and have a stable throughput \\n\\n![grpc-latency](grpc-latency.png)\\n![grpc](grpc.png)\\n\\nDue to the slower workers (on completion), we can see error reports of the workers not being able to accept further job pushes. This has been mentioned in the previous blog post as well.  This, in consequence, means the worker sends FAIL commands for such jobs, to give them back. It has a cascading effect, as jobs are sent back and forth, impacting the general process instance execution latency (which grows up to 60s compared to 0.2s).\\n\\n\\n### Investigating Worker Errors\\n\\nIn our previous experiments, we have seen the following exceptions\\n\\n```\\n13:25:14.684 [pool-4-thread-3] WARN  io.camunda.client.job.worker - Worker benchmark failed to handle job with key 4503599628992806 of type benchmark-task, sending fail command to broker\\njava.lang.IllegalStateException: Queue full\\n\\tat java.base/java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:?]\\n\\tat java.base/java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:329) ~[?:?]\\n\\tat io.camunda.zeebe.Worker.lambda$handleJob$1(Worker.java:122) ~[classes/:?]\\n\\tat io.camunda.client.impl.worker.JobRunnableFactoryImpl.executeJob(JobRunnableFactoryImpl.java:45) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat io.camunda.client.impl.worker.JobRunnableFactoryImpl.lambda$create$0(JobRunnableFactoryImpl.java:40) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat io.camunda.client.impl.worker.BlockingExecutor.lambda$execute$0(BlockingExecutor.java:50) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]\\n\\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?\\n```\\n\\nThis is actually coming from the Worker (benchmark) application, as it is collecting all [the request futures in a blocking queue](https://github.com/camunda/camunda/blob/main/zeebe/benchmarks/project/src/main/java/io/camunda/zeebe/Worker.java#L54).\\n\\nAs the performance is lower of handling requests, we collect more futures in the worker, causing to fill the queue. This in the end causes also to fail more jobs - causing even more work.\\n\\nThis allows explains why our workers have a higher memory consumption - we had to increase the worker memory to have a stable worker.\\n\\n### Profiling the System\\n\\nWith the previous results, we were encouraged to do some profiling. For the start we used [JFR](https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170) for some basic profiling.\\n\\nYou can do this by:\\n\\n```shell\\n  kubectl exec -it \\"$1\\" -- jcmd 1 JFR.start duration=100s filename=/usr/local/camunda/data/flight-$(date +%d%m%y-%H%M).jfr\\n```\\n\\nIf the flight recording is done, you can copy the recording (via `kubectl cp`) and open it with Intellij (JMC didn\'t work for me) \\n\\n![first-profile](first-profile.png)\\n\\nWe see that the Spring filter chaining is dominating the profile, which is not unexpected as every request has gone through this chain. As this is a CPU based sampling profile, it is likely to be part of the profile. Still, it was something interesting to note and investigate.\\n\\n#### Path pattern matching\\n\\nSome research showed that it might be interesting to look into other path pattern matchers, as we use the (legacy) [ant path matcher](https://github.com/camunda/camunda/blob/main/dist/src/main/resources/application.properties#L17) with [regex](https://github.com/camunda/camunda/blob/main/authentication/src/main/java/io/camunda/authentication/config/WebSecurityConfig.java#L86).  \\n\\n**Resources:**\\n\\n * PathPattern - https://spring.io/blog/2020/06/30/url-matching-with-pathpattern-in-spring-mvc#pathpattern\\n * [Results of using PathPattern and related discussion on GH](https://github.com/spring-projects/spring-framework/issues/31098#issuecomment-1891737375)\\n\\n#### Gateway - Broker request latency\\n\\nAs we have such a high request-response latency, we have to find out where the time is spent. Ideally, we would have some sort of tracing (which we didn\'t have yet), or we would look at metrics that cover sub-parts of the system and the request-response cycle.\\n\\nThe REST API request-response latency metric, we can take it as the complete round trip, accepting the request on the gateway edge, converting it to a Broker request, sending it to the Broker, the Broker processes, sends the response back, etc.\\n\\nLuckily, we have a metric that is covering the part of sending the Broker request (from the other side of the Gateway) to the Broker and wait for the response. See related [code here](https://github.com/camunda/camunda/blob/main/zeebe/broker-client/src/main/java/io/camunda/zeebe/broker/client/impl/BrokerRequestManager.java#L153).\\n\\nThe difference shows us that there is not a small overhead, meaning that actually the Gateway to Broker request-response is slower with REST as well, which is unexpected.\\n\\nThis can either be because different data is sent, or a different API is used, or some other execution mechanics, etc.\\n\\nUsing the same cluster and enabling the REST API later, we can see the immediate effect on performance.\\n\\n![rest-enabled](rest-enabled.png)\\n\\n##### Request handling execution logic\\n\\nA difference we have spotted with REST API and gRPC is the usage of the BrokerClient.\\n\\nWhile we use on the gRPC side the [BrokerClient with retries](https://github.com/camunda/camunda/blob/main/zeebe/gateway-grpc/src/main/java/io/camunda/zeebe/gateway/EndpointManager.java#L457) and direct response handling, on the REST API we use no retries and [handle the response async with the ForkJoinPool](https://github.com/camunda/camunda/blob/main/service/src/main/java/io/camunda/service/ApiServices.java#L55).\\n\\nAs our benchmark clusters have two CPUs, [meaning 1 Thread for the common ForkJoin thread pool](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html) we expected some contention on the thread.\\n\\nFor testing purposes, we increased the thread count by: `-Djava.util.concurrent.ForkJoinPool.common.parallelism=8`\\n\\nIn a profile we can see that more threads are used, but it doesn\'t change anything in the performance.\\n\\n![profile-inc-fork-join](profile-inc-fork-join.png)\\n\\n![rest-gw-metrics-after-increaese-thread-pool](rest-gw-metrics-after-increaese-thread-pool.png)\\n\\nThe assumption was that we might not be able to handle the response in time with one thread, and this causes some contention also on the Gateway-Broker request-response cycle, but this is not the case.\\n\\nWe seem to spend time somewhere else or have a general resource contention issue. What we can see is that we have to work with more CPU throttling, then without REST API usage.\\n\\n![rest-api-cpu-throttling.png](rest-api-cpu-throttling.png)\\n\\nIncreasing the CPU resolves the general performance problem, hinting even more that we might have some issues with threads competing with resources, etc.\\n\\nIn the following screenshot, you see the test with 6 CPUs per Camunda application.\\n\\n![six-cpus](six-cpus.png)\\n\\nCompared to the previous run with 2 CPUs per Camunda application, where it had to fight with a lot of CPU throttling. The request-response latency was five times higher on average.\\n\\n![two-cpus](two-cpus.png)\\n\\nWe have to further investigate this based on this knowledge.\\n\\n## Day 2: Profiling and Experimenting\\n\\nYesterday I was taking profiles with 100s, to reduce the noise. Still, we can see that the filter chain is taking ~40% of the complete profile.\\n\\n![jfr-10-minutes-filter-chain.png](jfr-10-minutes-filter-chain.png)\\n\\nWhen opening the JFR recording with JMC, we get some hints, related to context switches, CPU throttling (which we already know) and the inverted parallelism of GC (also mentioning high IO).\\n\\n![locks-and-contention-context-switch.png](locks-and-contention-context-switch.png)\\n![jfr-cpu-throttling-detection.png](jfr-cpu-throttling-detection.png)\\n![gc-ineffeciency-high-io.png](gc-ineffeciency-high-io.png)\\n\\nWe have already seen in our metrics, for example, that we fight with high CPU throttling\\n\\n![rest-base-cpu](rest-base-cpu.png)\\n\\nTo better analyze (and circumvent that we have no tracing), I added some more metrics to understand where time is spent. Furthermore, I created a temporary dashboard to break down where time is spent.\\n\\nWhen we look at the base with gRPC (taking our weekly benchmarks), we can see all latencies are low, and mostly under 5 ms.\\n\\n![grpc-break-down.png](grpc-break-down.png)\\n\\nAs soon as we enable the REST API, we can see the latencies go up. The most significant increase we see is in the job activations.\\n\\n![rest-break-down](rest-break-down.png)\\n\\nFascinating is that the write to process latency, the time from acceptance by the CommandAPI until the processor processes this command, also increases.\\n\\n### Virtual threads\\n\\nTo remove some thoughts about potential IO and CPU contention, I experimented with virtual threads, which we can [easily enable for Spring](https://www.baeldung.com/spring-6-virtual-threads).\\n\\nI set the following system property on the statefulset.\\n\\n```shell\\n-Dspring.threads.virtual.enabled=true\\n```\\n\\nTaking a new profile, we can see that all the http threads are gone, but still the filtering is prominent.\\n\\n![jfr-virtual-threads.png](jfr-virtual-threads.png)\\n\\nChecking our metrics break-down again we see there is no benefit here.\\n\\n![virtual-threads-break-down.png](virtual-threads-break-down.png)\\n\\n### Direct handling\\n\\nInvestigating the code basis, we saw several times `#handleAsync` without using an extra executor, causing to use of the ForkJoinPool (as mentioned the other day). One idea was to [directly handle the future completions](https://github.com/camunda/camunda/commit/265d7164f5384be8c443c30b20e432582df09c24), meaning the response handling, etc.\\n\\nWe didn\'t observe any benefits with this.\\n\\n![direct-handling-breakdown.png](direct-handling-breakdown.png)\\n\\nIn the JFR recording, we can see that less Threads are used, but the Spring filter chain is also super prominent.\\n![direct-handling-v2-profile-too-much-filtering.png](direct-handling-v2-profile-too-much-filtering.png)\\n\\n### Spring PathPattern parser for MVC\\n\\nAt the end of the day I finally came to try the `PathPattern` parser. As mentioned the other day, it is recommended to use it over the legacy `AntPathMatcher`. \\n\\nThe migration was [rather simple](https://github.com/camunda/camunda/commit/357522d8355a624a1c07e1fb889561254b0305ba), we can replace the `spring.mvc.pathmatch.matching-strategy=ant_path_matcher` with\\n`spring.mvc.pathmatch.matching-strategy=path_pattern_parser`, we only had to fix some occurrences of regex combinations with `**`, as it is only allowed to have `**` at the end (no regex after).\\n\\nSee related branch [ck-pattern-path-parse](https://github.com/camunda/camunda/commits/ck-pattern-path-parse/).\\n\\n![path-pattern-breakdown](path-pattern-breakdown.png)\\n\\nWe were able to reduce the latencies by half, which also allowed us to bring back our throughput.\\n\\n![path-pattern-general.png](path-pattern-general.png)\\n\\nI did a cross-check with the current SNAPSHOT, and weirdly the SNAPSHOT now behaved the same. I will run this for a while to see the results, as it might fail after a certain period of time. As this might also be related to where the pods are scheduled (noisy neighbours etc.)\\n\\n![rest-base-v2-breakdown.png](rest-base-v2-breakdown.png)\\n![rest-base-v2-general.png](rest-base-v2-general.png)\\n\\n### Combination of direct handle and PathPattern\\n\\nOn top of the above, I [combined the direct handling and PathPattern usage](https://github.com/camunda/camunda/commits/ck-direct-handle/), and this gave us the best results.\\n\\nThe latencies are only two times higher than gRPC vs before 5 times (and more).\\n\\n![combination-of-all-breakdown.png](combination-of-all-breakdown.png)\\n\\nThe throttling of the CPU was reduced by half as well.\\n\\n![combination-of-all-cpu.png](combination-of-all-cpu.png)\\n\\nThis gives a great stable throughput again.\\n\\n![combination-of-all-general.png](combination-of-all-general.png)\\n\\n## Day 3: Observing load tests and further experimenting\\n\\nYesterday, I have started several load tests for things I have tried out in code (like PathPattern or direct response handling), but also from different commits of the main branch (the current SNASPHOT, some commits that touch the rest gateway, and from begin of the week).\\n\\nFrom what we observed is that some load tests can run stable for quite a while, until they break down. It is often related to restarts/rescheduling, or is already in general suboptimal resource distribution. At some point, the CPU throttling increases, and then the performance breaks down.\\n\\n![all-namespaces-throughput](all-namespaces-throughput.png)\\n\\nInteresting was that on all JFR recordings (with and without PathPattern), I still saw the Spring filter chain take a big chunk of the profile. This is because the filter chain itself doesn\'t change with using a different pattern parser.\\n\\n![rest-base-v3-jfr.png](rest-base-v3-jfr.png)\\n![path-pattern-jfr.png](path-pattern-jfr.png)\\n![combination-of-all-jfr.png](combination-of-all-jfr.png)\\n\\nBased on the profile, we do not see much of a difference.\\n\\nToday, I will validate the following:\\n\\n * Is anti-affinity still enabled with our platform charts\\n * Combination of virtual threads (to reduce the thread count and blocking behavior), with PathPattern (as this was the most stable test)\\n * Maybe increasing the CPU limits again, to remove the K8 CPU throttling to better understand the system performance (until which the CPU consumption grows) and profile again\\n * Investigate further - likely try a different profiler like asyncProfiler\\n\\n### Anti-affinity\\n\\nSimply as a sanity check, I wanted to validate whether we still use our [anti-affinity configuration](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) in our charts. This is to make sure that brokers are not scheduled on the same node. Unfortunately, this only works on the namespace level.  \\n\\n\\nIndeed, we still have the configuration set:\\n\\n```yaml\\n    spec:\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchExpressions:\\n              - key: app.kubernetes.io/component\\n                operator: In\\n                values:\\n                - core\\n            topologyKey: kubernetes.io/hostname\\n```\\n\\nWhile this helps in the same namespace, this doesn\'t prevent to have brokers from different namespaces from being scheduled on the same node (AFAIK). Potential for a noisy neighbor. But this is also the reason why we use smaller nodes, and try to assign most of the resources to the corresponding broker pods (which makes them effectively alone on the node).\\n\\n### REST Base more CPU\\n\\nTo validate once more how the base (simply with REST API enabled) performs with more CPU, we have set up a test with 6 CPUs (request + limit). This is an increase of factor three (from 2 CPU to 6 CPU).\\n\\nIn general, the test was stable.\\n\\n![rest-base-more-cpu-general.png](rest-base-more-cpu-general.png)\\n![rest-base-more-cpu-latency.png](rest-base-more-cpu-latency.png)\\n\\nAs soon as we increased the CPU the throttling went down.\\n![rest-base-more-cpu-throttle.png](rest-base-more-cpu-throttle.png)\\n\\nThe consumption went up to 3 CPU, comparing to our gRPC benchmarks, this is an increase of factor two!\\n\\n![rest-base-more-cpu-usage.png](rest-base-more-cpu-usage.png)\\n\\n\\nWhile observing the test, we noticed some weird behavior of the workers. There are multiple regular job activation requests sent (while we still have Job Push enabled and in use). \\n\\n![rest-base-more-cpu-throughput.png](rest-base-more-cpu-throughput.png)\\n\\nThis is also causing to have much higher job COMPLETE command rate, where most of them are actually rejected. We see ~500 job completion rejections per second!\\n\\n![rest-base-more-cpu-logstream.png](rest-base-more-cpu-logstream.png)\\n\\nWhy we have this behavior is not yet fully clear. The load test is stabilizing at a later point and running straight for several days.\\n\\n![rest-base-more-cpu-longer-general.png](rest-base-more-cpu-longer-general.png)\\n\\nAt some-point it went into struggle again, as it run out of disk space. The exporter had a too big backlog, and was not able to catch up.\\n\\n![rest-base-more-cpu-exporting.png](rest-base-more-cpu-exporting.png)\\n\\nThis might be related to the huge number of commands and rejections that need to be skipped.\\n\\n### Combination of VT and PathPattern\\n\\nAs another experiment, we run a load test with enabling virtual threads and PathPattern parser on Spring.\\n\\nTo summarize, it doesn\'t help to reduce the CPU consumption to a level that the system can run stable.\\nAt the beginning, the worker was able to complete at least ~30 jobs per second, but later it fully stopped. \\n\\n![vt-pathpattern-cpu-general.png](vt-pathpattern-cpu-general.png)\\n\\n![vt-pathpattern-cpu-usage.png](vt-pathpattern-latency.png)\\n![vt-pathpattern-cpu-usage.png](vt-pathpattern-throughput.png)\\n\\n\\nIn our JFR recording, we see a similar pattern, where the Spring filtering is still taking most of the samples.\\n\\n![vt-pathpattern-cpu-jfr.png](vt-pathpattern-cpu-jfr.png)\\n\\nEnabling the virtual threads on spring, at least seem to remove the HTTP threads we normally had in our profiles.\\n\\nThe CPU throttling is rather high, causing the performance problems we see here.\\n\\n![vt-pathpattern-cpu-throttle.png](vt-pathpattern-cpu-throttle.png)\\n\\nZeebe-2 is often between 50-80% CPU throttling, as it is consuming 1.8 CPU (limit is 2).\\n\\n![vt-pathpattern-cpu-usage.png](vt-pathpattern-cpu-usage.png)\\n\\nThe workers stopped working at some point completely. Investigating this, we can see that it fails with some OOM as well.\\n\\n```shell\\nJul 05, 2025 07:16:48.330 [pool-4-thread-8] WARN  io.camunda.client.job.worker - Worker benchmark failed to handle job with key 2251799882041322 of type benchmark-task, sending fail command to broker\\njava.lang.IllegalStateException: Queue full\\nException in thread \\"prometheus-http-1-6\\" java.lang.OutOfMemoryError: Java heap space\\n\\n\\nJul 05, 2025 7:18:48 AM io.prometheus.metrics.exporter.httpserver.HttpExchangeAdapter sendErrorResponseWithStackTrace\\nSEVERE: The Prometheus metrics HTTPServer caught an Exception while trying to send the metrics response.\\njava.io.IOException: Broken pipe\\n\\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\\n\\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\\n\\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\\n\\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\\n\\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\\n\\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\\n\\tat jdk.httpserver/sun.net.httpserver.Request$WriteStream.write(Request.java:421)\\n\\tat jdk.httpserver/sun.net.httpserver.ChunkedOutputStream.writeChunk(ChunkedOutputStream.java:131)\\n\\tat jdk.httpserver/sun.net.httpserver.ChunkedOutputStream.flush(ChunkedOutputStream.java:165)\\n\\tat jdk.httpserver/sun.net.httpserver.ChunkedOutputStream.close(ChunkedOutputStream.java:140)\\n\\tat jdk.httpserver/sun.net.httpserver.PlaceholderOutputStream.close(ExchangeImpl.java:477)\\n\\tat java.base/java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:272)\\n\\tat io.prometheus.metrics.exporter.common.PrometheusScrapeHandler.handleRequest(PrometheusScrapeHandler.java:74)\\n\\tat io.prometheus.metrics.exporter.httpserver.MetricsHandler.handle(MetricsHandler.java:33)\\n\\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:98)\\n\\tat jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)\\n\\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:101)\\n\\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:871)\\n\\tat jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:98)\\n\\tat jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:847)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\n```\\n\\n### Async profiler\\n\\nTo enrich our insights and inputs (have more data to investigate), we tried to set up [async profiler](https://github.com/async-profiler/async-profiler) with our load tests.\\n\\nWe had some [out dated documentation](https://github.com/camunda/camunda/tree/main/zeebe/benchmarks/docs/debug#async-profiler) in our mono repository. Due to several refactorings, restructurings, etc. this guide was no longer working.\\n\\nI was able to create a script to set it up for now:\\n\\n```shell\\n#!/bin/bash -xeu\\n# Usage:\\n#   ./executeProfiling.sh <POD-NAME>\\nset -oxe pipefail\\n\\nnode=$1\\n\\n# Download and extract latest async profiler\\ncurl -L https://github.com/jvm-profiling-tools/async-profiler/releases/download/v4.0/async-profiler-4.0-linux-x64.tar.gz -o profiler.tar.gz\\ncat profiler.tar.gz | tar xzv \\n\\n# Copy async profiler to pod\\nkubectl cp async-profiler-4.0-linux-x64/bin/asprof \\"$node\\":/usr/local/camunda/data/asprof\\nkubectl exec \\"$node\\" -- mkdir -p /usr/local/camunda/data/lib\\nkubectl cp async-profiler-4.0-linux-x64/lib/libasyncProfiler.so \\"$node\\":/usr/local/camunda/data/libasyncProfiler.so\\nkubectl exec \\"$node\\" -- chmod +x /usr/local/camunda/data/asprof\\n\\n# Run profiling\\nfilename=flamegraph-$(date +%Y-%m-%d_%H-%M-%S).html\\nPID=$(kubectl exec \\"$node\\" -- jps | grep Standalone | cut -d \\" \\" -f 1)\\nkubectl exec \\"$node\\" -- ./data/asprof -e itimer -d 100 -t -f \\"/usr/local/camunda/data/$filename\\" --libpath /usr/local/camunda/data/libasyncProfiler.so \\"$PID\\"\\n\\n# Copy result\\nkubectl cp \\"$node:/usr/local/camunda/data/$filename\\" \\"$node-$filename\\"\\n```\\n\\nThe results need to be investigated next.\\n\\n### Follow-up questions\\n\\n1. Why are benchmark applications targeted at the same gateway? How does the IP resolution work with the headless service (which returns an array of IPs)? It looks like it is picking always the same gateway.\\n2. Why are the workers sending so often job activations, while the job push is active?\\n3. Why do we have 500+ job completions per second? Overloading the cluster?\\n\\n\\n## Day 4: Investigate profiles and experiments\\n\\nWe will continue with investigating certain areas of our REST API, checking profiles, and experimenting with ideas.\\n\\n### Combination with more CPU\\n\\nThe virtual threads and PathPattern parser setting test was combined with more CPUs (from 2 to 3 CPUs).\\n\\n![vt-pp-more-cpu-general](vt-pp-more-cpu-general.png)\\n![vt-pp-more-cpu-latency](vt-pp-more-cpu-latency.png)\\n\\nThe test is running stable, but needs to be further observed (as we have seen, they might fail at a later point in time).\\n\\n![vt-pp-cpu](vt-pp-cpu.png)\\n\\nThe CPU consumption and throttling looks rather stable.\\n\\n#### Running for a day\\n\\n![vt-pp-later-general.png](vt-pp-latern-general.png)\\n\\nThe test was running stably for a good amount of time, but suddenly broke down.\\nThe CPU usage increases heavily, causing throttling and breaking the system again, but this is just a symptom.\\n\\n![vt-pp-later-cpu.png](vt-pp-later-cpu.png)\\n\\n\\nWhen we investigate further the metrics, we can see that the latency, especially the commit latency, is increasing at the same time. \\n\\n![vt-pp-later-latency.png](vt-pp-later-latency.png)\\n\\nThis is likely because we might exhaust our available I/O. Indeed, we write or commit more at this point in time.\\n\\n![vt-pp-later-commit-rate.png](vt-pp-later-commit-rate.png)\\n\\nFurther investigation highlights that the rejections of JOB completions are spiking high\\n\\n![vt-pp-increase-of-rejections.png](vt-pp-increase-of-rejections.png)\\n\\nIt looks like that a new job stream has been started, at which time the cluster starts to go into a failure mode.\\n\\n![](vt-pp-later-stream-start.png)\\n\\nAdditionally, more jobs are pushed out to the clients, causing more to complete (duplicate), increasing the rejections.\\n![](vt-pp-later-more-push.png)\\n\\nWe can also see that the workers start to crash the loop, because they receive too many jobs.\\n![](vt-pp-later-worker-restart.png)\\n\\n### Investigating command distribution\\n\\nRunning all of these tests, I investigate several things, and realized that for all of these tests, there is always one Camunda pod doing more than the others. To me, it looks like our load is not even distributed.\\n\\n![gateway-cmd-distribution](gateway-cmd-distribution.png)\\n\\nDue to the imbalanc,e one pod is doing more than the others, this pod is sacrificing of CPU throttling.\\n\\n![gateway-cmd-distribution-cpu.png](gateway-cmd-distribution-cpu.png)\\n\\nI think the challenge we face here is related to our setup using [a headless service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) in our [Camunda Platform Helm Chart](https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform-8.8/templates/core/service.yaml).\\n\\nThis means we have a service deployed in K8 that is returning all IPs for all PODs when resolving. Likely, our client applications simply use the first IP they retrieve (instead of doing some more clever).\\nI think this would be something we should further investigate. Potential options are to have client load balancing (with the multiple IPs), use a different service, or \\nDeploy the standalone gateway (again) to better separate the concerns.\\n\\n\\n### Profiling with Async Profiler\\n\\nAs mentioned the other day, I have run the async profiler to get some more information/details from a different angle of the application execution.\\n\\nAgain, what we can see is that the web filter chaining is taking a big chunk of the samples.\\n\\n![async-profile-rest-more-cpu-filter.png](async-profile-rest-more-cpu-filter.png)\\n\\nFurthermore, logging is also a big part of the profile.\\n\\n![async-profile-rest-more-cpu-logging.png](async-profile-rest-more-cpu-logging.png)\\n\\nAt the time of profiling, we were retrieving a lot of errors from the Brokers, due to rejections, etc. (see above).\\n\\nWe can see that we repeatedly log exceptions with no message at all.\\n\\n![logging-null](logging-null.png)\\n![logs-repeating](logs-repeating.png)\\n\\n#### Usage metrics\\n\\nWhen I investigated this further and checked our logging, I saw that we wrote a LOT of usage metrics logs\\n\\n![usage-metrics-rest-logs.png](usage-metrics-rest-logs.png)\\n\\nBased on the metrics, the exporting of usage metrics seem to be correlating to the state size growing.\\n![usage-metrics-rest-state-size.png](usage-metrics-rest-test.png)\\n\\n![usage-metrics-rest-state-size.png](usage-metrics-rest-state-size.png)\\n\\nThis needs to be further clarified, whether this is expected.\\n\\n## Last day (experimenting)\\n\\n### Load balance\\n\\nAs discovered on the previous day, we are sending requests mainly to one node. This is because of the usage of a [headless service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) in our [Camunda Platform Helm Chart](https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform-8.8/templates/core/service.yaml).\\n\\nToday, I experimented with using a different service to access the Gateway API with the clients.\\n\\n<details>\\n<summary>Manifest</summary>\\n```yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  annotations:\\n    meta.helm.sh/release-name: ck-rest-baseload-balancer\\n    meta.helm.sh/release-namespace: ck-rest-baseload-balancer\\n  labels:\\n    app: camunda-platform\\n    app.kubernetes.io/component: gateway\\n    app.kubernetes.io/instance: ck-rest-baseload-balancer\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/name: camunda-platform\\n    app.kubernetes.io/part-of: camunda-platform\\n    app.kubernetes.io/version: ck-rest-base-more-cpu-cd459997\\n    helm.sh/chart: camunda-platform-13.0.0-alpha4.2\\n  name: ck-rest-baseload-balancer-core-gw\\n  namespace: ck-rest-baseload-balancer\\nspec:\\n  type: ClusterIP # <---- That is the important part\\n  ports:\\n  - name: http\\n    port: 8080\\n    protocol: TCP\\n    targetPort: 8080\\n  - name: internal\\n    port: 26502\\n    protocol: TCP\\n    targetPort: 26502\\n  - name: command\\n    port: 26501\\n    protocol: TCP\\n    targetPort: 26501\\n  - name: server\\n    port: 9600\\n    protocol: TCP\\n    targetPort: 9600\\n  - name: gateway\\n    port: 26500\\n    protocol: TCP\\n    targetPort: 26500\\n  selector:\\n    app: camunda-platform\\n    app.kubernetes.io/component: core\\n    app.kubernetes.io/instance: ck-rest-baseload-balancer\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/name: camunda-platform\\n    app.kubernetes.io/part-of: camunda-platform\\n```\\n</details>\\n\\nBy deploying this service and changing the client application deployments, we can see the effect directly.\\n\\n![service-differences-load.png](service-differences-load.png)\\n\\nThe load is well distributed, and the CPU is as well.\\n\\n![service-differences-cpu.png](service-differences-cpu.png)\\n\\nIn general the performance looks pretty stable and good.\\n![service-differences-general.png](service-differences-general.png)\\n\\n#### After the night\\n\\nAfter running it for a while, the cluster was still looking quite stable.\\n\\n![service-differences-general-later.png](service-differences-general-later.png)\\n![service-differences-cpu-later.png](service-differences-cpu-later.png)\\n\\n#### After lunch\\n\\n![service-differences-general-later2.png](service-differences-general-later2.png)\\n![service-differences-cpu-later2.png](service-differences-cpu-later2.png)\\n\\nThe general throughput looks still stable, even if the latency is much higher than for gRPC (was mostly ~3-5 ms).\\n\\n![service-differences-latency-later.png](service-differences-latency-later.png)\\n\\n#### Exporter runs into an issue\\n\\nObserving the cluster, we have detected that the disk usage is increasing over time.\\n\\n![service-differences-disk.png](service-differences-disk.png)\\n\\nThe reason seem to be that the exporter is not able to catch up. Something that needs to be investigated separately.\\n\\n![service-differences-exporter-not-catch-up.png](service-differences-exporter-not-catch-up.png)\\n\\n## Conclusion \\n\\n**Quint essence:** REST API is more CPU intense/heavy than gRPC.\\n\\nIn general, this is not surprising. The REST API (and incl. Spring) works completely differently and is not as optimized for performance as gRPC is.\\nWe can see this also in our latencies, which are twice+ higher even when we have enough resources available.\\n\\nWhen correctly and evenly distributing the load we are able to handle the expected load on the cluster. Of course, this goes just until a certain load (even higher load) until it dips (as the CPU is exhausted again).\\n\\nWith gRPC, the bad request distribution was not an issue, as the overhead is low and has less of an impact.\\n\\nThe same behavior we had when assigning more resources to the cluster. Indicating CPU as the bottleneck (the issue is parallelizable)\\nThe CPU consumption can\'t be pinpointed to one single thing, but multiple inefficiencies coming together.\\n\\n### Discovered issues:\\n\\nDuring the investigation, the following issues have been discovered, which we should look at (and ideally fix).\\n\\n* [Helm Chart 8.8 is using a headless service for the single Application](https://github.com/camunda/camunda-platform-helm/issues/3784)\\n* **REST API**\\n    * [Investigate and improve web filter chain - as this was the dominator in all our profiles](https://github.com/camunda/camunda/issues/35067)\\n    * [Sub-optimal logging in REST API v2](https://github.com/camunda/camunda/issues/35069)\\n    * [REST API response handling is running into contention](https://github.com/camunda/camunda/issues/35076)\\n* **Zeebe**\\n    * [Usage metrics heavily looping](https://github.com/camunda/camunda/issues/35071)\\n    * [Usage metrics records are divided unnecessary](https://github.com/camunda/camunda/issues/35072)\\n    * [Job push vicious circle](https://github.com/camunda/camunda/issues/35074)\\n    * [Camunda Exporter is not able to catch up on all partitions](https://github.com/camunda/camunda/issues/35080)\\n* Clients\\n    * [Client failure handling ends in Stackoverflow (causing the client to completelty stop working)](https://github.com/camunda/camunda/issues/34597)\\n    * [Workers and Starters are holding futures in a queue](https://github.com/camunda/camunda/issues/35077)"},{"id":"/2025/06/30/Performance-of-REST-API","metadata":{"permalink":"/zeebe-chaos/2025/06/30/Performance-of-REST-API","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-06-30-Performance-of-REST-API/index.md","source":"@site/blog/2025-06-30-Performance-of-REST-API/index.md","title":"Performance of REST API","description":"In today\'s Chaos day we wanted to experiment with the new REST API (v2) as a replacement for our previous used gRPC API.","date":"2025-06-30T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":6.2,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Performance of REST API","date":"2025-06-30T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Follow up REST API performance","permalink":"/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance"},"nextItem":{"title":"How does Zeebe behave with NFS","permalink":"/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS"}},"content":"In today\'s Chaos day we wanted to experiment with the new REST API (v2) as a replacement for our previous used gRPC API.\\n\\nPer default, our load tests make use of the gRPC, but as we want to make REST API the default and release this fully with 8.8, we want to make sure to test this accordingly in regard to reliability.\\n\\n**TL;DR;** We observed severe performance regression when using the REST API, even when job streaming is in use by the job workers (over gRPC). Our client seems to have a higher memory consumption, which caused some instabilities in our tests as well. With the new API, we lack certain observability, which makes it harder to dive into certain details. We should investigate this further and find potential bottlenecks and improvements. \\n\\n![general](general-overview.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment (Pt. 1)\\n\\nTo experiment with the REST API, we have to adjust our client applications to make use of the REST API. This is done by the following PR [#34527](https://github.com/camunda/camunda/pull/34527). We can take our normal benchmark/load tests where we run 150 PI/s, and enable the REST API usage. To make this possible, the charts have been adjusted by this PR [#269](https://github.com/camunda/zeebe-benchmark-helm/pull/269).\\n\\nAs a base to compare we can use our weekly benchmarks. We use gRPC here as the default in the client applications (starter + worker).\\nIn our weekly benchmarks, we can see that we are able to create and complete 150 process instances per second.\\n\\n![base-general](base-general.png)\\n\\nThe performance is stable, and we have low backpressure. As the process instances are quite simple (with one service task), the execution time is rather low with 0.2 seconds on average.\\n\\n![base-latency](base-latency.png)\\n\\n### Expected\\n\\nWhen using the REST API, we expect some more overhead (maybe ~10%), like serializing and sending data over the wire (as gRPC is optimized for it). In general, we expect a stable performing system.\\n\\n### Actual\\n\\nObserving the first experiment, we saw a degradation of performance **by more than 70%**. Additionally, we seem to have no metrics for the REST API requests. Backpressure seems to be zero, while we\'re not performing as expected.\\n\\n![exp1-general](exp1-general.png)\\n\\nThe process instance completion latency has been increased to above than one minute.\\n\\n![exp1-latency](exp1-latency.png)\\n\\nWe can observe with our metrics that job push is still in use, and the job workers get to work on the available jobs.\\n\\n![exp1-push](exp1-push.png)\\n\\nThe issue we are seeing is related to crash looping workers (which we can also see in the panels above about Pod restarts).\\n\\n```shell\\n$ kgpo\\nNAME                                                              READY   STATUS             RESTARTS         AGE\\nbenchmark-worker-5765dbfb55-2gckc                                 1/1     Running            34 (5m40s ago)   4h1m\\nbenchmark-worker-5765dbfb55-6mckn                                 1/1     Running            26 (69m ago)     3h46m\\nbenchmark-worker-5765dbfb55-qtrmm                                 0/1     CrashLoopBackOff   33 (4m20s ago)   4h1m\\nck-bojan-rest-benchmark-prometheus-elasticsearch-exporter-v7nqk   1/1     Running            0                3h46m\\nck-bojan-rest-benchmark-zeebe-0                                   1/1     Running            0                4h1m\\nck-bojan-rest-benchmark-zeebe-1                                   1/1     Running            0                107m\\nck-bojan-rest-benchmark-zeebe-2                                   1/1     Running            0                4h1m\\nelastic-0                                                         1/1     Running            0                3h45m\\nelastic-1                                                         1/1     Running            0                4h1m\\nelastic-2                                                         1/1     Running            0                4h1m\\nleader-balancer-29188095-9gdhd                                    0/1     Completed          0                8m39s\\nstarter-677bc5cb4-pr7xq                                           1/1     Running            0                3h46m\\n\\n```\\n\\nInvestigating the respective pods, we can see that they are failing because of OOM errors.\\n\\n```shell\\n    Last State:     Terminated\\n      Reason:       OOMKilled\\n      Exit Code:    137\\n      Started:      Mon, 30 Jun 2025 14:15:14 +0200\\n      Finished:     Mon, 30 Jun 2025 14:18:00 +0200\\n    Ready:          True\\n    Restart Count:  34\\n    Limits:\\n      cpu:     500m\\n      memory:  256Mi\\n    Requests:\\n      cpu:     500m\\n      memory:  256Mi\\n```\\n\\n### Result\\n\\nOur first experiment failed to validate our expectation of:\\n\\n> When using the REST API we expect some more overhead (maybe ~10%), like serializing and sending data over the wire (as gRPC is optimized for it). In general, we expect a stable performing system.\\n\\n  * :x: We were not able to prove that our load tests run with simply enabling the REST API with minimal impact\\n  * :x: The performance of the system was not stable.\\n  * :x: The workers, can\'t work with the same amount of memory they used before for gRPC.\\n\\n## Chaos Experiment (Pt. 2)\\n\\nTo validate whether our experiment would work with the REST API and the workers having more memory, we increase the resource usage of the workers.\\n\\n```shell\\n    State:          Running\\n      Started:      Mon, 30 Jun 2025 14:25:40 +0200\\n    Ready:          True\\n    Restart Count:  0\\n    Limits:\\n      cpu:     500m\\n      memory:  1Gi\\n    Requests:\\n      cpu:     500m\\n      memory:  1Gi\\n```\\n\\n## Expected\\n\\nWhen the client applications have enough resources the expected performance of the REST API usage should be minimally lower than with the gRPC API. The system should perform stable.\\n\\n\\n\\n## Actual\\n\\nAs soon as we configured the workers, and gave them more memory, they stopped to crash loop. Still we are not able to reache the same performance (not even close) as with our normal base (weekly benchmark).\\n\\n![exp2-general](exp2-general.png)\\n\\nThe performance (after 1430) looks more stale (less fluctuating), but still not well.\\n\\n![exp2-push](exp2-push.png)\\n\\nWe seem to push more jobs out, on a constant rate. Looking at the logstream metrics, we can see that we rejecting a lot of commands, especially COMPLETEs and FAILs.\\n\\n![exp2-log](exp2-log.png)\\n\\nWe see interesting warning logs by the workers:\\n\\n```shell\\n12:40:07.532 [pool-4-thread-2] WARN  io.camunda.client.job.worker - Worker benchmark failed to handle job with key 4503599643648545 of type benchmark-task, sending fail command to broker\\njava.lang.IllegalStateException: Queue full\\n\\tat java.base/java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:?]\\n\\tat java.base/java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:329) ~[?:?]\\n\\tat io.camunda.zeebe.Worker.lambda$handleJob$1(Worker.java:122) ~[classes/:?]\\n\\tat io.camunda.client.impl.worker.JobRunnableFactoryImpl.executeJob(JobRunnableFactoryImpl.java:45) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat io.camunda.client.impl.worker.JobRunnableFactoryImpl.lambda$create$0(JobRunnableFactoryImpl.java:40) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat io.camunda.client.impl.worker.BlockingExecutor.lambda$execute$0(BlockingExecutor.java:50) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]\\n\\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583) [?:?]\\n```\\n\\nI think it is not fully clear what the user should do with this. AFAIK, based on the implementation, it is also not how we expected it to behave, as we wanted to block in this case.\\n\\n![exp2-snapshot](exp2-snapshots.png)\\n\\nWe can see that even when we recover the workers, with more memory, we are not able to come back to a performing system. This is likely because we aggregated already quite some data, and are running in some weird timeout and completion/fail loops. This needs further investigation (follow-up).\\n\\n## Chaos Experiment (Pt. 3)\\n\\nWith our third experiment, we want to validate how our load tests perform with some clean state, and workers set up correctly.\\n\\n### Expected\\n\\nSee above.\\n\\n### Actual\\n\\nWith no previous data and stable workers, we seem to be able to reach higher throughput again.\\n\\n![exp3-general](exp3-general.png)\\n\\nThe latency looks fairly similar to our base (weekly) benchmarks. Here again, 99% of PIs need less than 0.25 seconds to complete.\\n\\n![exp3-latency](exp3-latency.png)\\n\\nAfter a while, the load tests seem to behave similarly to the previous ones, reporting several timeouts and completion rejections.\\n\\n![exp3-jobs](exp3-jobs-rejections.png)\\n\\nThis is degrading the performance completely. \\n\\n![exp3-general-drop](exp3-general-drop.png)\\n\\n\\n![exp3-latency-mess](exp3-latency-mess.png)\\n\\nOur workers seem to report similar issues as before, regarding having a full queue:\\n\\n```shell\\n13:25:14.684 [pool-4-thread-3] WARN  io.camunda.client.job.worker - Worker benchmark failed to handle job with key 4503599628992806 of type benchmark-task, sending fail command to broker\\njava.lang.IllegalStateException: Queue full\\n\\tat java.base/java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:?]\\n\\tat java.base/java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:329) ~[?:?]\\n\\tat io.camunda.zeebe.Worker.lambda$handleJob$1(Worker.java:122) ~[classes/:?]\\n\\tat io.camunda.client.impl.worker.JobRunnableFactoryImpl.executeJob(JobRunnableFactoryImpl.java:45) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat io.camunda.client.impl.worker.JobRunnableFactoryImpl.lambda$create$0(JobRunnableFactoryImpl.java:40) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat io.camunda.client.impl.worker.BlockingExecutor.lambda$execute$0(BlockingExecutor.java:50) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]\\n\\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?\\n```\\n\\n:x: Right now it seems that the usage of the REST API can impact the general performance of the system (even with the usage of the Job streaming in the workers). As of now it is not clear why, which we have to further investigate and clarify.\\n\\n## Found Bugs\\n\\nFollowing issues and follow ups have been noted down\\n\\n  * REST API usage affects highly general performance of the system\\n  * REST API observability is missing\\n  * Clients using the REST API have higher memory usage\\n  * Worker seem to fail with unexpected error/warning messages when receiving jobs."},{"id":"/2025/06/12/How-does-Zeebe-behave-with-NFS","metadata":{"permalink":"/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-06-12-How-does-Zeebe-behave-with-NFS/index.md","source":"@site/blog/2025-06-12-How-does-Zeebe-behave-with-NFS/index.md","title":"How does Zeebe behave with NFS","description":"This week, we (Lena, Nicolas, Roman, and I) held a workshop where we looked into how Zeebe behaves with network file storage (NFS).","date":"2025-06-12T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":12.845,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"How does Zeebe behave with NFS","date":"2025-06-12T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Performance of REST API","permalink":"/zeebe-chaos/2025/06/30/Performance-of-REST-API"},"nextItem":{"title":"Lower memory consumption of Camunda deployment","permalink":"/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment"}},"content":"This week, we ([Lena](https://github.com/lenaschoenburg), [Nicolas](https://github.com/npepinpe), [Roman](https://github.com/romansmirnov), and [I](https://github.com/ChrisKujawa)) held a workshop where we looked into how Zeebe behaves with network file storage (NFS).\\n\\nWe ran several experiments with NFS and Zeebe, and messing around with connectivity.\\n\\n**TL;DR;** We were able to show that NFS can handle certain connectivity issues, just causing Zeebe to process slower. IF we completely lose the connection to the NFS server, several issues can arise, like IOExceptions on flush (where RAFT goes into inactive mode) or SIGBUS errors on reading (like replay), causing the JVM to crash.\\n\\n\x3c!--truncate--\x3e\\n\\n## Setup\\n\\n> Note:\\n> \\n> You can skip this section if you\'re not interested in how we set up the NFS server\\n\\n\\nFor our experiments, we want to have a quick feedback loop and small blast radius, meaning avoiding using K8, or any other cloud services. The idea was to set up a NFS server via docker, and mess with the network, to cause NFS errors.\\n\\n### Run NFS Docker Container\\n\\nAfter a smaller research we were able [to find a project](https://github.com/normal-computing/docker-nfs-server), that provides us a NFS server docker image.\\n\\nThis can be run via: \\n\\n```shell\\nsudo podman run \\\\\\n   # Needs privileged access for setting up the exports rule, etc\\\\\\n  --privileged\\n  # Mounting a local directory as volume into the container\\n  -v /home/cqjawa/nfs-workshop/nfs:/mnt/data:rw  \\\\\\n   # expose the NFS por\\n  -p 2049:2049t \\\\\\n   # Allowing the local host IP to access the NFS server \\n  -e NFS_SERVER_ALLOWED_CLIENTS=10.88.0.0/12 \\\\\\n   # Enable DEBUG LOGS\\n  -e NFS_SERVER_DEBUG=1 \\\\ \\n   ghcr.io/normal-computing/nfs-server:latest\\n```\\n\\n### Mount the NFS to local file storage\\n\\nTo use the NFS server and make it available to our Zeebe container, we first have to mount it via the NFS client.\\n\\nThis can be done via:\\n\\n```shell\\nsudo mount -v -t nfs4 \\\\\\n  -o proto=tcp,port=2049,soft,timeo=10 \\\\\\n  localhost:/ \\\\\\n  ~/nfs-workshop/nfs-client-mount/\\n```\\n\\n* `-v` verbose\\n* `-t` file system type: tells the client to use NFS4 \\n* `-o` Options for the mount: `proto=tcp,port=2049,soft,timeo=10` \\n    * Protocol options, like transport via `tcp`, port to be used, [soft mount](https://kb.netapp.com/on-prem/ontap/da/NAS/NAS-KBs/What_are_the_differences_between_hard_mount_and_soft_mount) to make sure to retry on unavailability and not block, timeout after 10s\\n\\n### Run the Zeebe Container\\n\\nAfter we mounted the NFS to our local filesystem, we can start our Zeebe container. \\n\\n```shell\\n podman run -d \\\\\\n   -v /home/cqjawa/nfs-workshop/nfs-client-mount/:/usr/local/zeebe/data \\\\\\n   -p 26500:26500 \\\\\\n   -p 9600:9600 \\\\\\n   gcr.io/zeebe-io/zeebe:8.7.5-root\\n```\\n\\nThis is mounting our NFS mounted directory into the container as the data directory for the Zeebe container.\\n\\n### Running load\\n\\nFor simplicity, we used `zbctl` to start some load. As a first step, we had to deploy a process model.\\n\\n```shell\\n zbctl --insecure deploy one_task.bpmn \\n```\\n\\nThis was using the [one_task.bpmn](https://github.com/camunda/zeebe-chaos/blob/main/go-chaos/internal/bpmn/one_task.bpmn) from `go-chaos/`.\\n\\nCreating instances in a loop:\\n\\n```shell\\nwhile [[ true ]];\\ndo \\n    zbctl --insecure \\\\\\n    create instance 2251799813685250;\\n    sleep 5;\\ndone\\n```\\n\\nRunning worker:\\n\\n```shell\\n zbctl --insecure \\\\\\n   create worker \\"benchmark-task\\"  \\\\\\n   --handler \\"echo {\\\\\\"result\\\\\\":\\\\\\"Pong\\\\\\"}\\"\\n```\\n\\n## Chaos Experiment - Use iptables with containerized NFS Server\\n\\nWe wanted to disrupt the NFS connections with `iptables` and cause some errors. \\n\\n### Expected\\n\\nWe can drop packages with `iptables`, and we can observe errors in the Zeebe container logs.\\n\\n### Actual\\n\\nSetting up the following iptables rule should allow us to disrupt the NFS connection, but it didn\'t worked.\\n```shell\\nsudo iptables -A OUTPUT -p tcp --dport 2049 --sport 2049 -d localhost -j DROP\\n```\\n\\n\\nAt the end we were setting up a lots of different rules, but nothing seem to work. \\n\\n```shell\\nEvery 1.0s: sudo iptables -L -v                                                                                                             cq-p14s: Thu Jun 12 16:01:28 2025\\n\\nChain INPUT (policy ACCEPT 6090K packets, 11G bytes)\\n pkts bytes target     prot opt in     out     source               destination\\n    0     0 DROP       tcp  --  any    any     anywhere             cq-p14s              tcp dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             10.0.88.5            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             10.0.88.1            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             10.88.0.5            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             cq-p14s              tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             anywhere             tcp spt:nfs dpt:nfs\\n\\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\\n pkts bytes target     prot opt in     out     source               destination\\n\\nChain OUTPUT (policy ACCEPT 6182K packets, 22G bytes)\\n pkts bytes target     prot opt in     out     source               destination\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             0.0.0.0              tcp dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             cq-p14s              tcp dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             11.0.88.5            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             10.0.88.5            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             0.0.0.0              tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             10.0.88.1            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             10.88.0.5            tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             cq-p14s              tcp spt:nfs dpt:nfs\\n    0     0 DROP       tcp  --  any    any     anywhere             anywhere             tcp spt:nfs dpt:nfs\\n\\n```\\n\\nWe even suspended the NFS server, via `docker pause`. We were able to observe that data was still synced between directories.\\n\\nThis was some indication for us, that the kernel might do some magic behind the scenes, and the NFS server didn\'t worked as we expected it to.\\n\\n\\n## Chaos Experiment - Use iptables with an external NFS Server\\n\\nAs we were not able to disrupt the network, we thought it might make sense to externalize the NFS server (to a different host).\\n\\n### Setup external NFS \\n\\nWe followed [this guide](https://idroot.us/install-nfs-server-fedora-41), to set up a NFS server running on a different machine.\\n\\n### Mount external NFS\\n\\nThe mounting was quite similar to before, now using a different host\\n\\n```shell\\nsudo  mount -v -t nfs4 -o proto=tcp,port=2049,soft,timeo=10 192.168.24.110:/ ~/nfs-workshop/nfs-client-mount/\\n```\\n\\n### Run Zeebe Container\\n\\nThe same for running the Zeebe container.\\n\\n```shell\\npodman run -d -v /home/cqjawa/nfs-workshop/nfs-client-mount/srv/nfs/:/usr/local/zeebe/data -p 26500:26500 -p 9600:9600 gcr.io/zeebe-io/zeebe:8.7.5-root\\n```\\n\\n\\n### Expected\\n\\nWe were expecting some errors during processing and writing when the connection was completely dropped.\\n\\n### Actual\\n\\n\\nSimilar to previous `iptables` we dropped all outgoing packages for the port `2049` with the new destination.\\n\\n```shell\\nsudo iptables -A OUTPUT -p tcp --dport 2049 -d 192.168.24.110 -j DROP\\n```\\n\\n```shell\\nEvery 1.0s: sudo iptables -L -v                                                                                                            cq-p14s: Thu Jun 12 16:13:44 2025\\n\\nChain INPUT (policy ACCEPT 6211K packets, 11G bytes)\\n pkts bytes target     prot opt in     out     source               destination\\n\\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\\n pkts bytes target     prot opt in     out     source               destination\\n\\nChain OUTPUT (policy ACCEPT 6297K packets, 23G bytes)\\n pkts bytes target     prot opt in     out     source               destination\\n   35 2064K DROP       tcp  --  any    any     anywhere             192.168.24.110\\t tcp dpt:nfs\\n\\n```\\n\\nNow we were actually able to observe some errors. The clients were receiving `DEADLINE EXCEEDED` exceptions (starter and worker).\\n\\n```shell\\n2025/06/12 16:14:03 Failed to activate jobs for worker \'zbctl\': rpc error: code = DeadlineExceeded desc = context deadline exceeded\\nError: rpc error: code = DeadlineExceeded desc = context deadline exceeded\\nError: rpc error: code = DeadlineExceeded desc = context deadline exceeded\\nError: rpc error: code = DeadlineExceeded desc = stream terminated by RST_STREAM with error code: CANCEL\\nError: rpc error: code = DeadlineExceeded desc = context deadline exceeded\\n```\\n\\n\\nAfter some time running with the disconnected NFS server, Zeebe actually failed to flush\\n\\n```shell\\n[2025-06-12 09:02:00.819] [raft-server-0-1] [{actor-name=raft-server-1, actor-scheduler=Broker-0, partitionId=1, raft-role=LEADER}] ERROR\\n        io.atomix.raft.impl.RaftContext - An uncaught exception occurred, transition to inactive role\\njava.io.UncheckedIOException: java.io.IOException: Input/output error (msync with parameter MS_SYNC failed)\\n        at java.base/java.nio.MappedMemoryUtils.force(Unknown Source) ~[?:?]\\n        at java.base/java.nio.Buffer$2.force(Unknown Source) ~[?:?]\\n        at java.base/jdk.internal.misc.ScopedMemoryAccess.forceInternal(Unknown Source) ~[?:?]\\n        at java.base/jdk.internal.misc.ScopedMemoryAccess.force(Unknown Source) ~[?:?]\\n        at java.base/java.nio.MappedByteBuffer.force(Unknown Source) ~[?:?]\\n        at java.base/java.nio.MappedByteBuffer.force(Unknown Source) ~[?:?]\\n        at io.camunda.zeebe.journal.file.Segment.flush(Segment.java:125) ~[zeebe-journal-8.7.5.jar:8.7.5]\\n        at io.camunda.zeebe.journal.file.SegmentsFlusher.flush(SegmentsFlusher.java:58) ~[zeebe-journal-8.7.5.jar:8.7.5]\\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.flush(SegmentedJournalWriter.java:125) ~[zeebe-journal-8.7.5.jar:8.7.5]\\n        at io.camunda.zeebe.journal.file.SegmentedJournal.flush(SegmentedJournal.java:173) ~[zeebe-journal-8.7.5.jar:8.7.5]\\n        at io.atomix.raft.storage.log.RaftLogFlusher$DirectFlusher.flush(RaftLogFlusher.java:73) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\\n        at io.atomix.raft.storage.log.RaftLog.flush(RaftLog.java:196) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\\n        at io.atomix.raft.impl.RaftContext.setCommitIndex(RaftContext.java:538) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\\n        at io.atomix.raft.roles.LeaderAppender.appendEntries(LeaderAppender.java:560) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\\n        at io.atomix.raft.roles.LeaderRole.replicate(LeaderRole.java:740) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\\n        at io.atomix.raft.roles.LeaderRole.safeAppendEntry(LeaderRole.java:735) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\\n        at io.atomix.raft.roles.LeaderRole.lambda$appendEntry$15(LeaderRole.java:701) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\\n        at io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:178) ~[zeebe-atomix-utils-8.7.5.jar:8.7.5]\\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]\\n        at java.base/java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]\\n        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[?:?]\\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\\n        at java.base/java.lang.Thread.run(Unknown Source) [?:?]\\nCaused by: java.io.IOException: Input/output error (msync with parameter MS_SYNC failed)\\n        at java.base/java.nio.MappedMemoryUtils.force0(Native Method) ~[?:?]\\n        ... 24 more\\n```\\n\\nThis caused the RAFT Leader role to become inactive, and uninstalling all related services.\\n\\n```shell\\nINFO io.atomix.raft.impl.RaftContext - Transitioning to INACTIVE\\n```\\n\\nFurthermore, interesting is that the `DiskSpaceMonitor` was detecting OOD and pausing the stream processor.\\n```shell\\n[2025-06-12 09:02:00.795] [zb-actors-0] [{actor-name=DiskSpaceUsageMonitorActor, actor-scheduler=Broker-0}] WARN \\n        io.camunda.zeebe.broker.system - Out of disk space. Current available 0 bytes. Minimum needed 2147483648 bytes.\\n[2025-06-12 09:02:00.796] [zb-actors-0] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] WARN \\n        io.camunda.zeebe.broker.system - Disk space usage is above threshold. Pausing stream processor.\\n```\\n\\nAt the end, the system was not running anymore. This means availability was impacted, but not durability, as we do not write anything wrong (or do not continue with dirty data)\\n\\n## Chaos Experiment 3 - Random dropping packages\\n\\nIt is possible with `iptables` to randomly drop packages, allow to validate how the system behaves on certain package loss.\\n\\n### Expected\\n\\nWe expected that here the system might also fail, potentially, with some exceptions.\\n\\n### Actual\\n\\nRunning the following command sets up an `iptables` rule that drops random packets with `80%` probability for destination port `2049`\\n\\n```shell\\nsudo iptables -A OUTPUT -p tcp --dport 2049 -d 192.168.24.110 -m statistic --mode random --probability 0.80 -j DROP\\n```\\n\\nAs NFS is TCP based it seem to be that NFS can handle certain data/package loss, and is repeating the packages. \\n\\nThe general processing was much slower, this was observed by the rate of how many instances were created and jobs completed.\\n\\nOther than that the system continued to run healthy.\\n\\n## Chaos Experiment 4 - Drop connection on reading\\n\\nWe wanted to cause some SIGBUS errors, as we knew this can happen with mmapped files, like it is used in Zeebe. This might be reproduced on reading of memory mapped data. \\n\\nFor this we planned to create a lot of data on our Zeebe system and restarting it, causing Zeebe to fail on replay when the connection is blocked.\\n\\n### Expected \\n\\nWe expected that during read, we would cause a SIGBUS, causing the system to crash\\n\\n### Actual\\n\\nTo make sure we are creating continuous segments, and not compacting (causing longer replay) we increased the snapshot period and reduced the log segment size.\\n\\n```shell\\npodman run -d \\\\\\n  -v /home/cqjawa/nfs-workshop/nfs-client-mount/srv/nfs/:/usr/local/zeebe/data \\\\\\n  -p 26500:26500 -p 9600:9600 \\\\\\n  -e ZEEBE_BROKER_THREADS_CPUTHREADCOUNT=2 \\\\\\n  -e ZEEBE_BROKER_THREADS_IOTHREADCOUNT=2 \\\\\\n  -e ZEEBE_BROKER_DATA_LOGSEGMENTSIZE=16MB \\\\\\n  -e ZEEBE_BROKER_DATA_SNAPSHOTPERIOD=8h \\\\\\n  gcr.io/zeebe-io/zeebe:8.7.5-root\\n```\\n\\nFirst we set up an `iptable` rule to make sure that the reading was slower from NFS (by random dropping ~80% of packages).\\n\\n```shell\\nsudo iptables -A OUTPUT -p tcp --dport 2049 -d 192.168.24.110 -m statistic --mode random --probability 0.80 -j DROP\\n```\\n\\n```shell\\n[2025-06-12 09:25:00.543] [zb-actors-1] [{actor-name=StreamProcessor-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.processor - Processor starts replay of events. [snapshot-position: 611, replay-mode: PROCESSING]\\n```\\n\\nWhen we saw that the StreamProcessor was starting with replay we started to drop packages again completely.\\n\\n```shell\\nsudo iptables -A OUTPUT -p tcp --dport 2049 -d 192.168.24.110 -j DROP\\n```\\n\\nAfter a certain period of time, we ran into a SIGBUS Error\\n\\n```shell\\n[2025-06-12 09:25:00.543] [zb-actors-1] [{actor-name=StreamProcessor-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.processor - Processor starts replay of events. [snapshot-position: 611, replay-mode: PROCESSING]\\n[2025-06-12 09:25:00.545] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning CommandApiService\\n[2025-06-12 09:25:00.547] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning SnapshotDirector\\n[2025-06-12 09:25:00.549] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning ExporterDirector\\n[2025-06-12 09:25:00.555] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning BackupApiRequestHandler\\n[2025-06-12 09:25:00.557] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning Admin API\\n[2025-06-12 09:25:00.558] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 completed\\n[2025-06-12 09:25:00.561] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \\n\\tio.camunda.zeebe.broker.system - ZeebePartition-1 recovered, marking it as healthy\\n[2025-06-12 09:25:00.562] [zb-actors-1] [{actor-name=HealthCheckService, actor-scheduler=Broker-0}] INFO \\n\\tio.camunda.zeebe.broker.system - Partition-1 recovered, marking it as healthy\\n#\\n# A fatal error has been detected by the Java Runtime Environment:\\n#\\n#  SIGBUS (0x7) at pc=0x00007f89ec4601a5, pid=2, tid=49\\n#\\n# JRE version: OpenJDK Runtime Environment Temurin-21.0.7+6 (21.0.7+6) (build 21.0.7+6-LTS)\\n# Java VM: OpenJDK 64-Bit Server VM Temurin-21.0.7+6 (21.0.7+6-LTS, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\\n# Problematic frame:\\n# v  ~StubRoutines::updateBytesCRC32C 0x00007f89ec4601a5\\n#\\n# Core dump will be written. Default location: Core dumps may be processed with \\"/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %d\\" (or dumping to /usr/local/zeebe/core.2)\\n#\\n# An error report file with more information is saved as:\\n# /usr/local/zeebe/hs_err_pid2.log\\n[275.689s][warning][os] Loading hsdis library failed\\n#\\n# If you would like to submit a bug report, please visit:\\n#   https://github.com/adoptium/adoptium-support/issues\\n```\\n\\nThis caused the JVM to crash and stop the Docker container, as expected.\\n\\n## Results\\n\\nWith the workshop on experimenting with NFS, we got several learnings on how Zeebe and NFS behave on connectivity issues, summarized as follows:\\n\\n  * We could confirm that network errors lead to unrecoverable SIGBUS errors, which cause the broker to crash.\\n     * This is due primarily to our usage of mmap both in RocksDB and Zeebe.\\n     * There is an easy workaround with RocksDB where you can simply turn off mmap, but no such workaround exists in Zeebe at the moment.\\n     * This only impacts availability as the application crashes, but since Zeebe is designed to be crash resilient, so no inconsistencies or data corruption.\\n     * We don\u2019t have a clear idea of the frequency of these errors - it\u2019s essentially environment-based (i.e., how bad the network connectivity is).\\n * With only partial connectivity (simulated by dropping packets, e.g. 70% of packets), we mostly observed performance issues, as things got slower; however, messages were retried, so no errors occurred.\\n * Network errors when using normal file I/O resulted in IOException as expected. \\n     * This caused the Raft partition to go inactive, for example, when the leader fails to flush on commit (a known issue which is already planned to be fixed for graceful error handling).\\n * When the NFS server was unavailable, the disk space monitor detected that there was no more disk space available, and writes stopped.\\n * Did not test that it recovers when the server is back, but we expect it would.\\n * **Minor**, but we should open an issue for it:\\n     * When the leader goes inactive, we report an internal error that there is no message handler for command-api-1, but really we should be returning an UNAVAILABLE as a proper error, and not logging this as error level (we have other means to detect this).\\n\\nWhat does this mean?\\n\\n* We can say Zeebe can work with NFS, but it is not yet supported.\\n* We need to improve certain error handling, like flushing errors, to better support it.\\n* When operating Zeebe on bare-metal and having an unreliable environment SIGBUS might be more likely and crashin JVM be more problematic then using an K8 deployment, where pods automatically getting rescheduled"},{"id":"/2025/06/05/Lower-memory-consumption-of-Camunda-deployment","metadata":{"permalink":"/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-06-05-Lower-memory-consumption-of-Camunda-deployment/index.md","source":"@site/blog/2025-06-05-Lower-memory-consumption-of-Camunda-deployment/index.md","title":"Lower memory consumption of Camunda deployment","description":"I\'m back to finally do some load testing again.","date":"2025-06-05T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":8.795,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Lower memory consumption of Camunda deployment","date":"2025-06-05T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"How does Zeebe behave with NFS","permalink":"/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS"},"nextItem":{"title":"News from Camunda Exporter project","permalink":"/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project"}},"content":"I\'m back to finally do some load testing again. \\n\\nIn the past months, we have changed our architecture. This was to deploy instead all of our components as a separate deployment, \\nwe now have one single statefulset. This statefulset is running our single Camunda standalone application, \\ncombining all components together. \\n\\n![simpler deployment](simpler-deployment.png)\\n\\nMore details on this change we will share on a separate blog post. For simplicity, in our load tests (benchmark helm charts), we\\n combined all the resources we had split over multiple deployments together, see related PR [#213](https://github.com/camunda/zeebe-benchmark-helm/pull/213).\\n\\nWe are currently running our test with the following resources by default:\\n\\n```yaml\\n    Limits:\\n      cpu:     2\\n      memory:  12Gi\\n    Requests:\\n      cpu:      2\\n      memory:   6Gi\\n```\\n\\nIn today\'s Chaos day, I want to look into our resource consumption and whether we can reduce our used requests and limits.\\n\\n**TL;DR;** We have focused on experimenting with different memory resources, and were able to show that we can reduce the used memory by 75%, and our previous provisioned resources by more than 80% for our load tests.\\n\\n\x3c!--truncate--\x3e\\n\\n## Checking weekly benchmarks\\n\\nBefore I started to experiment and reduce it. I validated whether we actually have room for improvement. For that, I check our\\nweekly load tests. These are tests we start every week, that are running for four weeks straight. These can be used as a good reference point (base).\\n\\nI picked the mixed load test, which is running our realistic benchmark using a  more complex process model, covering more elements, etc.\\n\\n![base general](base-general.png)\\n\\nWhen we look at the general metrics, we can see it reaches, on average, ~100 task completions per second. As we use pre-emptive nodes, it might happen that workers, starters, or even the Camunda application are restarted in between.\\n\\n### Memory consumption\\n\\nLooking at the memory consumption, we can see that we still have quite some headroom.\\n\\n![base-memory](base-memory.png)\\n\\nOur deployments use between three and four gigabytes of memory, which is divided by the JVM heap, JVM metaspace, native memory usage like RocksDB, off-heap usage etc.\\n\\nFor example, we can see that the JVM uses less than one gigabyte for the heap, but can use up to ~3.5 gigabytes for its heap. This is related to the default JVM settings, which are ~25% of the available memory on the machine.\\n\\n![base-jvm-mem](base-jvm-mem.png)\\n\\n[RocksDB](https://rocksdb.org/), the embedded key-value store that Zeebe uses to store its state, is per default configured to use 512 MB per partition. We can observe via exposed metrics this as well.\\n\\n![base-rocksdb](base-rocksdb.png)\\n\\nWe can set this memory limit for RocksDB via an [experimental configuration](https://github.com/camunda/camunda/blob/main/zeebe/broker/src/main/java/io/camunda/zeebe/broker/system/configuration/RocksdbCfg.java#L23). For example, via environment variable: `ZEEBE_BROKER_EXPERIMENTAL_ROCKSDB_MEMORYLIMIT` or property: `zeebe.broker.experimental.rocksdb.memoryLimit`.\\n\\n### CPU Consumption\\n\\nAfter having checked the memory consumption, we can look at the CPU consumption. As mentioned earlier, we are running a rather more complex orchestration use case that involves more work on processing, exporting, etc. \\n\\nHere we can already see that we scratch on our limits, we can observe some throttling for some of our pods.\\n\\n![base-cpu](base-cpu.png)\\n\\nFor today, I will focus on the memory consumption to improve it. We might want to look into the CPU consumption on another day. \\n\\n## 1. Experiment: Reduce memory limits generally\\n\\nAs a first experiment, I tried to reduce the general memory to something which I thought made sense based on the observation I made earlier. This means setting requests and limits to four gigabytes. \\n\\n```yaml\\n    Limits:\\n      cpu:     2\\n      memory:  4Gi\\n    Requests:\\n      cpu:      2\\n      memory:   4Gi\\n```\\n\\n**This is a 66% decrease from the previous limit and a 33% decrease from the previous used requests!** \\n\\nBe aware that I set both to the same value on purpose. This is to make sizing, scheduling, and memory management more predictable. Furthermore, to reduce the chance of getting OOMs/killed/evicted.\\n\\nThe memory request is used for Kubernetes pod scheduling. This means the limit is not a guaranteed size, but more like a guard to prevent the container to use more. If a container uses more than its requests [there is a chance to be evicted](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run), if the node becomes memory pressure. If it exceeds its limits, it will be killed eventually by the kernel.\\n\\n### Expected\\n\\nAs we observed in our weekly load test, we are able to use less than 1 gigabyte of Java Heap, which is ~25% of four gigabytes, and we normally use three partitions, a 512 MB (~1,5 Gi). I expect that four gigabytes of memory should perform well.\\n\\n### Actual\\n\\nThe general performance is comparable to our base, we do not spot any frequent restarts.\\n\\n![exp1-general](exp1-general.png)\\n\\nLooking at the memory, we see that we are able to run with the reduced memory as well.\\n\\n![exp1-memory](exp1-memory.png)\\n\\nThe JVM memory usage even shows us that we are able to use less memory; previously, we used ~1 gig as heap, now it is around 256 MB.\\n\\n![exp1-jvm-mem](exp1-jvm.png)\\n\\n**This gives us room for further improvement. Let\'s continue with Experiment 2**\\n\\n## 2. Experiment: Reduce memory limits and RocksDB memory\\n\\nWith the results from Experiment 1, I was confident that we could run with less memory. I was wondering what if we reduced the memory limit of RocksDB.\\n\\nAs mentioned earlier can be done via a property or an environment variable. For our next experiment, I set our limit\\n to 128 MB. This is a 75% reduction of previous used memory for RocksDB per partition.\\n\\n```shell\\nzeebe.broker.experimental.rocksdb.memoryLimit: 128MB\\n```\\n\\nWe are running similar configurations in our SaaS environment, so I knew this is working, but I don\'t know how this behaves on a more complex use case and benchmark.\\n\\n### Expected\\n\\nMy expectation would be that the general memory consumption is reduced, not affecting the JVM. Our load test should run stable still.\\n\\n### Actual\\n\\nIndeed, the general performance looks similar, with some smaller outliers but still performing good.\\n\\n![exp2-general](exp1-general.png)\\n\\n\\nWe reduced the memory consumption for the process by half! It is now around 1.5 gigabytes, while it was in the previous experiment around three gigabytes, and at the start, close to four.\\n\\n![exp2-mem](exp2-mem.png)\\n\\n\\nIn our RocksDB related metrics, we are able to observe the actual size of our RocksDB instance as well, which is indeed 128 MB.\\n\\n![exp2-rocks](exp2-rocks.png)\\n\\n## 3. Experiment: Half it\\n\\nAs we were still running fine, and wanted to reach a point where it doesn\'t run well anymore. I simply thought about reducing our resources by half again.\\n\\nChanging our deployment resources to two gigabytes:\\n```yaml\\n    Limits:\\n      cpu:     2\\n      memory:  2Gi\\n    Requests:\\n      cpu:      2\\n      memory:   2Gi\\n\\n```\\n\\nConfiguring RocksDB memory limit to 64MB\\n```\\nzeebe.broker.experimental.rocksdb.memoryLimit: 64MB\\n```\\n\\n### Expected\\n\\nSimilar to above, I was still expecting that it works, as we saw that the JVM usage was rather low and still performing good.\\n\\n### Actual\\n\\nThe performance of the test still looks acceptable. We see some restarts, but they seem not to be related to memory pressure.\\n\\n![exp3-general](exp3-general.png)\\n\\nAgain, we were able to reduce the memory, but not with such big steps as before. For this load test, we have on average a ~1.2 G memory usage per pod.\\n\\n![exp3-mem](exp3-mem.png)\\n\\nWhen we look at the JVM metrics, we can see that we are getting closer to our maximum, commited, and used heap values. Still, the used heap was reduced and is now around ~128 MB in many cases.\\n\\n![exp3-jvm](exp3-jvm.png)\\n\\nThe RocksDB instance now uses 64MB as expected.\\n\\n![exp3-rocks](exp3-rocks.png)\\n\\n## 4. Experiment: Half it (again)\\n\\nAs I want to bring it to its limits, I reduced the memory resources once more by half.\\n```yaml\\n    Limits:\\n      cpu:     2\\n      memory:  1Gi\\n    Requests:\\n      cpu:      2\\n      memory:   1Gi\\n\\n```\\n\\nBut to not change too many things at once (sorry for doing it earlier :D), I kept the previous RocksDB configuration:\\n\\n```\\nzeebe.broker.experimental.rocksdb.memoryLimit: 64MB\\n```\\n\\n### Expected\\n\\nI felt that this might be quite low on its limits, but still expected it to work, looking at the JVM heap usage metrics.\\n\\n### Actual\\n\\nAs we can see, this was a tremendous fail. The pods were in an OOM loop and never became stable.  \\n\\n![exp4-general](exp4-general.png)\\n\\n![exp-mem](exp4-mem.png)\\n\\nWith this, we were able to find our limits.\\n\\n## 5. Experiment: Half RocksDb once more\\n\\nNot accepting the previous failure, I simply wanted to try out what happens when I reduce once more the RocksDB memory limit.\\n\\nThis means setting the limit to 32 MB.\\n```\\nzeebe.broker.experimental.rocksdb.memoryLimit: 32MB\\n```\\n\\n### Expected\\n\\nAt this point, this was really exploration, I had the feeling that it might help if we reduce a little the RocksDB memory.\\n\\n### Actual\\n\\nReducing the RocksDB memory limit allowed the Camunda application to perform as before! Without any performance impact :rocket: At the end we experienced a restart of all applications.\\n\\n![exp5-general](exp5-general.png)\\n\\nLooking at the process memory metrics, we can see that it is slowly increasing until it was OOM killed. This smells like a memory leak here.\\n\\n![exp5-mem](exp5-mem.png)\\n\\n```shell\\n   Last State:     Terminated\\n      Reason:       OOMKilled\\n      Exit Code:    137\\n      Started:      Wed, 04 Jun 2025 20:55:59 +0200\\n      Finished:     Thu, 05 Jun 2025 12:07:51 +0200\\n```\\n\\nThe JVM seem to perform correctly, and here we can observe any increasing usage. That indicates that there might be an issue with off heap (native) memory usage.\\n\\n![exp5-jvm](exp5-jvm.png)\\n\\nOur current native memory metrics don\'t highlight any specific ones either. While we can see that the metaspace uses a lot of space already, which also indicates that we likely can\'t reduce our memory usage more (except tuning this as well).\\n\\n![exp5-native](exp5-native.png)\\n\\nThe RocksDB memory usage looks stable as well.\\n\\n![exp5-rocks](exp5-rocks.png)\\n\\n\\n## Conclusion\\n\\nWith today\'s experiments and investigations, we were able to show that we are able to reduce our memory consumption.\\n\\nFrom previously used 12 Gi limit and 6 Gi request, we were able to show that it is running with 1 Gi limit and request, when we reduce the RocksDB memory limit as well. **This is an over 80-90% reduction for the assigned memory.** Looking at our usage, we showed that the actual process memory usage has been reduced from ~4 Gi to 1 Gi, that is a **75% reduction**!\\n\\nTo reduce the chance of getting OOM more frequently (until we investigated the potential resource leak), I propose to use 2 Gi as limits and requests and 64 MB RocksDb memory limit, which was running stable as well (see [Experiment 3](index.md#3-experiment--half-it)). This showed a memory usage of around ~1.2 Gi, which is still a **70% reduction** to previously, and ~70-80% reduction of assigned resources.\\n\\nWe can say this Chaos Day was a success, and I\'m looking forward to the next one :rocket: \\n\\n## Found Bugs\\n\\n * Several panels were broken related to memory, and their tooltip and legends. I fixed this during the investigation.\\n * Potential native memory leak"},{"id":"/2024/12/12/News-from-Camunda-Exporter-project","metadata":{"permalink":"/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-12-12-News-from-Camunda-Exporter-project/index.md","source":"@site/blog/2024-12-12-News-from-Camunda-Exporter-project/index.md","title":"News from Camunda Exporter project","description":"In this Chaos day, we want to verify the current state of the exporter project and run benchmarks with it. Comparing","date":"2024-12-12T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.65,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"News from Camunda Exporter project","date":"2024-12-12T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Lower memory consumption of Camunda deployment","permalink":"/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment"},"nextItem":{"title":"Impact of Camunda Exporter on processing performance","permalink":"/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance"}},"content":"In this Chaos day, we want to verify the current state of the exporter project and run benchmarks with it. Comparing\\nwith a previous version (v8.6.6) should give us a good hint on the current state and potential improvements.\\n\\n**TL;DR;** The latency of user data availability has improved due to our architecture change, but we still need to fix some bugs before our planned release of the Camunda Exporter. This experiment allows us to detect three new bugs, fixing this should allow us to make the system more stable.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n### Benchmarks\\n\\nWe have seen in previous experiments and benchmarks that the realistic benchmarks are not yet totally reliable, as they seem to overload at some point the system. This can happen if there is a hiccup and jobs take longer to process. Jobs in the queue are getting delayed, and time out, they are sent out to different workers, but we will reach them at some point again the jobs, and we will publish also for this job a message. This in general increases the load of the system as we have to timeout jobs, we have to handle additional message publish, etc.\\n\\nAdditionally, message publish can be rejected, when this happens we wait for another timeout adding again load on the system, more and more retries happen, etc. This breaks the benchmark performance.\\n\\nTo avoid this, we reduce the benchmark payload for now, which is in charge of creating multiple instances and call activities, etc. To be specific, they reduced the items from 50 to 5\\nbut scaled the starter to start more instances. With this payload, we can scale more fine granular. Each instance can create 5 sub-instances, when creating three process instances  we create effectively 15 instances/token.\\n\\nAs this benchmark runs quite stable, it allows us to better compare the latency between base and main.\\n\\n### Details Experiment\\n\\nWe will run two benchmarks one against 8.6.6, call based, and one against the current main branch (commit a1609130).\\n\\n### Expected\\n\\nWhen running the base and the main and comparing each other we expect that the general throughput should be similar.\\nFurthermore, we expect that the latency until the user sees data (or data is written into ES and searchable) should be lowered on the main branch rather than on the base.\\n\\nNote: Right now we don\'t have a good metric to measure that data is available for the user, we plan to implement this in the starter benchmark application at some point via querying the REST API. For now, we calculate different average latencies together, whereas we take as elastic search flush a constant of 2 seconds.\\n\\nWe expect a reduction of latency as we reduce one additional hop/usage of ES as intermediate storage, before aggregation.\\n\\n#### Base\\n\\n![current-8.6](../2024-10-24-Camunda-Exporter-MVP/current-miro.png)\\n\\n#### Main\\n\\n![main-target](./target.png)\\n\\n### Actual\\n\\nWe have set up both benchmarks, running as described above with changed payloads.\\n\\n#### General Performance\\n\\nThe general throughput performance looks similar. The resource consumption looks similar as well, but we didn\'t investigate this more deeply. Will be done separate.\\n\\n##### Base general\\n\\n![base-general](base-general.png)\\n\\n##### Main general\\n![main-general](main-general.png)\\n\\n#### Latency\\n\\nThis experiment aims to show the difference in the data availability for the user.\\n\\nIn order to better visualize the dashboard has been adjusted for this experiment.\\n\\n##### Base latency\\n\\n![base-latency](base-latencies-tree.png)\\n\\n##### Main latency\\n\\nAs we expected we were able to reduce the latency data is available for the user by the additional ES flush, reducing it by ~2 seconds.\\n\\n![main-latency](main-latencies-tree.png)\\n\\n### Result\\n\\nWe were able to show that the latency has been reduced under normal load.\\n\\n**Note:** Be aware this experiment only runs benchmarks with less-to-normal load, on higher load this might change, and need to be tested separately.\\n\\n#### Found Bugs\\n\\nWithin the experiment, we run into several other issues. Especially after running for a while, when pods got restarted and importer have been enabled, the Camunda Exporter broke.\\n\\n\\n![exporting-fail](exporting-fail.png)\\n\\nThis caused to increase in the latency.\\n\\n![exporting-fail-latency](exporting-fail-latency.png)\\n\\nThe exporter was not able to detect correctly anymore that the importing was done but was still flushing periodically (which is as well wrong)\\n\\nSee related GitHub issue(s)\\n\\n * [Importer(s) are not communicating import done correctly](https://github.com/camunda/camunda/issues/26046)\\n * [Exporter flushes periodically even when importer not completed](https://github.com/camunda/camunda/issues/26047) \\n\\nFurthermore, based on logs we saw that the treePath hasn\'t been published correctly in the Exporter.\\n \\n * [Camunda Exporter is not able to consume treePath](https://github.com/camunda/camunda/issues/26048)"},{"id":"/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance","metadata":{"permalink":"/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-11-14-Impact-of-Camunda-Exporter-on-processing-performance/index.md","source":"@site/blog/2024-11-14-Impact-of-Camunda-Exporter-on-processing-performance/index.md","title":"Impact of Camunda Exporter on processing performance","description":"In our last Chaos day we experimented with the Camunda Exporter MVP. After our MVP we continued with Iteration 2, where we migrated the Archiver deployments and added a new Migration component (allows us to harmonize indices).","date":"2024-11-14T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":4.145,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Impact of Camunda Exporter on processing performance","date":"2024-11-14T00:00:00.000Z","categories":["chaos_experiment","performance"],"tags":["performance"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"News from Camunda Exporter project","permalink":"/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project"},"nextItem":{"title":"Camunda Exporter MVP","permalink":"/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP"}},"content":"In our [last Chaos day](../2024-10-24-Camunda-Exporter-MVP/index.md) we experimented with the Camunda Exporter MVP. After our MVP we continued with Iteration 2, where we migrated the Archiver deployments and added a new Migration component (allows us to harmonize indices).\\n\\n![](it2-migration.png)\\n\\nAdditionally, [some fixes and improvements](https://github.com/camunda/zeebe-benchmark-helm/pull/202) have been done to the realistic benchmarks that should allow us to better compare the general performance with a realistic good performing benchmark.\\n\\nActually, this is what we want to explore and experiment with today.\\n\\n* Does the Camunda Exporter (since the last benchmark) impact performance of the overall system? \\n  * If so how? \\n* How can we potentially mitigate this?\\n\\n**TL;DR;** Today\'s, results showed that enabling the Camunda Exporter causes a 25% processing throughput drop. We identified the CPU as a bottleneck. It seems to be mitigated by either adjusting the CPU requests or removing the ES exporter. With these results, we are equipped to make further investigations and decisions.  \\n\\n\x3c!--truncate--\x3e\\n\\n## Benchmarks\\n\\nAs in the [last Chaos day](../2024-10-24-Camunda-Exporter-MVP/index.md) we use the new realistic benchmarks, that contain a much more complex process model and workload.\\nWe recently found some smaller issues in our benchmarks, related to [CPU throttling](https://github.com/camunda/zeebe-benchmark-helm/pull/204) and [undersized workers](https://github.com/camunda/zeebe-benchmark-helm/pull/202), these issues have been fixed. This allowed us to reach a much better workload/throughput on our weekly benchmarks, which we take here as a base for our comparison.\\n\\nThe newest benchmark helm charts have been updated to the first [Camunda Platform alpha1](https://github.com/camunda/zeebe-benchmark-helm/releases/tag/zeebe-benchmark-0.3.8), which includes the Camunda Exporter.\\n\\nToday we run the following benchmarks\\n\\n * Use Camunda Exporter, with disabled Importer in our benchmark\\n * Use Camunda Exporter, with disabled Importer and disabled ES exporter\\n * Use Camunda Exporter, with disabled Importer and higher CPU on brokers\\n\\n### Benchmark: Base\\n\\nAs we can see we can have a healthy cluster with a stable load where we reach to complete ~50 process instances, with that ~100 tasks, per second. All of this with a low backpressure. \\n\\n![general](base-general.png)\\n\\n\\n### Benchmark: Camunda Exporter\\n\\nWhen running our benchmarks with the Camunda Exporter the first thing we can observe is that the backpressure is much higher and the throughput went down by ~25-30%. We are now able to complete ~36 process instances, meaning 72 tasks, per second.\\n\\n![](it2-exporter-general.png)\\n\\n#### Latency\\n\\nLooking at the processing latency we can observe a significant increase\\n\\n##### Base\\n\\n![](base-latency.png)\\n![](base-latency2.png)\\n\\n##### Camunda Exporter\\n\\nThe process instance execution p99 has been increased from ~4s to +60s, the p50 went from ~0,5s to ~3,7s.\\n\\n![](it2-exporter-latency.png)\\n![](it2-exporter-latency2.png)\\n\\n#### CPU\\n\\nInvesting this, we can look at the CPU. On our base Benchmark, we have CPU throttling at around 20%.\\n\\n![cpu](base-cpu.png)\\n\\nWhen comparing this with the Camunda Exporter benchmark, we can see that the CPU throttling went up to 80%. The benchmark is close to its limits.\\n\\n![](it2-exporter-cpu.png)\\n\\n\\n### Benchmark: Without ES exporter\\n\\nAs we have seen the Camunda Exporter, causes the Brokers to consume a lot more CPU. This is kind of  expected as there is much more running now in our system.\\n\\nAs an additional experiment, we want to run the Benchmarks with the Camunda Exporter, without the Elasticsearch exporter. The hypothesis is that we can reduce the resource consumption and use it for the Camunda Exporter. The Elasticsearch exporter is with 8.7, only necessary for Optimize.\\n\\n![](no-es-general.png)\\n\\nAfter setting up the benchmark we can observe that the throughput went back to normal.\\n\\n#### Latency\\n\\nThe latency is reduced, and we can also observe that it seems to drop over time as well.\\n\\n![](no-es-latency.png)\\n![](no-es-latency2.png)\\n\\n#### CPU\\n\\nThe CPU throttling is dropping at some point, which explains the other drop of latency. \\n\\n![](no-es-cpu.png)\\n\\n### Benchmark: More CPU\\n\\nAs we\'re migrating logic from the actual Importer deployment to the Camunda Exporter, we can get rid of such extra deployment and bound resources. Arguably we can use these free resources and assign them to the brokers.\\n\\nWhen we look at the Camunda Exporter benchmark, the Operate deployment itself doesn\'t use many resources and likely don\'t need the assigned ones.\\n\\n![](it2-exporter-operate-cpu.png)\\n\\n![](change-resources.png)\\n\\nThis change allows us to bring the throughput as well back to normal.\\n\\n![](more-cpu-general.png)\\n\\n#### Latency\\n\\nThe latency is similar to our base benchmark.\\n\\n![](more-cpu-latency.png)\\n![](more-cpu-latency2.png)\\n\\n#### CPU\\n\\nThe CPU throttling has been reduced to almost zero. Interesting is that we don\'t use much more CPU resources (just slightly more, before ~1350m now ~1450 CPU). Increasing our requests by a little, allowed us to remove the CPU throttling. This is something we likely want to investigate further. \\n\\n![](more-cpu-cpu.png)\\n\\n### Results\\n\\nAs we have seen, introducing (or enabling) the Camunda Exporter, can or will increase our processing latency and reduce our potential processing throughput. This obviously depends on the cluster load.\\n\\nWe were able to pinpoint the problem due to limited resources, to be specific CPU is the bottleneck.\\n\\nThis is expected, as running the Camunda Exporter means we are running more logic inside the Zeebe system. \\n\\nWe can mitigate this with:\\n\\n* reducing load from the system, via disabling the additional ES exporter\\n* give the system more resources"},{"id":"/2024/10/24/Camunda-Exporter-MVP","metadata":{"permalink":"/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-10-24-Camunda-Exporter-MVP/index.md","source":"@site/blog/2024-10-24-Camunda-Exporter-MVP/index.md","title":"Camunda Exporter MVP","description":"After a long pause, I come back with an interesting topic to share and experiment with. Right now we are re-architecture","date":"2024-10-24T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":6.15,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Camunda Exporter MVP","date":"2024-10-24T00:00:00.000Z","categories":["camunda","exporter"],"tags":["performance"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Impact of Camunda Exporter on processing performance","permalink":"/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance"},"nextItem":{"title":"Optimizing cluster sizing using a real world benchmark","permalink":"/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark"}},"content":"After a long pause, I come back with an interesting topic to share and experiment with. Right now we are re-architecture\\nCamunda 8. One important part (which I\'m contributing to) is to get rid of Webapps Importer/Archivers and move \\ndata aggregation closer to the engine (inside a Zeebe Exporter).\\n\\nToday, I want to experiment with the first increment/iteration of our so-called MVP. The MVP targets green field installations where you simply deploy Camunda (with a new Camunda Exporter enabled) without Importers.\\n\\n**TL;DR;** All our experiments were successful. The MVP is a success, and we are looking forward to further improvements and additions. Next stop Iteration 2: Adding Archiving historic data and preparing for data migration (and polishing MVP).\\n\\n## Camunda Exporter\\n\\nThe [Camunda Exporter project](https://github.com/camunda/product-hub/issues/2128) deserves a complete own blog post, here is just a short summary.\\n\\nOur current Camunda architecture looks something like this (simplified).\\n\\n![current](current-miro.png)\\n\\nIt has certain challenges, like:\\n\\n* Space: duplication of data in ES\\n* Maintenance: duplication of importer and archiver logic\\n* Performance: Round trip (delay) of data visible to the user\\n* Complexity: installation and operational complexity (we need separate pods to deploy)\\n* Scalability: The Importer is not scalable in the same way as Zeebe or brokers (and workload) are.\\n\\nThese challenges we obviously wanted to overcome and the plan (as mentioned earlier) is to get rid of the need of separate importers and archivers (and in general to have separate application; but this is a different topic).\\n\\nThe plan for this project looks something like this:\\n\\n![plan](how-brown-field.png)\\n\\nWe plan to:\\n\\n1. Harmonize the existing indices stored in Elasticsearch/Opensearch\\n    - Space: Reduce the unnecessary data duplication  \\n2. Move importer and archiver logic into a new Camunda exporter\\n    - Performance: This should allow us to reduce one additional hop (as we don\'t need to use ES/OS as a queue)\\n    - Maintenance: Indices and business logic is maintained in one place\\n    - Scalability: With this approach, we can scale with partitions, as Camunda Exporters are executed for each partition separately (soon partition scaling will be introduced)\\n    - Complexity: The Camunda Exporter will be built-in and shipped with Zeebe/Camunda 8. No additional pod/application is needed.\\n\\nNote: Optimize is right now out of scope (due to time), but will later be part of this as well.\\n\\n\\n### MVP\\n\\nAfter we know what we want to achieve what is the Minimum viable product (MVP)?\\n\\nWe have divided the Camunda Exporter in 3-4 iterations. You can see and read more about this [here](https://github.com/camunda/issues/issues/803).\\n\\nThe first iteration contains the MVP (the first breakthrough). Providing the Camunda Exporter with the basic functionality ported from the Operate and Tasklist importers, writing into harmonized indices. \\n\\nThe MVP is targeting green field installations (clean installations) of Camunda 8 with Camunda Exporter without running the old Importer (no data migration yet),\\n\\n![mvp](it1-mvp.png)\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWhat I want to verify today, when I deploy the Camunda 8 stack with Camunda Exporter (and Importer disabled):\\n\\n  * Are webapps schemas created in ES, by the new Camunda Exporter\\n  * Is data exported into the indices\\n  * Can Operate show data? (right now just checking for basic functionality) \\n\\nAdditionally, I would like to understand what the performance looks like, how the system behaves with two ES exporters (the old ES exporter and the new Camunda Exporter), and more.\\n\\nFor our experiment, I use a [newly defined realistic benchmark](https://github.com/camunda/camunda/issues/21472) (with a more complex process model). More about this in a separate blog post. \\n\\n### Expected\\n\\nI can deploy the newest helm charts (alpha stage), by disabling Importer manually, and will be able to use Zeebe and Operate together. See the verifications above.\\n\\n### Actual\\n\\nAs always we use our [benchmark-helm charts](https://github.com/camunda/zeebe-benchmark-helm) (that building on top of our [Camunda Platform Helm](https://github.com/camunda/camunda-platform-helm) charts).\\n\\n\\n### Installation\\n\\nI had to adjust our benchmarks to [use the alpha snapshots ](https://github.com/camunda/zeebe-benchmark-helm/commit/db682a89788d6c511083ec743c6cf7d358155e3c) \\n\\n```yaml\\ndependencies:\\n  - name: camunda-platform\\n    repository: \\"oci://ghcr.io/camunda/helm\\"\\n    version: \\"0.0.0-snapshot-alpha\\"\\n    condition: \\"camunda.enabled\\"\\n```\\n\\n\\nand [disable the Importer via ENV](https://github.com/camunda/zeebe-benchmark-helm/commit/aafac6e9ec78e9cfd2e59a5b6f30bf887a4fcbd0)\\n\\n```yaml\\nenv:\\n- name: CAMUNDA_OPERATE_IMPORTERENABLED\\nvalue: \\"false\\"\\n```\\n\\nWith that, we can install our chart:\\n\\n```shell\\n$ helm install zeebe-benchmark-test charts/zeebe-benchmark/ --render-subchart-notes -f charts/zeebe-benchmark/values-realistic-benchmark.yaml --set global.elasticsearch.prefix=null\\n```\\n\\n### Basic First Verification\\n\\nAfter our benchmark chart is deployed we can already see the first time our Camunda Exporter running :tada:\\n\\n![firsttime](first-time-seeing-camunda-exporter.png)\\n\\n\\nWorth mentioning that the Camunda Export already comes with some metrics, visible on our Zeebe Dashboard\\n\\n![metrics](mvp-c8-exporter-metrics.png)\\n![metrics2](mvp-c8-exporter-metrics2.png)\\n\\n\\nThe general overview also looks good. No obvious problem.\\n\\n![general](mvp-general-overview.png)\\n\\nLooking into logs we can see that at the start it fails temporarily because ES is not yet ready to accept the schema creation.\\n\\n```\\nERROR - Failed to open exporter \'CamundaExporter\'. Retrying...\\n```\\n\\n![log](exporter-opened.png)\\n\\nAt some point, the exporter can be opened and the loop stops. \\n\\nI think generally it shouldn\'t be an ERROR but more a WARN (but these are details we can fix). Follow-up.\\n\\n\\n### Verify Operate Data\\n\\nTo make sure that Operate is not importing, I checked the Operate dashboard. We can see that there is no Importer metrics. Furthermore, in the configuration and logs we see no indication of importing.\\n\\n![op-metrics](no-importer-metrics.png)\\n\\n\\nWe can now start to port-forward to operate:\\n\\n```shell\\nk port-forward svc/zeebe-benchmark-test-operate 8081:80\\n```\\n\\nWhen opening Operate we see unfortunately no data.\\n\\n![operate-no-data](mvp-no-data-operate.png)\\n\\n#### Investigating missing data\\n\\nWe need to understand why there is no data available for Operate.\\n\\nWhat we saw is that the Camunda Exporter is open (logs), that it is also makes progress and data is written to elastic (metrics). What we haven\'t checked Elasticsearch in detail. \\n\\nLooking into ES dashboard we can see that indices are created, but the Operate indices seem to be empty.\\n\\n![es-indices](mvp-operate-indices-empty.png)\\n\\nWhen checking the Zeebe indices:\\n\\n![zeebe-indices](mvp-zeebe-indices-filled.png)\\n\\nwe can see that they are filled. An attentive reader will also chekc that there actuall some prefix problem in the indices. \\n\\nThanks to Deepthi which spotted this as well (and told me), we were exporting to the wrong index names. There was a [bug](https://github.com/camunda/camunda-platform-helm/blob/46f6ee9d828439b0b1cf37bae4d135ba5281a832/charts/camunda-platform-alpha/templates/zeebe/configmap.yaml#L66) existing in the current alpha Helm chart version.\\n\\n![wrong-prefix](mvp-wrong-prefix.png)\\n\\nThis has been fixed with [PR-2506](https://github.com/camunda/camunda-platform-helm/pull/2506). Until this gets merged I changed this manually via:\\n\\n```shell\\n# Get the templates \\nhelm template zeebe-benchmark-test charts/zeebe-benchmark/ --render-subchart-notes -f charts/zeebe-benchmark/values-realistic-benchmark.yaml --output-dir templates\\n\\n# Adjust the config map - remove the prefix\\nvim templates/zeebe-benchmark/charts/camunda-platform/templates/zeebe/configmap.yaml \\n\\n# Apply all manifests\\nk apply -f . --recursive\\n```\\n\\n> **Note:**\\n> \\n> I also tried \\n> ```shell\\n> helm template charts/zeebe-benchmark/ --version 0.0.0-snapshot-alpha     --show-only charts/camunda-platform/templates/zeebe/configmap.yaml --set global.elasticsearch.prefix=null\\n> ```\\n> But this breaks the ES exporter.\\n\\nWith this change we were can see that indices are correctly created and filled!\\n\\n![indices-filled](mvp-fixed-prefix-indices.png)\\n\\nFinally, we are able to see data in Operate! :rocket: **WITHOUT ANY IMPORTER.** \\n\\n![mvp-operate-data.png](mvp-operate-data.png)\\n![mvp-operate-instance.png](mvp-operate-instance.png)\\n![mvp-operate-pi.png](mvp-operate-pi.png)\\n![operate-overview](mvp-decisions.png)\\n\\n\\n## Conclusion\\n\\nThe MVP is a success. We were able to provide a Camunda Exporter that creates the necessary harmonized schema and migrate the basic business logic from Operate and Tasklist into the exporter. This allows us to use only the Camunda Exporter without running any Importer pod/application. \\n\\nGreat work Team :rocket: :tada: \\n\\n**Next stop:** \\n\\n_Iteration 2:_\\n\\n * Implementing migration logic for old data\\n * Moving Archiver logic (for historical data) into the Exporter\\n * Polish MVP state (add some missing features like TreePath, etc.)\\n\\n### Additional notes\\n\\nThis time I was not able to deep dive into performance or stability for this change. I plan to do this next.\\n\\n### Found Bugs\\n\\n* ERROR log level for logs that are transitive\\n* Auth/User indices are still prefixed with identity"},{"id":"/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark","metadata":{"permalink":"/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-10-14-Optimizing-cluster-sizing-using-a-real-world-benchmark/index.md","source":"@site/blog/2024-10-14-Optimizing-cluster-sizing-using-a-real-world-benchmark/index.md","title":"Optimizing cluster sizing using a real world benchmark","description":"Our first goal of this experiment is to use a benchmarks to","date":"2024-10-14T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":6.125,"hasTruncateMarker":false,"authors":[{"name":"Rodrigo Lopes","title":"Associate Software Engineer @ Zeebe","url":"https://github.com/rodrigo-lourenco-lopes","imageURL":"https://github.com/rodrigo-lourenco-lopes.png","key":"rodrigo","page":null}],"frontMatter":{"layout":"posts","title":"Optimizing cluster sizing using a real world benchmark","date":"2024-10-14T00:00:00.000Z","categories":["chaos_experiment","performance"],"tags":["performance"],"authors":"rodrigo"},"unlisted":false,"prevItem":{"title":"Camunda Exporter MVP","permalink":"/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP"},"nextItem":{"title":"Improve Operate import latency","permalink":"/zeebe-chaos/2024/08/19/Operate-improve-import-latency"}},"content":"Our first goal of this experiment is to use a benchmarks to\\nderive new optimized cluster configuration that can handle\\nat least 100 tasks per second, while maintaining low backpressure and low latency.\\n\\nFor our experiment, we use a newly defined realistic benchmark (with a more complex process model). More about this in a separate blog post.\\n\\nThe second goal is to scale out optimized cluster configuration \\nresources linearly and see if the performance scales accordingly.\\n\\n**TL;DR;**\\n\\nWe used a realistic benchmark to derive a new \\ncluster configuration based on previous requirements. \\n\\nWhen we scale this base configuration linearly we see that the performance\\nincreases almost linearly as well, while maintaining low \\nbackpressure and low latency.\\n\\n## Chaos Experiment\\n\\n### Expected\\n\\nWe expect that we can find a cluster configuration that can handle at 100 \\ntasks second to be significantly reduced in resources in relation to our \\nsmaller clusters (G3-S HA Plan) since these can process significantly above \\nour initial target.\\n\\nWe also expect that we can scale this base configuration linearly, and that \\nthe processing tasks rate to grow initially a bit faster than linearly due to \\nthe lower relative overhead, and if we keep expanding further to flatten due \\nto the partition count being a bottleneck.\\n\\n### Actual\\n\\n#### Minimal Requirements for our Cluster\\n\\nBased on known customer usage, and our own previous experiments, we \\ndetermined that the new cluster would need to create and complete a \\nbaseline of 100 tasks per second, or about 8.6 million tasks per day.\\n\\nOther metrics that we want to preserve and keep track are the backpressure \\nto preserve user experience, guarantee that exporting speed can keep up \\nwith the processing speed, write-to-import latency which tells us how long \\nit takes for a record to be written to being imported by our other \\napps such as the operator.\\n\\n#### Reverse Engineering the Cluster Configuration\\n\\nFor our new configurations the only resources that we are going to change \\nare the ones relevant to the factors described above. These are the \\nresources allocated to our zeebe-brokers, gateway and elasticSearch.\\n\\nOur starting point in resources was the configuration for our G3-S HA Plan\\nas this already had the capability to significantly outperform the current \\ngoal of 100 tasks per second. \\n\\nThe next step was to deploy our realistic benchmark, with a payload of 5 \\ncustomer disputes per instance and start 7 instances per second, this \\ngenerated approximately 120 tasks per second (some buffer was added to guarantee performance).\\n\\nAfter this we reduced the resources iteratively until we saw any increase \\nin backpressure, given that no there was no backlog of records, and no \\nsignificant increase in the write to import latency.\\n\\nThe results for our new cluster are specified bellow in the tables, where \\nour starting cluster configuration is the G3-S HA Plan and the new \\nconfiguration cluster is the G3 - BasePackage HA.\\n\\n| G3-S HA                | CPU Limit | Memory Limit in GB |\\n|------------------------|-----------|--------------------|\\n| operate                | 2         | 2                  |\\n| operate.elasticsearch  | 6         | 6                  |\\n| optimize               | 2         | 2                  |\\n| tasklist               | 2         | 2                  |\\n| zeebe.broker           | 2.88      | 12                 |\\n| zeebe.gateway          | 0.9       | 0.8                |\\n| **TOTAL**              | **15.78** | **24.8**           |\\n\\n| G3 - BasePackage HA   | CPU Limit | Memory Limit in GB |\\n|-----------------------|-----------|--------------------|\\n| operate               | 1         | 1                  |\\n| operate.elasticsearch | 3         | 4.5                |\\n| optimize              | 1         | 1.6                |\\n| tasklist              | 1         | 1                  |\\n| zeebe.broker          | 1.5       | 4.5                |\\n| zeebe.gateway         | 0.6       | 1                  |\\n| **TOTAL**             | **8.1**   | **13.6**           |\\n\\n\\n##### Reduction in Resources for our Optimized Cluster\\n\\n|                       |   CPU Reduction (%) |   Memory Reduction (%) |\\n|:----------------------|--------------------:|-----------------------:|\\n| zeebe.broker          |             47.92   |                   62.5 |\\n| zeebe.gateway         |             33.33   |                  -25.0 |\\n| operate.elasticsearch |             50.00   |                   25.0 |\\n\\n\\nTotal cluster reduction: \\n\\n|                       | G3-S HA | G3 - BasePackage HA | Reduction (%) |\\n|:----------------------|--------:|--------------------:|--------------:|\\n| CPU Limits            |   15.78 |                 8.1 |            49 |\\n| Memory Limits         |    24.8 |                13.6 |            45 |\\n\\n\\nThe process of reducing the hardware requirements was donne initially by \\nscaling down the resources of the zeebe-broker, gateway and elasticSearch. \\nThe other components were left untouched, as they had no impact in our key \\nmetrics, and were scaled down later in separate experiences to maintain \\nuser experience.\\n\\n#### Scaling out the Cluster\\n\\nNow for the scaling procedure we intend to see if we can linearly increase \\nthe allocated resources and having a corresponding performance increase, \\nwhile keeping the backpressure low, low latency, and user experience.\\n\\nFor this we started with the G3 - BasePackage HA configuration and \\nincremented the load again until we saw any increase in backpressure, \\ncapture our key metrics and repeated the process for the cluster \\nconfiguration resources respectively multiplied by 2x, 3x, and 4x.\\n\\nThis means that the resources allocated for our clusters were:\\n\\n|               |   Base 1x |   Base 2x |   Base 3x |   Base 4x |\\n|:--------------|----------:|----------:|----------:|----------:|\\n| CPU Limits    |       8.7 |      17.4 |      26.1 |      34.8 |\\n| Memory Limits |      14.9 |      29.8 |      44.7 |      59.6 |\\n\\nThe results in the table bellow show the performance of our several cluster \\nconfigurations:\\n\\n|                          | Base 1x | Base 2x | Base 3x | Base 4x |\\n|:-------------------------|--------:|--------:|--------:|--------:|\\n| Process Instances/s      |       7 |      12 |      23 |      27 |\\n| Tasks/s                  |     125 |     217 |     414 |     486 |\\n| Average Backpressure     |      2% |      2% |      3% |      6% |\\n| Write-to-Import Latency  |     90s |    120s |    150s |    390s |\\n| Write-to-Process Latency |   140ms |    89ms |   200ms |   160ms |\\n| Records Processed Rate   |    2500 |    4700 |    7800 |   11400 |\\n| Records Exported Rate    |    2100 |    3900 |    6500 |    9200 |\\n\\nThis first observations is that the performance scales particularly well by \\njust adding more resources to the cluster, particularly for a linear \\nincrease of the resources the performance as measured by tasks completed \\nincreases slightly less than linearly (comparing the 1x and 4x task/s we \\nget 388% the initial rate).  \\n\\nThis a very good result as it means that we can scale our system linearly \\n(at least initially) to handle the expected increase in loads.\\n\\nImportantly, the backpressure is kept low, and the write-to-import latency \\nonly increases significantly if we leave the cluster running at max rate \\nfor long periods of time. For slightly lower rates the write-to-import \\nlatency is kept in the single digits of seconds or lower tens. This might \\nimply that a these sustained max rates, the amount records generated starts \\nto be too much for either ElasticSearch or our web apps that import these \\nrecords to handle. Some further investigation could be done here to \\ninvestigate the bottleneck.\\n\\nAnother metric also relevant but not shown in this table is the backlog of \\nrecords not exported, which kept at almost null through all the experiments \\nconducted.\\n\\n### Bugs found\\n\\nDuring the initial tests, we had several OOM errors in the gateways pods. \\nAfter some investigation, we found that this was exclusive to the Camunda 8.\\n6.0 version, which consumes more memory in the gateway than the previous \\nversions. This explains why the gateway memory limits were the only \\nresource that was increased in the new reduced cluster configuration."},{"id":"/2024/08/19/Operate-improve-import-latency","metadata":{"permalink":"/zeebe-chaos/2024/08/19/Operate-improve-import-latency","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-08-19-Operate-improve-import-latency/index.md","source":"@site/blog/2024-08-19-Operate-improve-import-latency/index.md","title":"Improve Operate import latency","description":"In our last Chaos Day we experimented with Operate and different load (Zeebe throughput). We observed that a higher load caused a lower import latency in Operate. The conclusion was that it might be related to Zeebe\'s exporting configuration, which is affected by a higher load.","date":"2024-08-19T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":8.705,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Improve Operate import latency","date":"2024-08-19T00:00:00.000Z","categories":["chaos_experiment","operate"],"tags":["performance"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Optimizing cluster sizing using a real world benchmark","permalink":"/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark"},"nextItem":{"title":"Operate load handling","permalink":"/zeebe-chaos/2024/08/16/Operate-load-handling"}},"content":"[In our last Chaos Day](../2024-08-16-Operate-load-handling/index.md) we experimented with Operate and different load (Zeebe throughput). We observed that a higher load caused a lower import latency in Operate. The conclusion was that it might be related to Zeebe\'s exporting configuration, which is affected by a higher load.\\n\\nIn today\'s chaos day we want to verify how different export and import configurations can affect the importing latency. \\n\\n**TL;DR;** We were able to decrease the import latency by ~35% (from 5.7 to 3.7 seconds), by simply reducing the `bulk.delay` configuration. This worked on low load and even higher load, without significant issues.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background \\n\\n_In the following I want to briefly explain a bit more the background of how exporting and importing play together. If you are already aware feel free to jump to the [next section](#chaos-experiment)._\\n\\n---\\n\\nTo understand how the importing of Operate is affected and works, we first have to take a look at Zeebe.\\n\\nZeebe exports data to Elasticsearch via its Elasticsearch Exporter. The exporter collects data before sending it to Elasticsearch in bulk requests. The amount of data, which is collected in the exporter, is configurable and by default set to 1000 records per batch/bulk. Additionally, there is a memory limit which is taken into account that is set to 10 MB. When the bulk request is reaching that size, the request is sent as well. To cover cases of low load, there is a delay option, which is per default set to 5 seconds. This means, that every 5 seconds the bulk request is sent, even if it is not full.\\n\\nThis explains also the results from [our last Chaos Day](../2024-08-16-Operate-load-handling/index.md), where the import latency was around 5 seconds on a lower load. \\n\\nIn the following, we have written down the sequence of steps a command has to take, and its resulting events until it is visible to the user in Operate. This should allow to better understand how and by what the import latency is affected, and what we might want to tune and experiment further.\\n\\n```\\nUser Command is sent to Gateway \\n--\x3eGateway sents Command to the right Broker\\n----\x3eBroker processes command and produces events\\n------\x3eEvents are exported by Broker to ES (worst case: 5s flush) \\n--------\x3eES refreshes after one second\\n----------\x3eOperate import processing/rewriting data\\n------------\x3eES refreshes after one second\\n--------------\x3eOperate can query the data -> User can see the data \\n```\\n\\nAbout Elasticsearch and its default refresh configuration, etc. you can read [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html#_unset_or_increase_the_refresh_interval). \\n\\nBased on this, we know we have the following minimum delay: \\n```\\ndelay = 2 seconds (due to ES refresh)\\n      + (5 seconds from exporter on low load)\\n      + network delay \\n      + processing delay \\n      + Exporter and Operate data un/marshaling/processing\\n```\\n\\nToday, we will experiment with the Elasticsearch exporter configurations to improve the import latency.\\n\\n## Chaos Experiment\\n\\nAs we have seen [in a previous chaos day](../2024-08-16-Operate-load-handling/index.md) high load affects the importing latency positively. The thesis is that this is due to the export flush delay, which is mostly affecting the exporting on lower load.\\n\\nToday we want to prove the following: \\n\\n> **Hypothesis**\\n> \\n> When we set the exporting/flush delay to a lower value (ex. 1 second), we are improving the import latency for lower load scenarios without affecting the system negatively.\\n\\nWe can define the following `unknowns`, that we want to explore further as well:\\n\\n * It is not clear how lower flush delay affects the system on higher loads. \\n * It is not clear how smaller values (under 1 second) for the flush delay affect the system, no matter of high or low load.\\n\\n### Expected\\n\\n1. When we set the exporting/flush delay to a lower value (ex. 1 second), we are improving the import latency for lower load scenarios without affecting the system negatively.\\n2. When we set the exporting/flush delay to a lower value (ex. 1 second), we are improving the import latency for higher load scenarios, **but decreasing the import throughput**\\n3. When we set the exporting/flush delay to a small value (under 1 second), we are affecting the import throughput negatively\\n\\n### Actual\\n\\nAs always, we set a base installation up to compare against. The load is moderate-to-low (15 PI/s). We can compare the data from the [last chaos day](../2024-08-16-Operate-load-handling/index.md) here as well.\\n\\n<details>\\n<summary>Base: Helm install command</summary>\\n<pre>\\nhelm install $(releaseName) $(chartPath) --render-subchart-notes \\\\\\n --set global.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebe.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebe.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebeGateway.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebeGateway.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set starter.rate=5 \\\\\\n --set worker.replicas=1 \\\\\\n --set timer.replicas=1 \\\\\\n --set timer.rate=5 \\\\\\n --set publisher.replicas=1 \\\\\\n --set publisher.rate=5 \\\\\\n --set camunda-platform.operate.enabled=true \\\\\\n --set camunda-platform.operate.image.repository=gcr.io/zeebe-io/operate \\\\\\n --set camunda-platform.operate.image.tag=ck-operate-benchmark \\\\\\n --set camunda-platform.elasticsearch.master.persistence.size=128Gi \\\\\\n --set camunda-platform.zeebe.retention.minimumAge=1d \\\\\\n</pre>\\n</details>\\n\\nWe see similar results as on the [last Chaos day](../2024-08-16-Operate-load-handling/index.md#base).\\n\\n![base-latency](base-latency.png)\\n![base-throughput](base-throughput.png)\\n\\nWe are able to import around 360 records per second, while Zeebe exports 413. Be aware that some are ignored by Operate.\\nA record has on average a delay of 5.69 seconds from being written by Zeebe to being imported by Operate (and written into the\\nend Elasticsearch index).\\n\\n#### First experiment: Lower flush delay\\n\\n> When we set the exporting/flush delay to a lower value (ex. 1 second), we are improving the import latency for lower load scenarios without affecting the system negatively.\\n\\nTo reduce the exporter flush delay we use the following configuration:\\n\\n```yaml\\nexporters:\\n  elasticsearch:\\n    args:\\n      bulk:\\n        delay: 1\\n```\\n\\nThis can be set in our [benchmark-helm](https://github.com/camunda/zeebe-benchmark-helm) directly via: `--set zeebe.config.zeebe.broker.exporters.elasticsearch.args.bulk.delay=1`\\n\\n<details>\\n<summary>Lower flush delay: Helm install command</summary>\\n<pre>\\nhelm install $(releaseName) $(chartPath) --render-subchart-notes \\\\\\n --set global.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebe.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebe.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebeGateway.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebeGateway.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set starter.rate=5 \\\\\\n --set worker.replicas=1 \\\\\\n --set timer.replicas=1 \\\\\\n --set timer.rate=5 \\\\\\n --set publisher.replicas=1 \\\\\\n --set publisher.rate=5 \\\\\\n --set camunda-platform.operate.enabled=true \\\\\\n --set camunda-platform.operate.image.repository=gcr.io/zeebe-io/operate \\\\\\n --set camunda-platform.operate.image.tag=ck-operate-benchmark \\\\\\n --set camunda-platform.elasticsearch.master.persistence.size=128Gi \\\\\\n --set camunda-platform.zeebe.retention.minimumAge=1d \\\\\\n --set zeebe.config.zeebe.broker.exporters.elasticsearch.args.bulk.delay=1\\n</pre>\\n</details>\\n\\n\\n![lower-delay](lower-delay-base.png)\\n![lower-delay-throughput](lower-delay-base-load-throughput.png)\\n\\nWith setting the `bulk.delay` to one second, we were able to reduce the import latency by ~2 seconds, from 5.69 to 3.68 seconds.\\nThat is a 35% decrease, while other factors stay the same. We can observe that the throughput stays the same (while of course, the load is rather moderate-to-low).\\n\\nThis proved our first hypothesis from above. :white_check_mark:\\n\\n#### Second Experiment: Lower delay with higher load\\n\\n> When we set the exporting/flush delay to a lower value (ex. 1 second), we are improving the import latency for higher load scenarios, **but decreasing the import throughput**\\n\\nSimilar to the first experiment we set the delay to one second, and increased the load in the same way as we did\\n[here](../2024-08-16-Operate-load-handling/index.md#high-load) before.\\n\\n<details>\\n<summary>Lower flush delay with high load: Helm install command</summary>\\n<pre>\\nhelm install $(releaseName) $(chartPath) --render-subchart-notes \\\\\\n --set global.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebe.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebe.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebeGateway.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebeGateway.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set starter.rate=50 \\\\\\n --set worker.replicas=1 \\\\\\n --set timer.replicas=1 \\\\\\n --set timer.rate=50 \\\\\\n --set publisher.replicas=1 \\\\\\n --set publisher.rate=50 \\\\\\n --set camunda-platform.operate.enabled=true \\\\\\n --set camunda-platform.operate.image.repository=gcr.io/zeebe-io/operate \\\\\\n --set camunda-platform.operate.image.tag=ck-operate-benchmark \\\\\\n --set camunda-platform.elasticsearch.master.persistence.size=128Gi \\\\\\n --set camunda-platform.zeebe.retention.minimumAge=1d \\\\\\n --set zeebe.config.zeebe.broker.exporters.elasticsearch.args.bulk.delay=1\\n</pre>\\n</details>\\n\\n\\n![higher-load](lower-delay-high-load-latency.png)\\n![higher-load-throughput](lower-delay-high-load-throughput.png)\\n\\nWe can see that the latency has been increased a bit, versus the lower load benchmark, but it has improved compared to the\\nbenchmark [the last chaos day](../2024-08-16-Operate-load-handling/index.md#high-load). :information: An interesting factor is that it seems that the throughput from Zeebe has changed as well, that in consequence increased the import throughput.\\n\\nLooking into it further, we can see that the job and process instance creation and completion have changed by ~13-18 percent. Before we had around 130 process instance completion per second.\\n\\n![backpressure-higher-load](backpressure-higher-load.png)\\n\\nIn the recent benchmark, we almost reach our target load (150 PI/s) with 147 process instance completions per second.\\n\\n![backpressure-higher-load-lower-delay](backpressure-lower-delay-higher-load.png)\\n\\nThe reason seem to be the different backpressure. Backpressure has been decreased from ~20 % to 5-10%. This might be because our backpressure strategy has recently changed and now takes exporting into account. See also [related chaos day about this topic](../2024-07-25-Using-flow-control-to-handle-bottlenecked-exporting/index.md).\\n\\n_**Update**_:\\n\\nLooking into it further, the backpressure is not affected by the newest feature (as it was not enabled by default). This was discussed internally with the Zeebe team.\\n\\n![higher-load-less-throughput-commit-latency](higher-load-less-throughput-commit-latency.png)\\n\\nThe slower benchmark, seem to have a degraded commit latency, which in consequence slows down the whole system. It is unclear right now, why this is.\\n\\nThe faster benchmark, with the configured exporting, has a much better commit latency. It is unlikely that the exporter configuration affected this part of the system. We will have to retry the both benchmarks.\\n\\n![higher-load-higher-throughput](higher-load-higher-throughput-commit-latency.png)\\n\\n_**Update 20-08-2024**_\\n\\nWe run additional benchmarks to verify the behavior on high load. This time we haven\'t seen any differences in terms\\nof processing performance in both benchmarks.\\n\\nThe benchmark without the configuration, reaches similar numbers (146 PI/s), as the other before.\\n\\n![20-08-high-throughput](2024-08-20_high-load-throughput.png)\\n\\nBenchmark with configuring the flush delay reaches comparable numbers. \\n\\n![20-08-high-throughput](2024-08-20_high-load-throughput-lower-delay.png)\\n\\nDuring running the benchmarks we run into another issue, for which we opened the following [issue #21376](https://github.com/camunda/camunda/issues/21376).\\n\\n##### Additional finding\\n\\nAn interesting additional finding has been done. When the Operate import fails or restarts (that can easily happen with preemptive nodes), then the importer backlog can be significant. This is especially an issue on higher constant load.\\n\\n![import-delay](import-delay.png)\\n\\nIn our benchmark after the importer failed, it took ~20 minutes until the backlog was processed and the import latency was back to normal.\\n\\n![recover-import-delay](import-delay-recover.png)\\n\\nThis shows that Operate, especially the importer is quite sensitive to restarts. This is likely to be changed and improved when\\nOperates importing mechanism is moved into Zeebe, as a separate exporter see [related GH issue](https://github.com/camunda/camunda/issues/16912).\\n\\nOn a lower load, the impact of an importer restart is negligible, as we can see below.\\n\\n![no-impoact-low-load-restart](no-import-delay-restart-low-load.png)\\n\\n#### Third experiment\\n\\n> When we set the exporting/flush delay to a small value (under 1 second), we are affecting the import throughput negatively\\n\\nWe were not able to set the `bulk.delay` to a smaller value than 1 second, as the configuration only accepts longs. The values seem to be expected to be seconds. When setting it to zero, no improvement has been observed (versus one second).\\n\\n## Potential improvements\\n\\n* Allow to configure `bulk.delay` in non-second format (be able to specify the time/duration format)\\n* The `bulk.delay` configures a timer, which gets triggered with the given value. This means the flush can happen, even if flush was executed before causing flush with little buffers.\\n* Importing is highly affected by pod restarts, this can cause issues on higher load, due to a growing backlog. Making import idempotent, and scaling importers would help here.\\n* [Zeebe exporting latency can increase significantly without clear root cause #21376](https://github.com/camunda/camunda/issues/21376)."},{"id":"/2024/08/16/Operate-load-handling","metadata":{"permalink":"/zeebe-chaos/2024/08/16/Operate-load-handling","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-08-16-Operate-load-handling/index.md","source":"@site/blog/2024-08-16-Operate-load-handling/index.md","title":"Operate load handling","description":"Happy to announce that we are broadening the scope of our Chaos days, to look holistically at the whole Camunda Platform, starting today.","date":"2024-08-16T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":7.145,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Operate load handling","date":"2024-08-16T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["performance"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Improve Operate import latency","permalink":"/zeebe-chaos/2024/08/19/Operate-improve-import-latency"},"nextItem":{"title":"Using flow control to handle bottleneck on exporting","permalink":"/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting"}},"content":":tada: Happy to announce that we are broadening the scope of our Chaos days, to look holistically at the whole Camunda Platform, starting today.\\nIn the past Chaos days we often had a close look (or concentrated mostly) at Zeebe performance and stability. \\n\\nToday, we will look at the Operate import performance and how Zeebe processing throughput might affect (or not?) the throughput and latency of the Operate import. Is it decoupled as we thought?\\n\\nThe import time is an important metric, representing the time until data from Zeebe processing is \\nvisible to the User (excluding Elasticsearch\'s indexing). It is measured from when the record is written to the log, by the Zeebe processor, until Operate reads/imports it from Elasticsearch and converts it into its data model. We got much feedback (and experienced this on our own) that\\nOperate is often lagging behind or is too slow, and of course we want to tackle and investigate this further.\\n\\nThe results from this Chaos day and related benchmarks should allow us to better understand how the current importing \\nof Operate performs, and what its affects. Likely it will be a series of posts to investigate this further. In general,\\nthe data will give us some guidance and comparable numbers for the future to improve the importing time. See also related GitHub issue [#16912](https://github.com/camunda/camunda/issues/16912) which targets to improve such.\\n\\n**TL;DR;** We were not able to show that Zeebe throughput doesn\'t affect Operate importing time. We have seen that Operate can be positively affected by the throughput of Zeebe. Surprisingly, Operate was faster to\\nimport if Zeebe produced more data (with a higher throughput). One explanation of this might be that Operate was then less idle.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nAs previously mentioned we will today look at the Operate\'s import latency and throughput. For that, I have created a \\nnew Benchmark dashboard. That allows us to see Zeebe and Operate performance together, at once. \\n\\n![default-latency](default-latency.png)\\n![default-throughput](default-throughput.png)\\n\\nDuring building that dashboard I realized that we missed some detail metrics. For example, the latency of writing and then exporting a record,\\nis currently not measured. Furthermore, we have operating limited metrics, thus allowing us only to see the average\\nlatency, not p99 nor p90. This needs to be enhanced in the future.\\n\\nWe will run three benchmarks (base, high load, and low load), and use again our [benchmark helm chart](https://github.com/camunda/zeebe-benchmark-helm) for such.\\nAll defaults from the helm charts are used, if not other specified. The most important ones, which are static over all benchmarks are listed below.\\n\\n| Config | Value |\\n|-------|--------|\\n|Broker| 3|\\n|Partitions|3|\\n|Replication|3|\\n|Broker Mem|4G|\\n|Broker CPU| 1350m|\\n|Broker Disk|32g|\\n|Gateways|2|\\n|Gateway Mem|1G|\\n|Gateway CPU|450m|\\n|ES nodes| 3|\\n|ES CPU|2|\\n|ES Mem|6G|\\n|ES Disk|128g|\\n| Operate replicas | 1 |\\n| Operate Memory | 2g |\\n| Operate CPU | 2 |\\n\\nWith the base, we should see how the import performs normally. As base, we will use the same configuration as we use in our weekly benchmarks, see\\n[here](https://github.com/camunda/camunda/blob/main/.github/workflows/zeebe-medic-benchmarks.yml#L78-L89).\\n\\nWe use the same applications that we use for our other benchmarks, the code can be found [here](https://github.com/camunda/camunda/tree/main/zeebe/benchmarks/project)\\n\\nThe base looks like the following:\\n\\n| Config  | Value     |\\n|---------|-----------|\\n| Starter | 5 PI/s    |\\n| Worker  | 1 Replica |\\n| Timer | 5  PI/s   |\\n| Publisher| 5   PI/s  |\\n| Variables | 46 Kb     |\\n\\n\\nThe \\"Starter\\" deploys a process model with one task and creates instances at a rate of 5 process instances per second (PI/s). The \\"Worker\\" is handling such related tasks. The \\"Timer\\" deploys a process model with one timer catch event, and creates instances in a rate of 5 PI/s. The \\"Publisher\\" deploys a process model with a message catch event, and publishes messages at a rate of 5 per second. On each process instance variables of the size of 46 kilobytes are sent as payload, to mimic a more realistic scenario.\\n\\nGoing out of the base configuration we are adjusting the rate to a higher value (multiplied by 10), and to a lower value (divided by 5). This means for the high load benchmark we will have a rate of 50 PI/s per application (~150 PI/s), and for the lower load, we will have a rate of 1 PI/s per application (~3 PI/s). \\n\\n### Expected\\n\\nWith the base benchmark, we will see how Operate is performing on a moderate load. As the importing of Operate is decoupled the higher load nor the lower load should have a significant impact on the importing time. It might be that due to a higher load on Zeebe, and a slightly bigger backlog the import time might be a bit higher for Operate.\\n\\n### Actual\\n\\n#### Base\\n\\n<details>\\n<summary>Helm install command</summary>\\n<pre>\\nhelm install $(releaseName) $(chartPath) --render-subchart-notes \\\\\\n --set global.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebe.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebe.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebeGateway.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebeGateway.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set starter.rate=5 \\\\\\n --set worker.replicas=1 \\\\\\n --set timer.replicas=1 \\\\\\n --set timer.rate=5 \\\\\\n --set publisher.replicas=1 \\\\\\n --set publisher.rate=5 \\\\\\n --set camunda-platform.operate.enabled=true \\\\\\n --set camunda-platform.operate.image.repository=gcr.io/zeebe-io/operate \\\\\\n --set camunda-platform.operate.image.tag=ck-operate-benchmark \\\\\\n --set camunda-platform.elasticsearch.master.persistence.size=128Gi \\\\\\n --set camunda-platform.zeebe.retention.minimumAge=1d \\\\\\n</pre>\\n</details>\\n\\nWith a moderate load (as described above) we can see how large the import delay already is.\\n\\n![base-latency](default-latency.png)\\n\\nThe import latency from Operate is above 5 seconds.\\n\\nAs expected we can see that we complete 15 process instances per second. We process around 145 records per second, and export 415 records per second. Operate is only reading 370 records per second because not all records are consumed by Operate. \\n\\n![base-throughput](default-throughput.png)\\n\\nHere it might make sense to configure the exporter, to only export the important ones.\\n\\n#### High load\\n\\n<details>\\n<summary>Helm install command</summary>\\n<pre>\\nhelm install $(releaseName) $(chartPath) --render-subchart-notes \\\\\\n --set global.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebe.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebe.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebeGateway.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebeGateway.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set starter.rate=50 \\\\\\n --set worker.replicas=3 \\\\\\n --set timer.replicas=1 \\\\\\n --set timer.rate=50 \\\\\\n --set publisher.replicas=1 \\\\\\n --set publisher.rate=50 \\\\\\n --set camunda-platform.operate.enabled=true \\\\\\n --set camunda-platform.operate.image.repository=gcr.io/zeebe-io/operate \\\\\\n --set camunda-platform.operate.image.tag=ck-operate-benchmark \\\\\\n --set camunda-platform.elasticsearch.master.persistence.size=128Gi \\\\\\n --set camunda-platform.zeebe.retention.minimumAge=1d \\\\\\n</pre>\\n</details>\\n\\nLooking at the high load benchmark, we can see something surprising. The Operate import latency has been decreased. From ~5.7 to 4.4 seconds, which is a 30% improvement. The Zeebe processing latency has been increased due to the higher load.\\n\\n![high-load-latency](high-load-latency.png)\\n\\n\\nWe can see that Zeebe is not able to handle ~150 instances, this can have multiple causes, too few workers, or other configurations, but this is irrelevant for today\'s benchmark.\\n\\n![high-load-throughput](high-load-throughput.png)\\n\\nA huge amount of records (3158) are imported by Operate per second, with the same configuration as for the base benchmark. It looks like there is still room (we might investigate this further next time).\\n\\n#### Low load\\n\\n<details>\\n<summary>Helm install command</summary>\\n<pre>\\nhelm install $(releaseName) $(chartPath) --render-subchart-notes \\\\\\n --set global.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebe.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebe.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set camunda-platform.zeebeGateway.image.repository=gcr.io/zeebe-io/zeebe \\\\\\n --set camunda-platform.zeebeGateway.image.tag=ck-operate-benchmark-1ad8f375 \\\\\\n --set starter.rate=1 \\\\\\n --set worker.replicas=1 \\\\\\n --set timer.replicas=1 \\\\\\n --set timer.rate=1 \\\\\\n --set publisher.replicas=1 \\\\\\n --set publisher.rate=1 \\\\\\n --set camunda-platform.operate.enabled=true \\\\\\n --set camunda-platform.operate.image.repository=gcr.io/zeebe-io/operate \\\\\\n --set camunda-platform.operate.image.tag=ck-operate-benchmark \\\\\\n --set camunda-platform.elasticsearch.master.persistence.size=128Gi \\\\\\n --set camunda-platform.zeebe.retention.minimumAge=1d \\\\\\n</pre>\\n</details>\\n\\n![low-load-latency](low-load-latency.png)\\n\\nUnexpected or even counterintuitive is that on a lower load the import time went up again or is similar to the base benchmark ~5.7 seconds to import a record.\\n\\n![low-load-throughput](low-load-throughput.png)\\n\\nZeebe is reaching the 3 PI/s and exporting again a bit more than Operate is importing, as described before likely to some filters.\\n\\n### Result\\n\\nWe were not able to prove that Zeebe throughput doesn\'t affect Operate\'s import time. What we have seen is that higher throughput on the Zeebe side positively affects Operate\'s import time (import delay decreases from 5.7 seconds to 4.4 seconds). This was not just a short outlier, it was shown over a long period.\\n\\nIt is likely related to how Zeebe exporting and Operate importing work together. Zeebe exporting collects several data before it is sent to Elasticsearch. Either if a certain time is due or a certain amount is reached. Operate might be idle from time to time and \\"sleep\\" and wake up every certain seconds to import again.\\n\\nWe have to investigate this further to understand all the details, but I think this as already an interesting learning.\\n\\n## Next\\n\\nIn the following, I listed some potential improvements and investigations we might want to do next:\\n\\n* We need better metrics in Operate, e.g. histograms to have p99, and p90 for import latency\\n* We need the measure the export latency, to better understand and compare how long the import time really is\\n* Investigate whether we can better configure exporting and importing, to reduce delays.\\n* Can we filter more records and this affects positively the importing?"},{"id":"/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting","metadata":{"permalink":"/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-07-25-Using-flow-control-to-handle-bottlenecked-exporting/index.md","source":"@site/blog/2024-07-25-Using-flow-control-to-handle-bottlenecked-exporting/index.md","title":"Using flow control to handle bottleneck on exporting","description":"Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).","date":"2024-07-25T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":4.325,"hasTruncateMarker":true,"authors":[{"name":"Rodrigo Lopes","title":"Associate Software Engineer @ Zeebe","url":"https://github.com/rodrigo-lourenco-lopes","imageURL":"https://github.com/rodrigo-lourenco-lopes.png","key":"rodrigo","page":null}],"frontMatter":{"layout":"posts","title":"Using flow control to handle bottleneck on exporting","date":"2024-07-25T00:00:00.000Z","categories":["performance"],"tags":["availability"],"authors":"rodrigo"},"unlisted":false,"prevItem":{"title":"Operate load handling","permalink":"/zeebe-chaos/2024/08/16/Operate-load-handling"},"nextItem":{"title":"Using flow control to handle uncontrolled process loops","permalink":"/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops"}},"content":"Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).\\nLimiting the write rate is a new feature that can be used to prevent building up an excessive exporting backlog.\\nThere are two ways to limit the write rate, either by setting a static limit or by enabling throttling that dynamically adjust the write rate based on the exporting backlog and rate.\\nIn these experiments, we will test both ways of limiting the write rate and observe the effects on processing and exporting.\\n\\n**TL;DR;**\\nBoth setting a static write rate limit and enabling throttling of the write rate can be used to prevent building up an excessive exporting backlog.\\nFor users, this will be seen as backpressure because processing speed is limited by the rate at which it can write processing results.\\n\\n\x3c!--truncate--\x3e\\n## Static write limit\\n\\nWe will construct a cluster under normal utilization and then artificially degrade the exporting process.\\nAfter this we will apply flow control settings to statically rate limit all writes.\\nThe limit will be set slightly lower than the observed exporting rate.\\n\\nFor this we will use the flow control endpoint to temporarily configure the write rate limit.\\n\\nTo fetch the current configuration we can port forward to one of the zeebe pods and use the command:\\n```Shell\\nGET /actuator/flowControl\\n```\\n\\n![original-configuration](original-configuration.png)\\n\\nTo configure the write rate limit we use the same endpoint, for example:\\n\\n```\\nPOST /actuator/flowControl\\n{\\n  \\"write\\": {\\n    \\"enabled\\": true,\\n    \\"limit\\": 400\\n  }\\n}\\n```\\n\\n### Expected\\n\\nWhen we start to degrade the exporting rate, we expect to see the exporting backlog to increase steadily.\\n\\nOnce a static write rate limit below the degraded exporting rate is applied, we expect fewer rates and slower processing.\\nThe exporting backlog should decrease again until we eventually reach zero backlog again.\\nBackpressure should increase because processing has slowed down and some requests will be rejected by the write rate limit.\\n\\n### Actual\\n\\nAfter we artificially degrade the exporter performance, we see a constant increase in records not exported since the processing is still happening at the same rate.\\n\\n![exporting-per-partition](exporting-per-partition-post-degraded-exporting.png)\\n\\n![processing-per-partition](processing-per-partition-post-degraded-exporting.png)\\n\\n![exporter-backlog](number-of-records-not-exported-post-degraded-exporting.png)\\n\\nAfter applying a static rate limit of 400 to be slightly lower than the observed 500-600 of the exporting rate, we see that the processing speed changes accordingly.\\n\\n![processing-per-partition-post-rate-limit](processing-per-partition-post-rate-limit.png)\\n\\nAs expected we also see this reflected in backpressure that sees the user commands being rejected in a much higher portion.\\n\\n![backpressure](backpressure-post-rate-limit.png)\\n\\nWe also observe that the backlog of records not exported starts to decrease at the rate of the difference between exported and written records.\\n\\n![exporter-backlog-post-rate-limit](number-of-records-not-exported-post-rate-limit.png)\\n\\nThese observations match our expectations and show that a static write rate limit can be used to prevent building up an excessive exporting backlog.\\n\\n## Dynamic write rate throttling\\n\\nChoosing a static write rate limit is not a full solution because we can\'t predict the actual exporting rates.\\nTo address this, we can enable write rate throttling that will dynamically adjust the write rate based on the exporting backlog and rate.\\n\\nTo enable write rate throttling we can use the flow control endpoint again, for example:\\n\\n```\\nPOST /actuator/flowControl\\n{\\n  \\"write\\": {\\n    \\"enabled\\": true,\\n    \\"limit\\": 2500,\\n    \\"throttling\\": {\\n      \\"enabled\\": true,\\n      \\"acceptableBacklog\\": 100000,\\n      \\"minimumLimit\\": 100,\\n      \\"resolution\\": \\"15s\\"\\n    }\\n  }\\n}\\n```\\n\\n### Expected\\n\\nSimilar to the first experiment, we expect to see the exporting backlog increase when we artificially degrade the exporting performance.\\nAfter enabling write rate throttling, we expect that the write rate is reduced significantly and eventually matches the exporting rate.\\nThe reduced write rate should show up as backpressure.\\nEventually, the exporting backlog settles at the configured acceptable backlog.\\n\\n### Actual\\nRe-running the same setup, but using the throttling of writes with an acceptable backlog at 100,000  of not exported records, and a limit higher than our processing speed (so has to not impact the experience), we get the following results:\\n\\n![number-of-records-not-exported-post-throttling](number-of-records-not-exported-post-throttling.png)\\n\\nThe orange underline metric displays when the throttled write rate is applied.\\n\\n![exporting-per-partition-post-throttling](exporting-per-partition-post-throttling.png)\\n\\nFrom the panels of the \u201cExporting per Partition\u201d and \u201cNumber of records not exported\u201d, we can observe that during the re-run of the experience our artificially degrading of the exporters only affected Exporters 2 and 3.\\nAfter we enable throttling, the backlog on these affected exporters starts to decrease as expected, later stabilizing on around 100,000 records.\\nThis will drop back to 0 once we remove the artificial degrading of the exporters.\\n\\n![backpressure-post-throttling](backpressure-post-throttling.png)\\n\\nIn the backpressure, we observe that this increases mostly on the affected partitions 2 and 3, and once the number of records not exported reaches the acceptable level this lowers slightly and stabilizes.\\n\\n![processing-per-partition-post-throttling](processing-per-partition-post-throttling.png)\\n\\nFinally, on the panel that shows the processing per partition, we also confirm the expectation that since one of the exporters was not affected by the artificial degrading of the exporter, some additional traffic gets re-routed to this partition, after the throttling gets applied.\\nOn the affected partitions we see the processing decreasing slightly in line with the exporting on the same partitions.\\n\\nOverall the observations match our expectations and show that write rate throttling succeeds and keeps exporting backlog limited."},{"id":"/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops","metadata":{"permalink":"/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-07-25-Using-flow-control-to-handle-uncontrolled-process-loops/index.md","source":"@site/blog/2024-07-25-Using-flow-control-to-handle-uncontrolled-process-loops/index.md","title":"Using flow control to handle uncontrolled process loops","description":"Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).","date":"2024-07-25T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.165,"hasTruncateMarker":true,"authors":[{"name":"Rodrigo Lopes","title":"Associate Software Engineer @ Zeebe","url":"https://github.com/rodrigo-lourenco-lopes","imageURL":"https://github.com/rodrigo-lourenco-lopes.png","key":"rodrigo","page":null}],"frontMatter":{"layout":"posts","title":"Using flow control to handle uncontrolled process loops","date":"2024-07-25T00:00:00.000Z","categories":["performance"],"tags":["availability"],"authors":"rodrigo"},"unlisted":false,"prevItem":{"title":"Using flow control to handle bottleneck on exporting","permalink":"/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting"},"nextItem":{"title":"Reducing the job activation delay","permalink":"/zeebe-chaos/2024/01/19/Job-Activation-Latency"}},"content":"Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).\\n\\nLimiting the write rate is a new feature that can be used to prevent building up an excessive exporting backlog.\\n\\nIn these experiments we will test what happens with the deployment of endless \\nloops that result in high processing load, and how we can use the new \\nflow control to keep the cluster stable. \\n\\n\\n\\n**TL;DR;**\\n\\nEnabling the write rate limiting can help mitigate the effects caused by \\nprocess instances that contain uncontrolled loops by preventing building up an \\nexcessive exporting backlog. \\n\\n\x3c!--truncate--\x3e\\n## Mitigating the performance impacts of deployed loops:\\n\\nWhen an uncontrolled loop is accidentally deployed this tends to use of \\nmost of the \\nprocessing resources of the partitions where instances are running.\\n\\nSuch instances completely occupies its partition, starves other instances and results in slow response times.\\n\\nUsually, these problems should be addressed before other issues arise, such as full disk due to a large backlog of not exported records (max exporting speed tends to be slower than max processing speed).\\n\\nUsing the write rate limiter, we can slow down the processing speed and \\ngive us more time to address the issue, while at the same time enabling us to reduce or maintain the backlog size and reduce risks of side effects.\\n\\nTo reduce the rate write limit we will use the unified control endpoint and configure write limit to be significantly lower than the processing speed.\\n\\nTo fetch the current configuration we can port forward to one of the zeebe pods and use the command:\\n```Shell\\nGET /actuator/flowControl\\n```\\n\\n![original-configuration](original-configuration.png)\\n\\nTo configure the write rate limit we use the same endpoint:\\n\\n```\\nPOST /actuator/flowControl\\n{\\n   \\"write\\": {\\n        \\"enabled\\": true,\\n        \\"limit\\": 3000,\\n  }\\n}\\n```\\n\\nFor this experiment we will test the impact of write rate limits both in \\nsingle loops and dual loops.\\n\\n## Single loop processing:\\n\\n![single-loop](single-loop.png)\\n\\nThis single-loop process will hoard the processing resources and never complete but will append to the processing queue only the next step in the process.\\n\\nThis means that the number of records not processed will only grow if many other processes or requests are arriving at the same time, at a faster rate than the cluster can process.\\n\\n### Expected results:\\n\\nWhen deploying a process instance with a single loop we should see the \\nprocessing rate in the partition increases significantly. \\n\\nThis can lead to processing speed to surpass the exporting speed, which \\nresults in increase in the backlog of exported records.\\n\\nUsing the rate write limits to restrict the processing speed enables us to \\nreduce the backlog size and give more time for the user to fix the \\nunderlying issues with the cluster.\\n\\n### Actual \\n\\nBy deploying a single loop model we can see that the processing and writing \\nincreases in the same partition and stabilizes around 5 000, later at \\naround 17:55 we apply the write rate limit of 3 000, and the processing \\ngets limited accordingly.\\n\\nThis leads to some of the requests being \\nredirected to \\nthe other partitions which cause the processing in these to increase.\\n\\n![single-loop-processing-per-partition](single-loop-processing-per-partition.png)\\n\\nWhen observing the backpressure, we can draw the same conclusions as from \\nthe processing per partition graph, after the model gets deployed, we see an \\nincrease in the backpressure to around 7% in the partition where the loop \\ninstance was deployed.\\n\\nOnce the limit gets set at around 17:55 the backpressure in this partition \\nincreases even more, to around 22% with the backpressure in the other partitions also increasing significantly.\\n\\nThis follows the expected results since with the limiting processing, the after partition will reject even more commands, which get redirected to the remaining partitions which also cause their load to increase and therefore their backpressure as well.\\n\\n![single-loop-backpressure](single-loop-backpressure.png)\\n\\nObserving the exporting per partition panel we can see that the exporting also increases in the affected partition, and this gets reduced after the limit gets imposed.\\n\\n![single-loop-exporting-per-partition](single-loop-exporting-per-partition.png)\\n\\n## Dual loop processing:\\n\\n![double-loop](dual-loop.png)\\n\\nOn the other hand, this dual loop process during its run will always create more records than can be processed since it doubles in the last step.\\n\\nThis will create a steady increase in records not processed even if no other processes or requests are competing for processing time.\\n\\n## Expected \\n\\nWhen deploying a process instance with a dual loop we should expect to see \\na rapid increase in the processing speed and also in the number of records \\nnot processed.\\n\\nIn this case, restricting the processing speed should not decrease the \\nbacklog of processed records since on each run of the loop more records are \\ncreated than \\nprocessed.\\n\\nHowever, it should help us at least in reducing the pace of the increase in \\nthe backlog and therefore give us more time to address the \\nunderlying problem.\\n\\n## Actual \\n\\nAfter deploying the dual loop we can see that the processing quickly jumps to its peak, at around 18:11 we configure the write rate limit at 3000.\\n\\nUnlike the previous experience here we can observe that the processing speed in the other partitions was already increasing before the configuration gets applied.\\n\\n![dual-loop-processing-per-partition](dual-loop-processing-per-partition.png)\\n\\n![dual-loop-exporting-per-partition](dual-loop-exporting-per-partition.png)\\n\\nObserving the backpressure we get the answer as to why the processing in the other partitions was already increasing before the configuration gets applied. \\n\\nThe backpressure had already reached at 100% which means that the dual loop process by itself hoarded completely the processing resources of the partition. \\n\\n![dual-loop-backpressure](dual-loop-backpressure.png)\\n\\nObserving the number of records not processed we conclude as expected that \\nlimiting the write rate cannot stop the records backlog from continuing to increase, but we can see that the slope of the curve is smaller after configuring the limit.\\n\\n![dual-loop-records-not-exported](dual-loop-number-of-records-not-processed.png)\\n\\n\\nOverall the results match our expectations that the flow control configuration can be leveraged to give us more control of the cluster, which in the case of acting on deployed loop instances can give us more tools to address these issues. \\n\\n_Footnote:\\n(As of the latest release, it is no longer possible to deploy processes that \\ncontain a straight-through processing loops such as the ones used in this \\nexperience)._"},{"id":"/2024/01/19/Job-Activation-Latency","metadata":{"permalink":"/zeebe-chaos/2024/01/19/Job-Activation-Latency","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2024-01-19-Job-Activation-Latency/index.md","source":"@site/blog/2024-01-19-Job-Activation-Latency/index.md","title":"Reducing the job activation delay","description":"With the addition of end-to-end job streaming capabilities in Zeebe, we wanted to measure the improvements in job activation latency:","date":"2024-01-19T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":11.045,"hasTruncateMarker":true,"authors":[{"name":"Nicolas Pepin-Perreault","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/npepinpe","imageURL":"https://github.com/npepinpe.png","key":"nicolas","page":null}],"frontMatter":{"layout":"posts","title":"Reducing the job activation delay","date":"2024-01-19T00:00:00.000Z","categories":["performance","bpmn"],"tags":["availability"],"authors":"nicolas"},"unlisted":false,"prevItem":{"title":"Using flow control to handle uncontrolled process loops","permalink":"/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops"},"nextItem":{"title":"Broker Scaling and Performance","permalink":"/zeebe-chaos/2023/12/20/Broker-scaling-performance"}},"content":"With the addition of end-to-end job streaming capabilities in Zeebe, we wanted to measure the improvements in job activation latency:\\n\\n- How much is a single job activation latency reduced?\\n- How much is the activation latency reduced between each task of the same process instance?\\n- How much is the activation latency reduced on large clusters with a high broker and partition count?\\n\\nAdditionally, we wanted to guarantee that every component involved in streaming, including clients, would remain resilient in the face of load surges.\\n\\n**TL;DR;** Job activation latency is greatly reduced, with task based workloads seeing up to 50% reduced overall execution latency. Completing a task now immediately triggers pushing out the next one, meaning the latency to activate the next task in a sequence is bounded by how much time it takes to process its completion in Zeebe. Activation latency is unaffected by how many partitions or brokers there in a cluster, as opposed to job polling, thus ensuring scalability of the system. Finally, reuse of gRPC\'s flow control mechanism ensure clients cannot be overloaded even in the face of load surges, without impacting other workloads in the cluster.\\n\\n[Head over to the documentation to learn how to start using job push!](https://docs.camunda.io/docs/components/concepts/job-workers/#job-streaming)\\n\\n\x3c!--truncate--\x3e\\n\\n## Why job activation latency matters\\n\\nJobs are one of the fundamental building blocks of Zeebe, representing primarily all tasks (e.g. service, send, user), as well as some less obvious symbols (e.g. intermediate message throw event). In essence, they represent the actual unit of work in a process, the part users will implement, i.e. the actual application code. To reduce the likelihood of a job being worked on by multiple clients at the same time, it first goes through an activation process, where it is soft-locked for a specific amount of time. Soft-locked here means anyone can still interact with it - they can complete the job, fail it, etc. Only the activation is locked out, meaning no one else can activate the job until it\'s timed out.\\n\\nThis means that most workloads will consist mostly of job interactions: creation, activation, completion, etc. As such, it\'s critical to ensure clients receive jobs as fast as possible in order to make progress.\\n\\n\\n## Polling: a first implementation\\n\\nBack in 2018, Zeebe introduced the `ActivateJobs` RPC for its gRPC clients, analogous to fetching and locking [external tasks in Camunda 7.x](https://docs.camunda.org/manual/7.20/user-guide/process-engine/external-tasks/). This endpoint allowed clients to activate fetch and activate a specific number of available jobs. In other words, it allowed them to _poll_ for jobs.\\n\\nThis was the first implementation to activate and work on jobs in Zeebe for multiple reason:\\n\\n- It follows a simple request/response pattern\\n- Flow control is delegated to the client/user\\n- Most other approaches will build onto the building blocks used by polling\\n- You will likely implement polling anyway as a fallback for other approaches (e.g. pushing)\\n\\nGrossly simplified, the implementation worked like this:\\n\\n![Job polling](./job-poll.png)\\n\\n- A client initiates an `ActivateJobs` call by sending an initial request\\n- The gateway receives the request and validates it\\n- The gateway starts polling each partition synchronously one by one\\n- Whenever jobs are received from a partition, it forwards them to the client\\n- When all partitions are exhausted, or the maximum number of jobs have been activated, the request is closed\\n\\nAlready we can infer certain performance bottle necks based on the following:\\n\\n- Every request - whether client to gateway, or gateway to broker - adds delay to the activation latency\\n- In the worst case scenario, we have to poll _every_ partition.\\n- The gateway does not know in advance which partitions have jobs available.\\n- Scaling out your clients may have adverse effects by sending out too many requests which all have to be processed independently\\n- [If you have a lot of jobs, you can run into major performance issues when accessing the set of available jobs](https://github.com/camunda/camunda/issues/11813)\\n\\nSo if we have, say, 30 partitions, and each gateway-to-broker request takes 100ms, fetching the jobs on the last partition will take up to 3 seconds, even though the actual activation time on that partition was only 100ms.\\n\\nFurthermore, if we have a sequence of tasks, fetching the next task in the sequence requires, in the worst case scenario, another complete round of polling through all the partitions, even though the task may already be available.\\n\\nOne would think a workaround to this issue would simply be to poll more often, but this can have an adverse impact: each polling request has to be processed by the brokers, and sending too many will simply flood your brokers and slow down all processing, further compounding the problem.\\n\\n### Long polling: a second implementation\\n\\nTo simplify things, the Zeebe team introduced [long polling in 2019](https://github.com/camunda/camunda/issues/2825). [Long polling](https://en.wikipedia.org/wiki/Push_technology#Long_polling) is a fairly common technique to emulate a push or streaming approach while maintaing the request-response pattern of polling. Essentially, if the server has nothing to send to the client, instead of completing the request it will hold it until content is available, or a timeout is reached.\\n\\nIn Zeebe, this means that if we did not reach the maximum number of jobs to activate after polling all partitions, the request is parked but not closed. Eventually when jobs are available, the brokers will make this information known to the gateways, who will then unpark the oldest request and start a new polling round.\\n\\n![Job polling](./job-long-poll.png)\\n\\nThis solved certain problems:\\n\\n- We reduced the amount of requests sent by clients, thus reducing load on the cluster.\\n- In some cases, we reduced the latency when activating the next task in sequence.\\n\\nHowever, there are still some issues:\\n\\n- When receiving the notification we _still_ have to poll all partitions.\\n- If you have multiple gateways, all gateways will start polling if they have parked requests. Some of them may not get any jobs, but they will still have sent requests to brokers which still all have to be processed.\\n- In high load cases, you still need another client request/poll cycle to fetch the next task in a sequence.\\n- Scaling out your clients still add more load on the system, even if the poll less often\\n\\n## Job push: third time\'s the charm\\n\\nIn order to solve these issues, the team decided to implement [a push-based approach to job activation](https://github.com/camunda/camunda/issues/11231).\\n\\nEssentially, we added a new `StreamActivatedJobs` RPC to our gRPC protocol, a so-called [server streaming RPC](https://grpc.io/docs/what-is-grpc/core-concepts/#server-streaming-rpc). In our case, this is meant to be a long-lived stream, such that the call is completed only if the client terminates it, or if the server is shutting down.\\n\\nThe stream itself has the following lifecycle:\\n\\n![Job push](./job-push.png)\\n\\n- The client initiates the stream by sending a job activation request much like with the `ActivateJobs` RPC.\\n  - Since the stream is meant to be long lived, however, there is no upper bound on the number of jobs to activate.\\n- The gateway registers the new stream with all brokers in the cluster\\n  - Note that there is no direct connection between brokers and client; the gateway acts as a proxy for the client.\\n- When jobs are available for activation (e.g. on creation, on timeout, on backoff, etc.), the broker activates the job and pushes it to the gateway.\\n- The gateway forwards the job to the client.\\n\\n[You can read more about the implementation as part of our docs.](https://docs.camunda.io/docs/components/concepts/job-workers/#how-it-works)\\n\\n> Experienced readers will immediately spot that push-based approaches run the risk of overloading the client. Thanks to the built-in flow control facilities of gRPC, we can still ensure clients are resilient in the face of load surges. See [here for an explanation](https://docs.camunda.io/docs/components/concepts/job-workers/#backpressure).\\n\\nThis solved most, if not all, of the problems listed above:\\n\\n- Brokers push jobs out immediately as they become available, removing the need for a gateway-to-broker request.\\n- Since the stream is long lived, there are almost no client requests required after the initial one.\\n- No need to poll every partition anymore.\\n- No thundering herd issues if you have many gateways all polling at the same time due to a notification.\\n- Scaling out your clients adds little to no load to the system, as idle clients simply do nothing.\\n- Even if you have a lot of jobs, in the average case, you never have to iterate over them and instead the broker pushes the job out on creation.\\n\\n### Tests, results, and comparisons\\n\\nIn order to compare the advantages of pushing to polling, we did three different experiments. \\n\\n> Note that all throughput measurements are in process instances executed per second, shortened to PI/s. Additionally, in the results shown below, dotted lines in graphs always refer to job polling measurements, and filled lines to job pushing.\\n\\n#### Cluster specifications\\n\\nNote that, unless specificed otherwise, we used the following clusters to run the tests: 3 brokers, 2 gateways, 3 partitions, replication factor 3.\\n\\n##### Brokers\\n\\n| Parameter | Value |\\n|-----------|-------|\\n| CPU request | 1350m |\\n| Memory request | 4Gi |\\n| CPU thread count | 3 |\\n| IO thread count | 3 |\\n| Disk type | [pd-ssd](https://cloud.google.com/compute/docs/disks#disk-types) |\\n| Disk size | 32Gi |\\n\\n> Disk type, size, and vCPU count in GCP is used to determine your maximum IOPS.\\n\\n##### Gateways\\n\\n| Parameter | Value |\\n|-----------|-------|\\n| CPU request | 450m |\\n| Kubernetes memory request | 1Gi |\\n| Management thread count | 2 |\\n\\n##### Workers\\n\\nTo simulate work, whenever workers receive an activated job, they will wait 50ms before completing it.\\n\\n| Parameter | Value |\\n|-----------|-------|\\n| CPU request | 500m |\\n| Kubernetes memory request | 256Mi |\\n| Thread count | 10 |\\n| Max jobs active | 60 |\\n\\n#### One task\\n\\nAs our baseline test, we ran a constant throughput of 150 PI/s of a single task process workload:\\n\\n![A single task BPMN process: start -> task -> end](./single-task-bpmn.png)\\n\\nSince each job takes at least 50ms of work, the lower bound execution latency for this process is 50ms.\\n\\n![Results of 150 PI/s single task process](./single-task-benchmark.png)\\n\\nThe results show a sharp decrease in both the p50 and p99 of the job lifetime (i.e. the time between creation and completion). Since this workload only consists of a single task, this is mirrored in the overall process execution latency. Overall, we see that switching to a push approach yields a p50 latency improvement of 50%, and a p99 improvement of 75%!\\n\\nAdditionally, we can see with job push that the Zeebe the p50 processing overhead is ~14ms, and the p99 ~390ms. For job polling, the p50 overhead is ~70ms, and the p99 overhead is ~1.7s.\\n\\n#### Ten tasks\\n\\nFor our next test, we ran a constant throughput of 150 PI/s of a ten tasks sequence process:\\n\\n![A ten tasks sequence BPMN process: start -> task_1 -> task_2 -> ... -> task_10 -> end](./ten-tasks-bpmn.png)\\n\\nSince each job takes at least 50ms of work, the lower bound execution latency for this process is 500ms.\\n\\n![Results of 150 PI/s single task process](./ten-tasks-benchmark.png)\\n\\nThe results show a sharp decrease in both the p50 and p99 of the job lifetime (i.e. the time between creation and completion). In this case, the process consists of several tasks, so the process execution latency is noticeably higher. But we can see that the p50 latency for job push is ~640ms. Overall, we see that switching to a push approach yields a p50 latency improvement of 30%, and a p99 improvement of 50%!\\n\\nAdditionally, we can see with job push that the Zeebe the p50 processing overhead is ~140ms, and the p99 ~1.8s. For job polling, the p50 overhead is ~1.4s, and the p99 overhead is ~4.3s.\\n\\n#### Large cluster\\n\\nIn order to verify that the approach will scale along with the cluster size, we next compared polling and pushing with a cluster of 30 brokers and 30 partitions. Again, we tested with the single task process as above, and a constant throughput of 150 PI/s.\\n\\n![Results of 150 PI/s against a large cluster](./thirty-partitions-benchmark.png)\\n\\nFor job push, we see a greatly improved p99 - since each partition is doing less work than before with 3 partitions, we can achieve much more stable performance, with the p99 being quite close to the p50. \\n\\nFor job poll however, we see the downside of having to poll each partition in turn: the p50 is worse than before, and even though the p99 is greatly improved, we can see a wave pattern where it will spike up to 3s, so a decrease compared to the smaller cluster.\\n\\n#### Client backpressure & load surges\\n\\nOne of the downsides of switching to a push approach, unfortunately, is that the client is now at risk of receiving more work than it can safely handle.\\n\\nThankfully, HTTP/2 and gRPC both have mechanisms to ensure flow control for server streaming RPCs.\\n\\n[You can find our tests results in a separate blog post](https://camunda.github.io/zeebe-chaos/2023/11/30/Job-push-overloading).\\n\\n## Further reading\\n\\nYou can read more about job push here:\\n\\n- [Streaming job workers](https://docs.camunda.io/docs/components/concepts/job-workers/#job-streaming)\\n- [Job push for the Java client](https://docs.camunda.io/docs/apis-tools/java-client/job-worker/#job-streaming)\\n- [Job push for the Go client](https://docs.camunda.io/docs/apis-tools/go-client/job-worker/#job-streaming)\\n- [Job push for spring-zeebe](https://github.com/camunda-community-hub/spring-zeebe#enable-job-streaming)\\n\\nAdditionally, we\'ve already written two other blog posts:\\n\\n- [Client backpressure resilience](https://camunda.github.io/zeebe-chaos/2023/11/30/Job-push-overloading)\\n- [Job stream fault tolerance](https://camunda.github.io/zeebe-chaos/2023/12/06/Job-Push-resiliency)"},{"id":"/2023/12/20/Broker-scaling-performance","metadata":{"permalink":"/zeebe-chaos/2023/12/20/Broker-scaling-performance","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-12-20-Broker-scaling-performance/index.md","source":"@site/blog/2023-12-20-Broker-scaling-performance/index.md","title":"Broker Scaling and Performance","description":"With Zeebe now supporting the addition and removal of brokers to a running cluster, we wanted to test three things:","date":"2023-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":5.9,"hasTruncateMarker":true,"authors":[{"name":"Lena Sch\xf6nburg","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/lenaschoenburg","imageURL":"https://github.com/lenaschoenburg.png","key":"lena","page":null},{"name":"Deepthi Akkoorath","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/deepthidevaki","imageURL":"https://github.com/deepthidevaki.png","key":"deepthi","page":null}],"frontMatter":{"layout":"posts","title":"Broker Scaling and Performance","date":"2023-12-20T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability","performance"],"authors":["lena","deepthi"]},"unlisted":false,"prevItem":{"title":"Reducing the job activation delay","permalink":"/zeebe-chaos/2024/01/19/Job-Activation-Latency"},"nextItem":{"title":"Dynamic Scaling with Dataloss","permalink":"/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss"}},"content":"With Zeebe now supporting the addition and removal of brokers to a running cluster, we wanted to test three things:\\n1. Is there an impact on processing performance while scaling?\\n2. Is scaling resilient to high processing load?\\n3. Can scaling up improve processing performance?\\n\\n**TL;DR;** Scaling up works even under high load and has low impact on processing performance. After scaling is complete, processing performance improves in both throughput and latency.\\n\\n\x3c!--truncate--\x3e\\n\\n## Impact of scaling on processing performance\\n\\nScaling up and down is an expensive operation where partition data is transferred between brokers, and leadership for partitions changes.\\nWe wanted to test how much impact this has on regular processing performance.\\n\\nTo do this, we ran a benchmark with 3 brokers, 6 partitions and replication factor 3.\\n\\nThe brokers are limited to 1.35 CPUs and 4GiB RAM each. \\nThey run with additional safety checks that are usually disabled in production and that slightly decrease the baseline processing performance.\\nEach broker uses a small 32GiB SSD for storage, limiting them to a few thousand IOPS.\\n\\nThe processing load was 150 processes per second, with a large payload of 32KiB each.\\nEach process instance has a single service task:\\n\\n![](./one_task.png)\\n\\nThe processing load is generated by our own [benchmarking application](https://github.com/camunda/camunda/tree/9e723b21b0e408fc2b97fd7d3f6b092af8e62dbe/benchmarks).\\n\\n### Expected\\n\\nWhen we scale up from 3 to 6 brokers, we expect a small impact on processing performance.\\nRequest latency may increase slightly, some requests may time out and some will be rejected due to backpressure.\\nThe overall throughput in terms of created and completed process instances as well as jobs may similarly decrease slightly.\\n\\n### Actual\\n\\nFollowing are screenshots of collected metrics.\\nThe blue annotation marks the time where scaling occurred.\\n\\nWe see a short increase in process instance duration, meaning that some process instances were finished slightly slower than before.\\n\\n![](./increased_process_duration.png)\\n\\nThe throughput in terms of created and completed process instances and jobs remained very stable.\\n\\n![](./stable_throughput.png)\\n\\nWe see a small increase in requests timing out or getting rejected by backpressure.\\n\\n![](./failed_requests.png)\\n\\nOverall, this matches our expectation and shows that scaling up has a small impact on processing performance.\\n\\n## Scaling under load\\n\\nSince scaling up is supposed to alleviate high processing load for brokers, it\'s important that it works even under high load.\\nFor this test, we increased the load on the same cluster setup as before to 210 instead of 150 process instances per second.\\nThis is roughly the maximum throughput that the 3 brokers with 6 partitions and replication factor 3 can handle.\\nWe can see this from the relatively high backpressure, as well as high process instance duration.\\n\\n![](./high_load_backpressure.png)\\n![](./high_load_latency.png)\\n\\n### Expected\\n\\nWe expect that scaling up to 6 brokers will still complete successfully, even under high load.\\nThe time it takes until scaling is complete might be slightly higher.\\nThe impact on processing performance, both in terms of throughput and latency, may be slightly larger than in the previous experiment.\\n\\n### Actual\\n\\nThe process instance duration did not increase, and even decreased slightly.\\n![](./high_load_scaling_latency.png)\\n\\nSimilarly, the throughput in terms of created and completed process instances and jobs remained relatively stable.\\n![](./high_load_scaling_throughput.png)\\n\\nThe number of failed requests increased slightly, but still well within an acceptable range.\\n![](./high_load_scaling_failed_requests.png)\\n\\nThe scaling operation took 5 minutes, a good portion of which is waiting for the new brokers to get scheduled and start up.\\n\\nOverall, this matches our expectation and shows that scaling can complete fast and with low impact on processing performance, even under high load.\\n\\n## Scaling up to improve performance\\n\\nThe most obvious goal of scaling brokers is to unlock additional processing performance.\\nWhile vertical scaling is also a great option, this can hit limits imposed by your infrastructure provider.\\nFor example, some machine types may offer great CPU performance but are severely limited in IOPS.\\nAdditionally, vertical scaling is often more expensive than horizontal scaling.\\nIt also comes with increased risk when a single machine fails because the remaining machines may already run at their limits and will then struggle to handle the additional load during failover.\\n\\nTo show how broker scaling can improve processing performance, we reused the same cluster setup as before.\\nWe have 3 brokers, 6 partitions and replication factor 3.\\n\\nThe brokers are limited to 1.35 CPUs and 4GiB RAM each. \\nThey run with additional safety checks that are usually disabled in production and that slightly decrease the baseline processing performance.\\nEach broker uses a small 32GiB SSD for storage, limiting them to a few thousand IOPS.\\n\\nWe changed the processing load slightly to simulate a more realistic scenario.\\nThe new process model consists of 10 tasks with two timers in-between, each delaying the process instance by 1 minute.\\n\\n![](./ten_tasks.png)\\n\\nThe processing load is generated by our own [benchmarking application](https://github.com/camunda/camunda/tree/9e723b21b0e408fc2b97fd7d3f6b092af8e62dbe/benchmarks), initially starting 40 process instances per second.\\n\\nThis results in 400 jobs created and completed per second.\\n\\nThis stresses the 3 brokers and we see backpressure on all partitions.\\n![](./perf_initial_backpressure.png)\\n\\nWe also see a few jobs timing out, indicating that the cluster is unable to handle this load consistency:\\n![](./perf_initial_timeouts.png)\\n\\nWe also see that many jobs are active for much longer than 1 second, even though the workers only delay completion by 50ms.\\n![](./perf_initial_job_lifetime.png)\\n\\nAs hinted at before, much of this performance limit can be attributed to the limited IOPS of the small SSDs.\\nWe see this in a very high commit and write latency, while the IOPS remain stable, right at the limit of what the SSDs can handle.\\n\\n![](./perf_initial_iops.png)\\n![](./perf_initial_commit_latency.png)\\n![](./perf_initial_write_latency.png)\\n\\n### Expected\\n\\nWhen we scale up to 6 brokers, and thus distribute the partitions such that each broker is only leader for 1 instead of 2 partitions, we expect that processing performance improves.\\n\\nAs these things usually go, we don\'t expect a doubling in performance but aiming for a 1.5x improvement seems reasonable.\\n\\n### Actual\\n\\nShortly after scaling up and after partition leadership has balanced, we see a significant improvement in backpressure.\\n![](./perf_after_backpressure.png)\\n\\nThe job lifetime decreases dramatically, with most jobs now taking < 50ms from creation until completion.\\n![](./perf_after_job_lifetime.png)\\n\\nOverall processing latency improves similarly.\\n![](./perf_after_processing_latency.png)\\n\\nMuch of this improvement can be attributed to the reduced IOPS.\\n\\n![](./perf_after_iops.png)\\n\\nCommit and write latency improves accordingly.\\n![](./perf_after_commit_latency.png)\\n![](./perf_after_write_latency.png)\\n\\nAnother source for improved performance is reduced CPU load.\\nWith 3 brokers being leader for 2 partitions each, they were hitting their CPU limits and got throttled by the underlying infrastructure.\\nWith 6 brokers, each only being leader for 1 partition, the CPU load is reduced and brokers are no longer throttled.\\n![](./perf_after_cpu.png)\\n\\n\\nWhile this is already a success, we can push things further now.\\nWe are able to increase the load from 40 to 65 process instances per second, resulting in 650 jobs created and completed per second.\\nThis is a 1.6x improvement over the initial load while achieving similar backpressure.\\n\\n![](./perf_increased_load_backpressure.png)\\n\\nJob lifetime and overall processing latency is still better than before scaling up, even though load increased by 1.6x\\n![](./perf_increased_load_job_lifetime.png)\\n![](./perf_increased_load_processing_latency.png)\\n\\n\\nOverall, this shows that scaling up can improve processing performance significantly, especially when the initial cluster setup is resource limited and vertical scaling is not possible."},{"id":"/2023/12/19/Dynamic-Scaling-with-Dataloss","metadata":{"permalink":"/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-12-19-Dynamic-Scaling-with-Dataloss/index.md","source":"@site/blog/2023-12-19-Dynamic-Scaling-with-Dataloss/index.md","title":"Dynamic Scaling with Dataloss","description":"We continue our previous experiments with dynamically scaling by now also testing whether","date":"2023-12-19T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":4.425,"hasTruncateMarker":true,"authors":[{"name":"Lena Sch\xf6nburg","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/lenaschoenburg","imageURL":"https://github.com/lenaschoenburg.png","key":"lena","page":null}],"frontMatter":{"layout":"posts","title":"Dynamic Scaling with Dataloss","date":"2023-12-19T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"lena"},"unlisted":false,"prevItem":{"title":"Broker Scaling and Performance","permalink":"/zeebe-chaos/2023/12/20/Broker-scaling-performance"},"nextItem":{"title":"Dynamically scaling brokers","permalink":"/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers"}},"content":"We continue our [previous experiments](../2023-12-18-Dynamically-scaling-brokers/index.md) with dynamically scaling by now also testing whether\\nthe cluster survives dataloss during the process.\\n\\nOne goal is to verify that we haven\'t accidentally introduced a single point of failure in the cluster.\\nAnother is to ensure that data loss does not corrupt the cluster topology.\\n\\n**TL;DR;**\\nEven with dataloss, the scaling completes successfully and with the expected results.\\nWe found that during scaling, a single broker of the previous cluster configuration can become a single point of failure by preventing a partition from electing a leader.\\nThis is not exactly a bug, but something that we want to improve.\\n\\n\x3c!--truncate--\x3e\\n\\n## Dataloss on the Coordinator\\n\\nZeebe uses Broker 0 as the coordinator for changes to the cluster topology.\\nWhile changes can only be initiated by the coordinator, losing the coordinator and it\'s data should not prevent the scaling operation from completing.\\nWhen the coordinator restarts without any data, it should be able to recover the cluster topology as well as the partition data from the remaining brokers.\\n\\nTo test this, we use the `zbchaos dataloss delete` and `zeebe dataloss recover` commands.\\nAfter deleting, the broker will not restart directly, instead waiting for the `zbchaos dataloss recover` command to be executed.\\nThe `zbchaos dataloss recover` command only unblocks the broker and allows it to start, it does not restore any data and we rely on normal replication for that.\\n\\nShortly after triggering a scale up with `zbchaos cluster scale --brokers 6`, we trigger dataloss on the coordinator with `zbchaos broker dataloss delete --nodeId 0`.\\nAfter observing the system for a while, we then restore the coordinator with `zbchaos dataloss recover --nodeId 0`.\\n\\n### Expected\\n\\nThe scaling operation eventually completes with the expected result of 6 brokers and 6 partitions, evenly distributed.\\nThe coordinator recovers after dataloss and eventually receives the cluster topology from the remaining brokers.\\nThe scaling operation should make progress while the coordinator is down.\\n\\n### Actual\\n\\nAfter starting the operation with `zbchaos cluster scale --brokers 6` we see that the operation has started:\\n```\\n$ zbchaos cluster scale --brokers 6\\nChange 18 is IN_PROGRESS with 0/24 operations complete\\n...\\n```\\n\\nWe then trigger dataloss on the coordinator with `zbchaos broker dataloss delete --nodeId 0`.\\n\\nAfter this, the operations do not make progress anymore and broker 5 is stuck trying to join partition 5:\\n\\n```json\\n{\\n  \\"Version\\": 18,\\n  ...\\n  \\"PendingChange\\": {\\n    \\"Id\\": 18,\\n    \\"Status\\": \\"IN_PROGRESS\\",\\n    \\"StartedAt\\": \\"\\",\\n    \\"CompletedAt\\": \\"\\",\\n    \\"InternalVersion\\": 0,\\n    \\"Completed\\": [\\n      {\\n        \\"Operation\\": \\"BROKER_ADD\\",\\n        \\"BrokerId\\": 3,\\n        \\"PartitionId\\": 0,\\n        \\"Priority\\": 0\\n      },\\n      {\\n        \\"Operation\\": \\"BROKER_ADD\\",\\n        \\"BrokerId\\": 4,\\n        \\"PartitionId\\": 0,\\n        \\"Priority\\": 0\\n      },\\n      {\\n        \\"Operation\\": \\"BROKER_ADD\\",\\n        \\"BrokerId\\": 5,\\n        \\"PartitionId\\": 0,\\n        \\"Priority\\": 0\\n      }\\n    ],\\n    \\"Pending\\": [\\n      {\\n        \\"Operation\\": \\"PARTITION_JOIN\\",\\n        \\"BrokerId\\": 5,\\n        \\"PartitionId\\": 5,\\n        \\"Priority\\": 2\\n      },\\n      ...\\n    ]\\n  }\\n}\\n```\\n\\nThe coordinator is a member of partition 5 but there are two remaining members of partition 5 that should allow broker 5 to join.\\n![](p5-roles.png)\\n\\nAfter restoring the coordinator again with `zbchaos dataloss recover --nodeId 0`, joining eventually completes and the scaling operation finishes successfully.\\n\\n![](scaleup-complete.png)\\n\\n```json\\n{\\n  \\"Version\\": 19,\\n  \\"LastChange\\": {\\n    \\"Id\\": 18,\\n    \\"Status\\": \\"COMPLETED\\",\\n    \\"StartedAt\\": \\"2023-12-18T17:05:55.849442157Z\\",\\n    \\"CompletedAt\\": \\"2023-12-18T17:17:32.913050015Z\\"\\n  },\\n  \\"PendingChange\\": null\\n}\\n```\\n\\nOverall, we achieve our goal that the scaling operation eventually completes with the expected result.\\nThe coordinator recovers after dataloss and eventually receives the cluster topology from the remaining brokers.\\n\\nHowever, it was unexpected that the scaling did not make progress while the coordinator was down.\\n\\n## Single point of failure during scaling\\n\\nThe issue that scaling did not make progress while the coordinator was reproducible.\\nEventually, we diagnosed it as the following edge case:\\n\\nWhen scaling up and and adding a new member to the replication group of a partition, the raft partition goes through joint consensus.\\nThe details of this process are described in the [raft paper](https://raft.github.io/raft.pdf), but here is a very short summary:\\nJoint consensus is similar to a 2-phase commit, where the leader of the partition first introduces a new _joint consensus_ configuration that requires quorum from both the old and new set of members.\\nAfter committing the joint consensus configuration, the leader leaves joint consensus by \\"forgetting\\" the old member set and only using the new member set.\\nOnly after this second configuration is committed, joining of the new member is complete.\\n\\nIn our example, the new set of members has size 4, one of which is the coordinator and one is the newly joining member.\\nWith 4 members, the quorum is 3, meaning that the partition can only elect a leader and process if at least 3 members are available.\\nIn our experiment, we made the coordinator unavailable, so we were already down to 3 members.\\nAdditionally, the newly joining member did not start yet because it was waiting for a successful join response from the leader.\\nThe newly joining member never received such a response because the joint-consensus phase was not completed.\\nThis resulted in only 2 out of 4 members being available, which is not enough to elect a leader.\\n\\nWe want to improve this behavior in the future but likely can\'t prevent it completely.\\nThat means that there is an increased risk of unavailable partitions during scaling.\\nHowever, this only occurs if another broker becomes unavailable with an unfortunate timing and resolves itself automatically once the broker is available again.\\n\\nZeebe issue: https://github.com/camunda/camunda/issues/15679"},{"id":"/2023/12/18/Dynamically-scaling-brokers","metadata":{"permalink":"/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-12-18-Dynamically-scaling-brokers/index.md","source":"@site/blog/2023-12-18-Dynamically-scaling-brokers/index.md","title":"Dynamically scaling brokers","description":"We experimented with the first version of dynamic scaling in Zeebe, adding or removing brokers for a running cluster.","date":"2023-12-18T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":6.05,"hasTruncateMarker":true,"authors":[{"name":"Lena Sch\xf6nburg","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/lenaschoenburg","imageURL":"https://github.com/lenaschoenburg.png","key":"lena","page":null}],"frontMatter":{"layout":"posts","title":"Dynamically scaling brokers","date":"2023-12-18T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability","performance"],"authors":"lena"},"unlisted":false,"prevItem":{"title":"Dynamic Scaling with Dataloss","permalink":"/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss"},"nextItem":{"title":"Job push resiliency","permalink":"/zeebe-chaos/2023/12/06/Job-Push-resiliency"}},"content":"We experimented with the first version of [dynamic scaling in Zeebe](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/operations/cluster-scaling/), adding or removing brokers for a running cluster.\\n\\nScaling up and down is a high-level operation that consists of many steps that need to be carried co-operatively by all brokers in the cluster.\\nFor example, adding new brokers first adds them to the replication group of the assigned partitions and then removes some of the older brokers from the replication group.\\nAdditionally, [priorities](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/configuration/priority-election/) need to be reconfigured to ensure that the cluster approaches balanced leadership eventually.\\n\\nThis orchestration over multiple steps ensures that all partitions are replicated by at least as many brokers as configured with the `replicationFactor`.\\nAs always, when it comes to orchestrating distributed systems, there are many edge cases and failure modes to consider.\\n\\nThe goal of this experiment was to verify that the operation is resilient to broker restarts.\\nWe can accept that operations take longer than usual to complete, but we need to make sure that the operation eventually succeeds with the expected cluster topology as result.\\n\\n**TL;DR;** Both scaling up and down is resilient to broker restarts, with the only effect that the operation takes longer than usual to complete.\\n\\n\x3c!--truncate--\x3e\\n\\n## Scaling up should be resilient to broker restarts\\n\\nWe start with a cluster of 3 brokers, 6 partitions and replication factor 3.\\nIf leadership is balanced, each broker should be leader for 2 partitions and follower for 4 partitions.\\nUsing more partitions than brokers allows us to scale up to more brokers, distributing the partitions such that each broker has less work to do.\\n\\nFor this experiment, we introduce chaos by letting a random broker restart every 30 seconds.\\n\\n### Expected\\n\\nEven when brokers are restarting, the scale operation should eventually succeed.\\nThe expected cluster topology after scaling up is 6 brokers, 6 partitions and replication factor 3, leading to 3 partitions for each broker instead of 6.\\n\\n### Actual\\n\\n#### Verify steady state\\n\\nThe current cluster topology queried with `zbchaos cluster status` shows 6 partitions with 3 replicas each, evenly distributed across the 3 brokers.\\n\\n```json\\n{\\n  \\"Brokers\\": [\\n    {\\n      \\"Id\\": 1,\\n      \\"Partitions\\": [\\n        {\\n          \\"Id\\": 1,\\n        },\\n        {\\n          \\"Id\\": 2,\\n        },\\n        {\\n          \\"Id\\": 3,\\n        },\\n        {\\n          \\"Id\\": 4,\\n        },\\n        {\\n          \\"Id\\": 5,\\n        },\\n        {\\n          \\"Id\\": 6,\\n        }\\n      ]\\n    },\\n    {\\n      \\"Id\\": 2,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 0,\\n      ...\\n    }\\n  ],\\n}\\n```\\nThe above is an abbreviated version of the actual output, which contains more information.\\n\\nAll partitions are reported as healthy and leadership is balanced::\\n\\n```bash\\n$ zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3         |Partition 4         |Partition 5         |Partition 6\\n0         |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\n\\n#### Scaling up with broker restarts\\n\\nWe start the scaling with `zbchaos cluster scale --brokers 6` and restart a random broker every 30 seconds:\\n\\n```bash\\n$ zbchaos cluster scale --brokers 6 & \\n$ while true; do sleep 30; zbchaos restart broker --nodeId $(shuf -i 0-5 -n 1); done\\n```\\n\\nAfter the scaling completed, we stop the restarting and let the cluste settle again for a few minutes.\\n\\n### Result\\n\\nThe scale operation succeeds and the newly reported cluster topology shows us 6 partitions with 3 replicas each, evenly distributed across 6 instead of 3 brokers:\\n\\n```json\\n{\\n  \\"Brokers\\": [\\n    {\\n      \\"Id\\": 1,\\n      \\"Partitions\\": [\\n        {\\n          \\"Id\\": 1,\\n        },\\n        {\\n          \\"Id\\": 2,\\n        },\\n        {\\n          \\"Id\\": 6,\\n        }\\n      ]\\n    },\\n    {\\n      \\"Id\\": 2,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 3,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 4,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 5,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 0,\\n      ...\\n    }\\n  ],\\n  \\"LastChange\\": {\\n    \\"Id\\": 14,\\n    \\"Status\\": \\"COMPLETED\\",\\n    \\"StartedAt\\": \\"2023-12-18T15:12:57.790824149Z\\",\\n    \\"CompletedAt\\": \\"2023-12-18T15:30:20.920657536Z\\"\\n  },\\n}\\n```\\n\\nAll partitions are reported as healthy and leadership is balanced:\\n\\n```bash\\n$ zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3         |Partition 4         |Partition 5         |Partition 6\\n0         |LEADER (HEALTHY)    |                    |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |                    |                    |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |                    |\\n3         |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |\\n4         |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |\\n5         |                    |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\n\\nThe operation succeeded in about 17 minutes, longer than usual because of the restarts:\\n![](scaleup-completed.png)\\n\\n\\n## Scaling down should be resilient to broker restarts\\n\\nExactly like scaling up, scaling down is also a high-level operation that consists of many steps that need to be carried out by all brokers in the cluster.\\nBefore a broker can leave, another broker first needs to join the replication group to ensure that we maintain a replication factor of 3 at all times.\\n\\n### Expected\\n\\nEven when brokers are restarting, the scale operation should eventually succeed with the expected cluster topology as result.\\n\\n### Actual\\n\\n#### Verify steady state\\n\\nWe start with the cluster topology that we got as result of the previous experiment.\\n6 partitions with 3 replicas distributed over 6 brokers:\\n\\n```json\\n{\\n  \\"Brokers\\": [\\n    {\\n      \\"Id\\": 1,\\n      \\"Partitions\\": [\\n        {\\n          \\"Id\\": 1,\\n        },\\n        {\\n          \\"Id\\": 2,\\n        },\\n        {\\n          \\"Id\\": 6,\\n        }\\n      ]\\n    },\\n    {\\n      \\"Id\\": 2,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 3,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 4,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 5,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 0,\\n      ...\\n    }\\n  ],\\n  \\"LastChange\\": {\\n    \\"Id\\": 14,\\n    \\"Status\\": \\"COMPLETED\\",\\n    \\"StartedAt\\": \\"2023-12-18T15:12:57.790824149Z\\",\\n    \\"CompletedAt\\": \\"2023-12-18T15:30:20.920657536Z\\"\\n  },\\n}\\n```\\n\\nAll partitions are reported as healthy and leadership is balanced:\\n\\n```bash\\n$ zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3         |Partition 4         |Partition 5         |Partition 6\\n0         |LEADER (HEALTHY)    |                    |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |                    |                    |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |                    |\\n3         |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |                    |\\n4         |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |\\n5         |                    |                    |                    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\n\\n\\n#### Scaling down with broker restarts\\n\\nWe scale down with `zbchaos cluster scale --brokers 3` and restart a random broker every 30 seconds:\\n\\n```bash\\n$ zbchaos cluster scale --brokers 3 &\\n$ while true; do sleep 30; zbchaos restart broker --nodeId $(shuf -i 0-5 -n 1); done\\n```\\n\\n### Result\\n\\nAll 6 partitions with 3 replicas each are evenly distributed across 3 brokers, leading to 6 partitions for each broker again.\\n\\n```json\\n{\\n  \\"Brokers\\": [\\n    {\\n      \\"Id\\": 1,\\n      \\"Partitions\\": [\\n        {\\n          \\"Id\\": 1,\\n        },\\n        {\\n          \\"Id\\": 2,\\n        },\\n        {\\n          \\"Id\\": 3,\\n        },\\n        {\\n          \\"Id\\": 4,\\n        },\\n        {\\n          \\"Id\\": 5,\\n        },\\n        {\\n          \\"Id\\": 6,\\n        }\\n      ]\\n    },\\n    {\\n      \\"Id\\": 2,\\n      ...\\n    },\\n    {\\n      \\"Id\\": 0,\\n      ...\\n    }\\n  ],\\n  \\"LastChange\\": {\\n    \\"Id\\": 16,\\n    \\"Status\\": \\"COMPLETED\\",\\n    \\"StartedAt\\": \\"2023-12-18T16:07:07.208363298Z\\",\\n    \\"CompletedAt\\": \\"2023-12-18T16:28:58.836369836Z\\"\\n  },\\n  \\"PendingChange\\": null\\n}\\n```\\n\\nAll partitions are healthy and leadership is distributed evenly:\\n\\n```bash\\n$ zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3           |Partition 4         |Partition 5         |Partition 6\\n0         |LEADER (UNHEALTHY)  |FOLLOWER (HEALTHY)  |FOLLOWER (UNHEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (UNHEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)    |FOLLOWER (HEALTHY)  |LEADER (UNHEALTHY)  |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)      |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\n\\nThe operation completes in 21 minutes, longer than usual because of the restarts:\\n![](scaledown-completed.png)"},{"id":"/2023/12/06/Job-Push-resiliency","metadata":{"permalink":"/zeebe-chaos/2023/12/06/Job-Push-resiliency","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-12-06-Job-Push-resiliency/index.md","source":"@site/blog/2023-12-06-Job-Push-resiliency/index.md","title":"Job push resiliency","description":"In today\'s chaos day we experimented with job push resiliency.","date":"2023-12-06T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"resiliency","permalink":"/zeebe-chaos/tags/resiliency"}],"readingTime":6.41,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"},{"name":"Nicolas Pepin-Perreault","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/npepinpe","imageURL":"https://github.com/npepinpe.png","key":"nicolas","page":null}],"frontMatter":{"layout":"posts","title":"Job push resiliency","date":"2023-12-06T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability","resiliency"],"authors":["zell","nicolas"]},"unlisted":false,"prevItem":{"title":"Dynamically scaling brokers","permalink":"/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers"},"nextItem":{"title":"Job push overloading","permalink":"/zeebe-chaos/2023/11/30/Job-push-overloading"}},"content":"In today\'s chaos day we experimented with job push resiliency.\\n\\nThe following experiments we have done today:\\n\\n1. Job streams should be resilient to gateway restarts/crash\\n2. Job streams should be resilient to leadership changes/leader restarts\\n3. Job streams should be resilient to cluster restarts\\n\\n**TL;DR;** All experiments succeeded and showcased the resiliency even on component restarts. :rocket:\\n\\n\x3c!--truncate--\x3e\\n\\nTo reduce the blast radius and to better verify that everything works as expected we use a trimmed version of our benchmark setup. This means three brokers, one partition, replication factor three, and one gateway. No starter deployed. We deployed one worker with a very high polling interval, to make sure that we rely on streaming.\\n\\n## Gateway restarts\\n\\nIn our first experiment, we wanted to verify that: Job streaming should be resilient to gateway restarts/crashes.\\n\\nThe experiment will look like the following:\\n\\n* Verify steady state:\\n  * Cluster is healthy\\n  * When creating an instance, and start streaming we can retrieve and complete the corresponding job\\n* Chaos injection:\\n  * Restarting the gateway\\n* Verify steady state:\\n    * Cluster is healthy\\n    * When creating an instance, and start streaming we can retrieve and complete the corresponding job\\n\\n### Expected\\n\\nWe expect that even after a gateway restart we can retrieve a job (the stream should be recreated) and complete our new instance.\\n\\n### Actual\\n\\nWe deployed the worker (with a replica of one), and configured it with a high polling interval `-Dapp.worker.pollingDelay=24h`.\\n\\nTo run any instances we need to deploy once the benchmark process model\\n```\\nzbchaos deploy process\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nDeployed given process model , under key 2251799813685249!\\n```\\n\\n#### Verify steady state\\n\\nWe verify the readiness and the instance creation.\\n\\n```sh\\n$ zbchaos verify readiness\\nAll Zeebe nodes are running.\\n\\n$ zbchaos verify instance-creation --awaitResult --verbose\\nFlags: {1 LEADER -1  10  msg true 1 LEADER -1 2 LEADER -1 1701853048870 false false true false false 30 false -1 benchmark 30   1 1 benchmark-task}\\nConnecting to ck-np-chaos-day-job-push\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nRunning experiment in self-managed environment.\\nPort forward to ck-np-chaos-day-job-push-zeebe-gateway-68695d9cb5-lhgg5\\nSuccessfully created port forwarding tunnel\\nWe await the result of the process instance creation, thus we skip the partition id check.\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: true]\\nCreated process instance with key 2251799813685251 on partition 1, required partition 0.\\nThe steady-state was successfully verified!\\n```\\n\\n#### Injecting chaos\\n\\nNext, we will restart the gateway.\\n\\n```shell\\n$ zbchaos restart gateway --verbose\\nFlags: {1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1701853221588 false false true false false 30 false -1 benchmark 30   1 1 benchmark-task}\\nConnecting to ck-np-chaos-day-job-push\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nRunning experiment in self-managed environment.\\nRestarted ck-np-chaos-day-job-push-zeebe-gateway-68695d9cb5-lhgg5\\n```\\n\\n#### Verify steady state\\n\\n```shell\\n$ zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n```\\n\\n```shell\\n$ zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n\\n### Result \\nThe experiment succeeded. We were able to verify the steady state after the chaos injection. Furthermore, we observe in the metrics as well that the jobs have been pushed after the gateway restart. :white_check_mark:\\n\\n![](job-push-gw-restart.png)\\n\\n\\n### With termination\\n\\nWe wanted to verify the same by terminating the gateway instead of a graceful shutdown (which is done within the restart command).\\n\\n```shell\\n$ zbchaos terminate gateway --verbose\\nFlags: {1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1701853482263 false false true false false 30 false -1 benchmark 30   1 1 benchmark-task}\\nConnecting to ck-np-chaos-day-job-push\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nRunning experiment in self-managed environment.\\nTerminated ck-np-chaos-day-job-push-zeebe-gateway-68695d9cb5-jqfzg\\n```\\n\\nVerifying the steady stated again showed no unexpected issues.\\n\\nOut of interest we checked what is happening in worker:\\n\\n```\\n09:05:44.047 [pool-5-thread-3] WARN  io.camunda.zeebe.client.job.worker - Failed to stream jobs of type \'benchmark-task\' to worker \'benchmark-worker\'\\nio.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\n...\\n```\\n\\nWe see as expected several `UNAVAILABLE: io exception` and later the worker recovered.\\n\\nBased on the metrics we can observe the same. Jobs are pushed to the workers even after restarting the gateway.\\n\\n![](job-push-gw-terminate.png)\\n\\n## Leader restart\\n\\nIn this experiment, we want to verify how resilient job push is on leader changes/restarts.\\n\\nThe verification of the steady state is the same as above, so I will skip this description here.\\n\\n### Expected\\n\\nWorkers shouldn\'t care about leader change, this should be handled fully by the gateway.\\n\\n### Actual\\n\\n#### Verify steady state\\n\\n```\\n$ zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n$ zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n#### Inject chaos\\n\\n```shell\\n$ zbchaos restart broker --partitionId 1 --role LEADER\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nRestarted ck-np-chaos-day-job-push-zeebe-0\\n```\\n\\n#### Verify steady state\\n\\n```shell\\n$ zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n$ zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n### Result \\n\\nWe were able to verify that a leader restart doesn\'t cause issues and job push can handle such events.\\n\\n![](job-push-leader-restart.png)\\n\\n\\nWe can see that the leader was changed, and also switched back shortly after.\\n![](leaderchanges.png)\\n\\nThis is caused by our leader-balancing cron job.\\n![](job-push-leader-restart-cronjob.png)\\n\\nThis also means we had two leader changes, and the push was even pushed by the restarted node.\\n\\n\\n## Complete cluster restart\\n\\nIn this experiment, we wanted to verify whether job push can also handle a complete cluster restart.\\n\\n### Expected\\n\\nJob push can handle a cluster restart and a corresponding job is pushed to the worker afterwards.\\n\\n### Actual\\n\\n#### Verify steady state\\n```\\n\u276f zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n\u276f zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n#### Inject chaos\\n\\nRight now `zbchaos` doesn\'t support restarting a complete cluster, so we had to fall back to `kubectl`. \\n\\n```sh\\n$ kubectl delete pod -l=app=camunda-platform\\npod \\"ck-np-chaos-day-job-push-zeebe-0\\" deleted\\npod \\"ck-np-chaos-day-job-push-zeebe-1\\" deleted\\npod \\"ck-np-chaos-day-job-push-zeebe-2\\" deleted\\npod \\"ck-np-chaos-day-job-push-zeebe-gateway-68695d9cb5-hj2pf\\" deleted\\n```\\n\\n#### Verify steady state\\n\\n```\\n$ zbchaos verify readiness\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nAll Zeebe nodes are running.\\n$ zbchaos verify instance-creation --awaitResult\\nFailed to retrieve SaaS CRD, fallback to self-managed mode. the server could not find the requested resource\\nThe steady-state was successfully verified!\\n```\\n\\n### Result\\n\\nAgain we were able to show that job push is resilient, and can even handle a complete cluster restart.\\n\\n![](job-push-cluster-restart.png)\\n\\n\\n## Found Bugs\\n\\n* On restart (especially on cluster restart) it looks like job push engine metrics are counted multiple times\\n* [We found a place where we should better handle the exception in pushing async.](https://github.com/camunda/camunda/blob/a86decce9a46218798663e3466267a49adef506e/transport/src/main/java/io/camunda/zeebe/transport/stream/impl/RemoteStreamPusher.java#L55-L56C14)"},{"id":"/2023/11/30/Job-push-overloading","metadata":{"permalink":"/zeebe-chaos/2023/11/30/Job-push-overloading","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-11-30-Job-push-overloading/index.md","source":"@site/blog/2023-11-30-Job-push-overloading/index.md","title":"Job push overloading","description":"In today\'s chaos day we (Nicolas and I) want to verify how job push behaves and in general, the Zeebe system when we have slow workers.","date":"2023-11-30T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.585,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"},{"name":"Nicolas Pepin-Perreault","title":"Senior Software Engineer @ Zeebe","url":"https://github.com/npepinpe","imageURL":"https://github.com/npepinpe.png","key":"nicolas","page":null}],"frontMatter":{"layout":"posts","title":"Job push overloading","date":"2023-11-30T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":["zell","nicolas"]},"unlisted":false,"prevItem":{"title":"Job push resiliency","permalink":"/zeebe-chaos/2023/12/06/Job-Push-resiliency"},"nextItem":{"title":"Hot backups impact on processing","permalink":"/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing"}},"content":"In today\'s chaos day we (Nicolas and I) want to verify how job push behaves and in general, the Zeebe system when we have slow workers.\\n\\n**TL;DR;** Right now it seems that even if we have a slow worker it doesn\'t impact the general system, and only affects the corresponding process instance, not other instances. We found no unexpected issues, everything performed pretty well. \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFirstly we want to verify that job push will not overload a worker or gateway when workers are slow.\\n\\n### Expected\\n\\nWe expect that if the workers are slowing down, the load is distributed to other workers (if available), and it is expected that the general performance (of the affected process instance) should be slowed down. We wouldn\'t expect any restarts/failures on the gateway or workers.\\n\\n### Actual\\n\\n\\nWe deployed a normal benchmark, with [default configurations](https://github.com/camunda/camunda/blob/main/benchmarks/setup/default/values.yaml).\\n\\n\\nWe slowed the workers down, in the sense that we changed [the completionDelay to 1250 ms](https://github.com/camunda/zeebe-benchmark-helm/blob/main/charts/zeebe-benchmark/templates/worker.yaml#L30)\\n\\n![](exp1-general.png)\\n\\nThe throughput is lower than normal, as expected.\\n\\nWe see no significant increase in memory usage on the gateway, nor any outages because of this.\\n\\n![](exp1-gw-memory.png)\\n\\nWe see that a high amount of job pushes fail (due to capacity constraints now in the workers).\\n!Jobs are yielded back to the engine.\\n\\n![](exp1-records.png)\\n\\n\\nSo far so good, first experiment worked as expected :white_check_mark:\\n\\n\\n## Second Chaos Experiment \\n\\nThe normal scenario when something is slow is for a user to scale up. This is what we did in the next experiment, we scaled the workers to 10 replicas (from 3), to verify how the system behaves in this case.\\n\\n\\nSomething to keep in mind when the completion delay is 1250ms, we [multiply the activation timeout by 6 in our workers](https://github.com/camunda/camunda/blob/7002d53a079c06ab3a94f5485f022681a41dc9ed/benchmarks/project/src/main/java/io/camunda/zeebe/Worker.java#L113). This means completionDelay: 1250 -> job timeout 7.5s\\n\\n### Expected\\n\\nWe expect that we can reach a higher throughput.\\n\\n### Actual\\n\\nScaling the workers to 10:\\n\\n```sh\\nk scale deployment worker --replicas=10\\n```\\n\\n![](exp2-general.png)\\n\\nWe can see that after scaling we can complete more jobs.\\n\\n\\nThe gateway memory seems to be not really affected.\\n![](exp2-gw-mem.png)\\n\\nIn the job push metrics we see less job push failures.\\n\\n![](exp2-job-push.png)\\n\\nWhen we check the written records we can see a decrease in yield, but an increase in timeouts. The reason is that we have to try several workers before giving it back.\\n\\n![](exp2-records.png)\\n![](exp2-records2.png)\\n\\n\\nExperiment two worked as expected. :white_check_mark:\\n\\n## Third Chaos Experiment \\n\\nIn a real-world scenario, it will not happen if you have a slow dependency, for which for example a worker waits that you can scale and this will solve your problems. Likely you will get even slower because more pressure is put on the dependency. To mimic this scenario we experimented with increasing the completion time again. A completion delay set to 2500ms, means a job timeout of 15s.\\n\\n### Expected\\n\\nWe expect after slowing down all workers again, that our throughput goes down again, but we should see no general error. Potentially a slight memory increase because of buffering of jobs.\\n\\nThis also means more yields and fewer timeouts.\\n\\n### Actual\\n\\nAs expected again we see a drop in throughput, but it is still a bit higher than at the first experiment.\\n\\n![](exp3-general.png)\\n\\nNo difference at all in the memory consumption, by the gateway.\\n\\n![](exp3-memory.png)\\n\\n\\nIn the records we can also again see that yield increase, and timeouts have been decreased.\\n\\n![](exp3-records.png)\\n\\n\\nExperiment three worked as expected. :white_check_mark:\\n\\n## Several further experiments\\n\\nWe did several further experiments where we scaled the workers, played with the completion delay, reduced the starter load etc. At some point, we reached a state size that was too big (~2 Gig) such that this impacted our processing. We had to drain the cluster and stop the starters completely.\\n\\n\\nInterestingly was that when we reduced the completion delay, we just had a slight increase in completion, when we scaled down the workers (marked with the annotation in the graph), to reduce activations, we saw no difference. \\n\\n![](exp6-general.png)\\n\\n\\nOnly when we hit a certain threshold in RocksDb (it seems to be at least), the completion went up by a lot.\\n\\n\\n![](exp6-state.png)\\n\\nThis is because the record processing latency was heavily reduced (likely the commit latency or iteration latency in RocksDb).\\n\\n![](exp6-latency.png)\\n\\n\\n\\n## Experiment with worker impact\\n\\nWe wanted to understand and experiment with the impact of a slow worker on different process instances. \\n\\nTo see such an impact in our metrics we had to patch our current execution metrics, such that includes the BPMN processId, so we can differentiate between execution times of different processes.\\n\\nSee the related branch for more details [ck-latency-metrics](https://github.com/camunda/camunda/tree/ck-latency-metrics)\\n\\n\\nFurthermore, a new process model was added `slow-task.bpm` and new deployments to create such instances and work on them. The process model was similar to the benchmark model, only the job type has been changed.\\n\\n### Expected\\n\\nTo verify was that whether a slow worker would impact other instances, this was uncertain territory we were hitting. \\n\\nTo be honest we expected it would affect them.\\n\\n### Actual\\n\\n\\nWe started the benchmark (default configs for broker and gateway), with additional configurations:\\n\\n * benchmark starter with 75 PI/s rate\\n * 3 benchmark worker (60 capacity) and completion delay of 50 ms\\n * slow-task starter with 75 PI/s rate\\n * 3 slow-worker (60 capacity) and a completion delay of 50 ms (at the begin)\\n\\n\\n![](slow-exp-base.png)\\n\\nWe can see based on the metrics that the execution latency is the same for both process instances, and we are able to complete our 150 PI/s.\\n\\n![](slow-exp-base-general.png) \\n\\n\\nWe slowed now the worker for the type `slow-task` down to a completion delay of 2500ms.\\n\\n![](slow-exp-slow-worker-2500ms-records.png)\\n\\nWe can see that we start to get `Job.YIELD` commands from the gateway, and we can see that the process instance execution is slowed down.\\n\\n![](slow-exp-slow-worker-2500ms.png)\\n\\nInterestingly that is only for the affected process instance, which we wanted to validate/verify. \\n\\n\\n### Reasoning\\n\\nOur first assumption was that both instance latencies would be impacted, because are writing YIELD commands, instead of being able to complete them. \\n\\n\\nBut another consequence comes into play. If fewer jobs are worked on, there are also fewer jobs completed, this means fewer process instances have to be continued (with batch processing until the end).\\n\\nThis means a load of yield underweights the normal load of job completions, with additional process instance continuation. That was an interesting insight for us.\\n\\n## Result\\n\\nRight now it seems that even if we have a slow worker it doesn\'t impact badly the general system, and only affects the corresponding process instance, not other instances.\\n\\nWhat we should or need to investigate further what if the job completion delay is much larger than the timeout. This is something we might want to test soon."},{"id":"/2023/11/07/Hot-backups-impact-on-processing","metadata":{"permalink":"/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-11-07-Hot-backups-impact-on-processing/index.md","source":"@site/blog/2023-11-07-Hot-backups-impact-on-processing/index.md","title":"Hot backups impact on processing","description":"Today, we want to experiment with hot backups in SaaS and a larger runtime state in Zeebe and how it impacts the ongoing processing in Zeebe (or not?). This is part of the investigation of a recently created bug issue we wanted to verify/reproduce #14696.","date":"2023-11-07T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.82,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Hot backups impact on processing","date":"2023-11-07T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Job push overloading","permalink":"/zeebe-chaos/2023/11/30/Job-push-overloading"},"nextItem":{"title":"Using Large Multi-Instance","permalink":"/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance"}},"content":"Today, we want to experiment with hot backups in SaaS and a larger runtime state in Zeebe and how it impacts the ongoing processing in Zeebe (or not?). This is part of the investigation of a recently created bug issue we wanted to verify/reproduce [#14696](https://github.com/camunda/camunda/issues/14696).\\n\\n**TL;DR;** We were able to prove that hot backups are indeed not impacting overall processing throughput in Zeebe. We found that having a full Elasticsearch disk might impact or even fail your backups, which is intransparent to the user.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFor the experiment, we have set up a Camunda SaaS cluster (G3-M configuration), and run the [cloud benchmark](https://github.com/camunda/camunda/tree/main/benchmarks/setup/cloud-default) workload against it. During the experiment, we will run a stable load, which will cause to increase in the runtime state. We will create/initiate in different stages backups to verify the impact on processing depending on state size.\\n\\nWe kept the starter rate (creation of process instance 100 PI/s) but reduced the worker capacity and replicas.\\n\\n\\n### Expected\\n\\nHot backups were built with the premise of not disrupting the processing throughput in Zeebe, which is why we define the following hypothesis:\\n\\n> **Hypothesis**\\n>\\n> Creating hot backups should not impact Zeebe\'s _processing throughput _no_ matter how_ large the runtime state is in Zeebe.\\n\\n### Actual\\n\\n\\nWe created a cluster in the Camunda SaaS environment (in our internal stage).\\n\\n#### Step one\\n\\nWe created a first backup to verify that it works without issues.\\n\\n##### Result\\n\\nSuccess on stage one creating a backup with no actual state.\\n\\n![first-backup](first-backup.png)\\n\\n\\n#### Step two\\n\\nWe started a stable load as mentioned [above](#chaos-experiment). After reaching around ~100 MB runtime state at each partition we triggered a backup.\\n\\n##### Result\\n\\n![](general-sec-bk.png)\\n\\nThe backup was successful and we were not able to observe any disruption in the processing throughput. We can see that during the backup is taken the exporting is paused (which is expected) and afterwards it is starting to export again. :white_check_mark:\\n\\n#### Step three\\n\\nAt a later stage, we tried to take a backup again with around ~300MB of runtime state in Zeebe.\\n\\n##### Result\\n\\nBased on the output from Console the backup was successful and took around one hour.\\n\\n![third-console](sec-bk-console.png)\\n\\nBased on our internal metrics we can also see that there is no impact on the processing throughput\\n\\n![general-third](general-third-bk.png)\\n\\n\\nWhat is unclear to me is that it looks like we only took a backup of partition two. This needs to be further investigated, it might be also that the metrics are just confusing since it is resetting after the pod restarts.:bug:\\n\\n#### Step four\\n\\nHere we come into struggle after running the load on the cluster for quite some time we reached a runtime state size of ~1 gig. Furthermore, we filled our Elasticsearch disk tremendously.\\n\\n##### Result\\n\\nAt this time we were no longer able to successfully create backups. Here I tried it first without interacting with the cluster. It failed after 1.5 hours, which is potentially the timeout.\\n\\n![](third-bk-console.png)\\n\\n\\nThe backup failed, because of elastic was full. \\n\\n\\n![elastic-full](deleting-indices4.png)\\n\\nI went ahead to remove some data from Elastic to keep experimenting.\\n\\n![elastic-indices](deleting-indices3.png)\\n![delete-indices](deleting-indices2.png)\\n\\n#### Step five\\nAfter cleaning the Elasticsearch we retried taking a backup again. At this step, we already reached a runtime state of ~1.25G for each partition, which is quite huge.\\n\\n##### Result\\n\\nThe backup took quite a while and failed again.\\n\\n\\n![last-bk](last-bk-failed.png)\\n\\nWhat we can see based on the times it is likely that it here timed out again. Taking a look at the metrics we see that a backup was processed in Zeebe.\\n\\nIt had no impact on the ongoing processing throughput.\\n\\n\\n![fifth-bk](general-fifth-bk.png)\\n\\nAt a later stage, we tried another backup with ~1.45G of runtime state even here we were not able to observe any issues related to the impact on processing throughput.\\n\\n\\n![sixth](general-sixth-bk.png)\\n\\n\\n### Conclusion\\n\\n:white_check_mark: We were able to prove that even on a large runtime state there is no observable impact on processing throughput in Zeebe. \\n\\nFurthermore, we have seen that having a large Elastic state (almost full disk) will impact taking backups and is likely to fail them. We might need to iterate here, whether we want to tune the backup strategy, give elastic more space when taking backups, or adjust watermarks, etc.\\n\\n#### Follow-ups\\n\\nWe have to investigate the marking of failed backups, whether it is because of a timeout in the Operator or whether these backups are failed. It looks to me like they are marked as failed, even if they may succeed.\\n\\nFurthermore, the completed backups seem to be misleading and are reset which causes inconsistent views."},{"id":"/2023/06/02/Using-Large-Multi-Instance","metadata":{"permalink":"/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-06-02-Using-Large-Multi-Instance/index.md","source":"@site/blog/2023-06-02-Using-Large-Multi-Instance/index.md","title":"Using Large Multi-Instance","description":"New day new chaos. In today\'s chaos day I want to pick up a topic, which had bothered people for long time. I created a chaos day three years ago around this topic as well.","date":"2023-06-02T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.77,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Using Large Multi-Instance","date":"2023-06-02T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Hot backups impact on processing","permalink":"/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing"},"nextItem":{"title":"Continuing SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle"}},"content":"New day new chaos. :skull: In today\'s chaos day I want to pick up a topic, which had bothered people for long time. I created a [chaos day three years ago](https://camunda.github.io/zeebe-chaos/2020/07/16/big-multi-instance/) around this topic as well. \\n\\nToday, we experiment with large multi-instances again. In the recent patch release [8.2.5](https://github.com/camunda/camunda/releases/tag/8.2.5) we fixed an issue with spawning larger multi instances. Previously if you have created a process instance with a large multi-instance it was likely that this caused to blacklist the process instance, since the multi-instance spawning ran into `maxMessageSize` limitations. \\n\\nThis means the process instance was stuck and was no longer executable. In Operate this was not shown and caused a lot of friction or confusion to users. With the recent fix, Zeebe should chunk even large collections into smaller batches to spawn/execute the multi-instance without any issues.\\n\\n**TL;DR;** We were able to see that even large multi-instances can be executed now. :white_check_mark: At some point, we experienced performance regressions (during creating new multi-instance elements) but the execution of the process instance doesn\'t fail anymore. One problem at a time, we will likely investigate further to improve the performance of such a use case.\\n\\nWhen we reached the `maxMessageSize` we got a rejection, if the input collection is too large we see some weird unexpected errors from NGINX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe do regularly game days in Camunda, and for such we also create projects to make incidents, etc. reproducible. In today\'s chaos day, I will reuse some code created by [Philipp Ossler](https://github.com/saig0), thanks for that :bow: Since we mimic in such game days customers, the process is a bit more complex than necessary for such chaos day, but I will keep it like that.\\n\\n![order-process](order-process.png)\\n\\nThe input collection `items`, which is used in the multi-instance is generated via:\\n\\n```shell\\n    // input size\\n    final var items = IntStream.range(0, size).mapToObj(i -> Map.ofEntries(\\n        entry(\\"id\\", i)\\n    )).toList();\\n```\\n\\nIn the following experiment, we will play around with the `size` value.\\n\\nFor the experiment, we will use a Camunda 8 SaaS cluster with the generation `Zeebe 8.2.5` (G3-S).\\n\\n### Expected\\n\\nWhen creating a process instance with a large collection, we expect based on the recent bug fix that the multi-instance creation is batched and created without issues. \\n\\nOne limiting factor might be the `maxMessageSize` with regard to the input collection, but in this case, I would expect that the creation of the process instance is already rejected before.\\n\\n### Actual\\n\\nBetween the following experiments, I always recreated the clusters, to reduce the blast radius and better understand and isolate the impact. \\n\\n#### Starting small (20k)\\n\\nIn previous versions, the multi-instance creation failed already quite early. For example in the game day reproducer project, we had a collection defined with `20.000` items, which we are now reusing for the start.\\n\\nThe creation of the process instance worked without any issues. We can observe in Operate the incremental creation of sub-process instances, which is great.\\n\\n![incremental-creation-20k](20k-operate-inc.png)\\n\\nWe can see in the metrics that batch processing is limited by only 2-4 commands in a batch. That is an interesting fact that might explain why it takes a while until all instances of the multi-instance sub-process are created. We can even see rollbacks during batch processing, visible in the \\"Number of batch processing retries\\" panel.\\n\\n![processing-metrics-20k](20k-processing-metrics.png)\\n\\nThe processing queue seems to increase dramatically.\\n\\nAfter a while, we can see that all 20k instances are created without any bigger issues. :rocket:\\n\\n![complete-20k](20k-operate-complete.png)\\n\\nIt took around 10 minutes. Taking a look at the metrics again we see that in between big command batches have been created/processed, which allowed us to reduce the processing queue.\\n\\n![processing-metrics-20k-pt2](20k-processing-metrics-2.png)\\n\\nIn between the backpressure was quite high, but after the creation of all instances, the cluster is in a healthy state again. The creation of such multi-instance worked :white_check_mark:\\n\\n![general-metrics-20k](20k-general-metrics.png)\\n\\n#### Increase collection (200k)\\n\\nAgain, the creation of such a process instance was not a problem itself. We can observe the creation of the sub-process instances (multi-instance) in Operate, which happens incrementally.\\n\\n![incremental-creation-200k](200k-operate-inc.png)\\n\\nIt takes ages until the instances are created (After 3h ~66k instances are created). Again we see here small chunks of batches, and there are also rollbacks during batch processing.\\n\\n![processing-metrics-200k](200k-processing-metrics.png)\\n\\nThe processing of that partitions is in this case blocked by the multi-instance creation, we can see that on the 100% back pressure. :x:\\n\\n![general-metrics-200k](200k-general-metrics.png)\\n\\nEven after one hour, not all instances are created (not even 20k), it takes longer than before the creation of 20.000 instances.\\n\\n![incremental-creation-200k-part2](200k-operate-inc2.png)\\n\\n#### Make it really big (2 million)\\n\\nTo escalate this even more I increase the input collection again by a factor of 10 to 2 million.\\n\\nAfter creation, I see as a response the following log message in my log:\\n\\n```\\nFailed to create process instance of \'order-process\'\\n\\nio.camunda.zeebe.client.api.command.ClientStatusException: HTTP status code 502\\ninvalid content-type: text/html\\nheaders: Metadata(:status=502,date=Fri, 02 Jun 2023 11:44:57 GMT,content-type=text/html,strict-transport-security=max-age=63072000; includeSubDomains,content-length=150)\\nDATA-----------------------------\\n<html>\\n<head><title>502 Bad Gateway</title></head>\\n<body>\\n<center><h1>502 Bad Gateway</h1></center>\\n<hr><center>nginx</center>\\n</body>\\n</html>\\n\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.createProcessInstance(ProcessApplication.java:90) ~[classes/:na]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.createInstanceOfProcess(ProcessApplication.java:71) ~[classes/:na]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.run(ProcessApplication.java:58) ~[classes/:na]\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:791) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:775) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:345) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1343) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1332) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.main(ProcessApplication.java:46) ~[classes/:na]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: HTTP status code 502\\ninvalid content-type: text/html\\nheaders: Metadata(:status=502,date=Fri, 02 Jun 2023 11:44:57 GMT,content-type=text/html,strict-transport-security=max-age=63072000; includeSubDomains,content-length=150)\\nDATA-----------------------------\\n<html>\\n<head><title>502 Bad Gateway</title></head>\\n<body>\\n<center><h1>502 Bad Gateway</h1></center>\\n<hr><center>nginx</center>\\n</body>\\n</html>\\n\\n\\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) ~[na:na]\\n\\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) ~[na:na]\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\t... 9 common frames omitted\\n```\\n\\n\\n\\nI tried to incrementally decrease the input collection until it is working again, when reaching 250k I finally see a better understandable error.\\n\\n```shell\\n2023-06-02 13:53:51.485 ERROR 29870 --- [           main] i.c.cloud.gameday.ProcessApplication     : Failed to create process instance of \'order-process\'\\n\\nio.camunda.zeebe.client.api.command.ClientStatusException: Command \'CREATE\' rejected with code \'EXCEEDED_BATCH_RECORD_SIZE\': \\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.createProcessInstance(ProcessApplication.java:90) ~[classes/:na]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.createInstanceOfProcess(ProcessApplication.java:71) ~[classes/:na]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.run(ProcessApplication.java:58) ~[classes/:na]\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:791) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:775) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:345) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1343) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1332) ~[spring-boot-2.5.2.jar:2.5.2]\\n\\tat io.camunda.cloud.gameday.ProcessApplication.main(ProcessApplication.java:46) ~[classes/:na]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNKNOWN: Command \'CREATE\' rejected with code \'EXCEEDED_BATCH_RECORD_SIZE\': \\n\\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396) ~[na:na]\\n\\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) ~[na:na]\\n\\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-8.0.5.jar:8.0.5]\\n\\t... 9 common frames omitted\\nCaused by: io.grpc.StatusRuntimeException: UNKNOWN: Command \'CREATE\' rejected with code \'EXCEEDED_BATCH_RECORD_SIZE\': \\n\\tat io.grpc.Status.asRuntimeException(Status.java:535) ~[grpc-api-1.45.1.jar:1.45.1]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:562) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:743) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:722) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) ~[grpc-core-1.45.1.jar:1.45.1]\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[na:na]\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[na:na]\\n\\tat java.base/java.lang.Thread.run(Thread.java:833) ~[na:na]\\n\\n2023-06-02 13:53:51.485  INFO 29870 --- [           main] i.c.cloud.gameday.ProcessApplication     : Created process instances with large collection. [order-id: \'ba65b59b-1584-48bb-af05-3724ea15fac9\']\\n\\n```\\n\\n### Results\\n\\nAs we have seen above we are able now to create much larger multi instances than before, with some drawbacks in performance, which needs to be investigated further.\\n\\nWhen reaching a certain limit (maxMessageSize) we get a described rejection by the broker, until we reach the limit of NGINX where the description is not that optimal. Here we can and should improve further.\\n\\n\\n## Found Bugs\\n\\n  * in a previous test I run into https://github.com/camunda/camunda/issues/12918\\n  * Related bug regarding the input collection https://github.com/camunda/camunda/issues/12873"},{"id":"/2023/05/19/Continuing-SST-Partitioning-toggle","metadata":{"permalink":"/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-05-19-Continuing-SST-Partitioning-toggle/index.md","source":"@site/blog/2023-05-19-Continuing-SST-Partitioning-toggle/index.md","title":"Continuing SST Partitioning toggle","description":"Today we want to continue with the experiment from last Chaos day, but this time","date":"2023-05-19T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":8.105,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Continuing SST Partitioning toggle","date":"2023-05-19T00:00:00.000Z","categories":["chaos_experiment","configuration"],"tags":["availability","data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Using Large Multi-Instance","permalink":"/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance"},"nextItem":{"title":"SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/15/SST-Partitioning-toggle"}},"content":"Today we want to continue with the experiment from [last Chaos day](../2023-05-15-SST-Partitioning-toggle/index.md), but this time\\nwith a bit more load. This should make sure that we trigger the compaction of RocksDB and cause the SST partitioning to happen, for real.\\n\\nThe reasons stay the same we want to find out whether it would be possible to enable and disable the flag/configuration without issues.\\n\\n**TL;DR;** Today\'s, experiments succeeded :rocket:. We were able to show that even with a higher number of process instances (bigger state) we can easily disable and enable the SST partitioning flag without issues. I also got a confirmation from a RocksDb contributor that our observations are correct, and that we can easily toggle this feature without issues.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nSimilar setup to the [last Chaos day](../2023-05-15-SST-Partitioning-toggle/index.md#chaos-experiment).\\nExcept this time we will enable Operate as well, in order to verify easily whether all instances have been completed.\\nOther than that we use the standard benchmark configuration, without clients.\\n\\nThe verification of the steady state will consist, of checking the readiness and healthiness of the cluster, via zbchaos and metrics. Furthermore, we will verify that we can access operate and that no instances are running. As defined in chaos engineering principles the process of a chaos experiment looks always the same, Verify the steady state, introduce chaos, and verify the steady state.\\n\\nIn our first experiment, we will enable the SST partitioning.\\n\\n**First chaos action**\\n  * Deploy a process model (which contains a [simple model](https://github.com/camunda/zeebe-chaos/blob/main/go-chaos/internal/bpmn/one_task.bpmn))\\n  * Start 1000 process instances (PIs), with a service task\\n  * Enable the SST partitioning\\n  * Restart the cluster, and await readiness\\n  * Complete the jobs (in consequence the PIs)\\n\\nIn our second experiment, we will disable the SST partitioning again.\\n\\n**Second chaos action:**\\n\\n  * Start 1000 process instances (PIs), with a service task\\n  * Disable the SST partitioning\\n  * Restart the cluster, and await readiness\\n  * Complete the jobs (in consequence the PIs)\\n\\n### Expected\\n\\n> When operating a cluster, I can enable and disable the SST partitioning without an impact on executing existing process instances. Existing PIs should still be executable and completable.\\n\\n### Actual\\n\\nAs linked above I used again our [benchmark/setup](https://github.com/camunda/camunda/tree/main/benchmarks/setup) scripts to set up a cluster.\\n\\n```shell\\n$ diff ../default/values.yaml values.yaml \\n40c40\\n<   replicas: 3\\n---\\n>   replicas: 0\\n47c47\\n<   replicas: 1\\n---\\n>   replicas: 0\\n85a86\\n>     zeebe.broker.experimental.rocksdb.enableSstPartitioning: \\"false\\"\\n96a98,100\\n>     identity:\\n>       auth:\\n>         enabled: false\\n326c330\\n<     enabled: false\\n---\\n>     enabled: true\\n```\\n\\n\\n#### First Experiment: Verify Steady state\\nTo verify the readiness and run all actions I used the [zbchaos](https://github.com/camunda/zeebe-chaos/tree/zbchaos-v1.0.0) tool.\\n\\n```shell\\n$ zbchaos verify readiness\\nAll Zeebe nodes are running.\\n```\\n\\nLooking at the metrics shows that everything looks healthy. The only weird part is the topology panel which seems to be broken.\\n![start](start.png)\\n\\nWhen requesting the topology via `zbchaos` we retrieve this:\\n\\n```shell\\n$ zbchaos topology\\n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1684476531915 false false true false false 30 false -1 benchmark 30  }\\nNode      |Partition 1         |Partition 2         |Partition 3\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)\\n0         |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n```\\n\\nFor now, we assume the dashboard has an issue and continue with the experiment.\\n\\nWe are able to access operate without issues, and see no instances yet.\\n\\n![operate](operate.png)\\n\\n\\n#### First Experiment: Chaos Action\\n\\nAfter the verification stage, we start with our chaos action, injecting chaos into the system.\\nThe first step is to deploy the mentioned simple process model:\\n```shell\\n$ zbchaos deploy process -v\\nConnecting to zell-chaos\\nRunning experiment in self-managed environment.\\nPort forward to zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\nSuccessfully created port forwarding tunnel\\nDeploy file bpmn/one_task.bpmn (size: 2526 bytes).\\nDeployed process model bpmn/one_task.bpmn successful with key 2251799813685249.\\nDeployed given process model , under key 2251799813685249!\\n```\\n\\nThis is then as well visible in operate.\\n\\n![operate-process](operate-process.png)\\n\\nAs the next step, we will create 1000 process instances of our simple process model, with one service task.\\nFor that, we can [use a new functionality](https://github.com/camunda/zeebe-chaos/tree/zell-chaos-create-count-of-instances) of `zbchaos` I built for this chaos day.\\n\\nOn the first try, I had smaller issues, with timeouts etc.\\n```shell\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\n[299/999] Created process instance with key 6755399441056339 on partition 3.\\npanic: Expected to create 999 process instances, but timed out after 30s created 299 instances.\\n```\\n\\nThis is the reason why I had to retry the creations in the end the count is not exactly 1000 :)\\n```shell\\n./dist/zbchaos verify instance-count --instanceCount 697 -v --timeoutInSec 300\\n...\\n[695/697] Created process instance with key 4503599627372489 on partition 2.\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\n[696/697] Created process instance with key 6755399441057737 on partition 3.\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\n[697/697] Created process instance with key 2251799813687255 on partition 1.\\nThe steady state was successfully verified!\\n```\\n\\n![pi](operate-pi.png)\\n\\nNow we are coming to the interesting part. Enabling SST partitioning.\\n\\n```shell\\n$ diff ../default/values.yaml values.yaml \\n85a86\\n>     zeebe.broker.experimental.rocksdb.enableSstPartitioning: \\"true\\"\\n$ make update \\nhelm upgrade --namespace zell-chaos zell-chaos zeebe-benchmark/zeebe-benchmark -f values.yaml\\n```\\n\\n> **Note**\\n> Changing the configmap doesn\'t restart pods! We need to delete all Zeebe pods, to apply the configuration.\\n\\n```shell\\n$ k delete pod -l app=camunda-platform\\npod \\"zell-chaos-zeebe-0\\" deleted\\npod \\"zell-chaos-zeebe-1\\" deleted\\npod \\"zell-chaos-zeebe-2\\" deleted\\npod \\"zell-chaos-zeebe-gateway-7bbdf9fd58-8j7d6\\" deleted\\npod \\"zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\" deleted\\n```\\n\\n```shell\\n$ zbchaos verify readiness \\nAll Zeebe nodes are running.\\n```\\n\\nNow starting to complete the previously created jobs, we can use again a new feature in `zbchaos` ([which has been added during the chaos day](https://github.com/camunda/zeebe-chaos/tree/zell-chaos-create-count-of-instances))\\nUnfortunately, I missed using the verbose flag.\\n```shell\\n$ ./dist/zbchaos verify job-completion --jobCount 1001 --timeoutInSec 1200\\nThe steady-state was successfully verified!\\n```\\n\\n#### First Experiment: Verify Steady state\\n\\nThe job completions worked without issues. The metrics are looking good, the topology panel seems to work again as well.\\n\\n![complete](metrics-complete.png)\\n\\nIn operate we can see that there are no longer any running instances and all of them have been completed.\\n\\n![complete-operate](operate-complete.png)\\n![complete-operate2](operate-complete2.png)\\n\\nThe first part of the experiment worked as expected :white_check_mark:\\n\\n#### Second Experiment: Chaos Action\\n\\nWe are skipping the verification step, due to previous verification, we directly start with creating 1000 process instances.\\n```\\n$ ./dist/zbchaos verify instance-count --instanceCount 1000 -v --timeoutInSec 300\\n\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\n[1000/1000] Successful command sent, got response with key 2251799813690599 on partition 1.\\nThe steady-state was successfully verified!\\n```\\n\\n![second](second-exp.png)\\n\\nDisable the SST partitioning flag and update the cluster.\\n```shell\\n$ make update \\n$ k delete pod -l app=camunda-platform\\n$ zbchaos verify readiness\\nAll Zeebe nodes are running.\\n```\\n\\nComplete all jobs:\\n\\n```shell\\n$ ./dist/zbchaos verify job-completion --jobCount 1000 --timeoutInSec 1200 -v\\n[999/1000] Successful command sent, got response with key 6755399441061073 on partition 3.\\nSend job activate command, with job type \'benchmark-task\'\\n[1000/1000] Successful command sent, got response with key 2251799813690604 on partition 1.\\nThe steady-state was successfully verified!\\n```\\n\\n#### Second Experiment: Verify Steady state\\n\\nAgain the experiment succeeded, we were able to show that even with a higher number of process instances we can easily disable and enable the SST partitioning flag.\\n\\n![verify](second-exp-verify.png)\\n![op-complete](second-exp-operate-complete.png)\\n\\nIn the snapshots at we can see that some more files are used.\\n\\n![snap](second-exp-snap.png)\\n\\nBut in RocksDb metrics we see no real compaction going on, which is why we will retry the same with a higher amount of data.\\n\\n![rocks](second-exp-rocks.png)\\n\\n\\n#### SST Partitioning and compaction\\n\\nI tried to run the experiment again but with more data (~11K instances).\\n\\nEven when the metrics don\'t show the compaction, I was able to see in the RocksDB that compacting is happening.\\n\\nAround 11:56 between different loads\\n\\n![thirdrun](thirdrun.png)\\n\\nWe see in the metrics of RocksDB that nothing\\n\\n![rocks](compacting.png)\\n\\nBut when checking the RocksDB logs\\n\\n```\\n$ cat data/raft-partition/partitions/1/runtime/LOG.old.1684493206724692 \\n...\\n\\n2023/05/19-09:56:40.652111 140580419004160  Options.sst_partitioner_factory: SstPartitionerFixedPrefixFactory\\n\\n...\\n2023/05/19-09:56:41.354244 140579153618688 (Original Log Time 2023/05/19-09:56:41.354149) EVENT_LOG_v1 {\\"time_micros\\": 1684490201354123, \\"job\\": 2, \\"event\\": \\"compaction_finished\\", \\"compaction_time_micros\\": 374078, \\"compaction_time_cpu_micros\\": 72361, \\"output_level\\": 3, \\"num_output_files\\": 14, \\"total_output_size\\": 6283132, \\"num_input_records\\": 69787, \\"num_output_records\\": 39118, \\"num_subcompactions\\": 1, \\"output_compression\\": \\"NoCompression\\", \\"num_single_delete_mismatches\\": 0, \\"num_single_delete_fallthrough\\": 0, \\"lsm_state\\": [0, 0, 0, 14]}\\n2023/05/19-09:56:41.354763 140579153618688 [le/delete_scheduler.cc:77] Deleted file /usr/local/zeebe/data/raft-partition/partitions/1/runtime/000045.sst immediately, rate_bytes_per_sec 0, total_trash_size 0 max_trash_db_ratio 0.250000\\n2023/05/19-09:56:41.354786 140579153618688 EVENT_LOG_v1 {\\"time_micros\\": 1684490201354782, \\"job\\": 2, \\"event\\": \\"table_file_deletion\\", \\"file_number\\": 45}\\n2023/05/19-09:56:41.355217 140579153618688 [le/delete_scheduler.cc:77] Deleted file /usr/local/zeebe/data/raft-partition/partitions/1/runtime/000044.sst immediately, rate_bytes_per_sec 0, total_trash_size 0 max_trash_db_ratio 0.250000\\n2023/05/19-09:56:41.355247 140579153618688 EVENT_LOG_v1 {\\"time_micros\\": 1684490201355243, \\"job\\": 2, \\"event\\": \\"table_file_deletion\\", \\"file_number\\": 44}\\n2023/05/19-09:56:41.355765 140579153618688 [le/delete_scheduler.cc:77] Deleted file /usr/local/zeebe/data/raft-partition/partitions/1/runtime/000043.sst immediately, rate_bytes_per_sec 0, total_trash_size 0 max_trash_db_ratio 0.250000\\n```\\nWe see several lines which indicate the compaction.\\n\\n## Conclusion\\n\\n We have seen that even when we toggle the SST partitioning, we are able to make progress and our stored data is not impacted. This is a great out come since it means we can easily enable such configuration on existing clusters and gains the performance benefits for larger states as we have seen in previous benchmarks.\\n\\nI have posted a question related to this topic in the [RocksDb google group](https://groups.google.com/g/rocksdb/c/Ys-yZIznZwU) and I got a private answer which contains the following:\\n\\n> Partitioner is just a hinter when compaction should split the file. Default compaction is also splitting by file size. So it has no functional effect and you can change configuration anytime.\\n>\\n> Partitioner does not need to be simple prefix only, but one can use more complicated strategy.\\n\\nThis confirms our observation and makes it much more trustworthy.\\n\\n\\n## Found Bugs\\n\\n * Grafana Topology Panel seems to be buggy from time to time\\n * RocksDB compaction panel seems to show no data (might be related to a short time frame)"},{"id":"/2023/05/15/SST-Partitioning-toggle","metadata":{"permalink":"/zeebe-chaos/2023/05/15/SST-Partitioning-toggle","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-05-15-SST-Partitioning-toggle/index.md","source":"@site/blog/2023-05-15-SST-Partitioning-toggle/index.md","title":"SST Partitioning toggle","description":"On this chaos day I wanted to experiment with a new experimental feature we have released recently. The enablement of the partitioning of the SST files in RocksDB. This is an experimental feature from RocksDb, which we made available now for our users as well, since we have seen great benefits in performance, especially with larger runtime data.","date":"2023-05-15T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":6.695,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"SST Partitioning toggle","date":"2023-05-15T00:00:00.000Z","categories":["chaos_experiment","configuration"],"tags":["availability","data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Continuing SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle"},"nextItem":{"title":"Gateway Termination","permalink":"/zeebe-chaos/2023/04/06/gateway-termination"}},"content":"On this chaos day I wanted to experiment with a new experimental feature we have released recently. The [enablement of the partitioning of the SST files in RocksDB](https://github.com/camunda/camunda/pull/12483). This is an experimental feature from RocksDb, which we made available now for our users as well, since we have seen great benefits in performance, especially with larger runtime data.\\n\\nI wanted to experiment a bit with the SST partitioning and find out whether it would be possible to enable and disable the flag/configuration without issues.\\n\\n**TL;DR;** The first experiment was successful, it looks like we can enable and disable the partitioning without impacting the execution of one existing PI. We need to experiment a bit more with larger data sets to force RocksDB compaction, to be fully sure. \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFor our chaos experiment we set up again our [normal benchmark cluster](https://github.com/camunda/camunda/tree/main/benchmarks/setup), this time without any clients (no workers/starters).\\n\\nSetting all client replicas to zero:\\n```diff\\n$ diff default/values.yaml zell-chaos/values.yaml \\n40c40\\n<   replicas: 3\\n---\\n>   replicas: 0\\n47c47\\n<   replicas: 1\\n---\\n>   replicas: 0\\n``` \\n\\nThe experiment we want to do on this chaos day will look like the following:\\n\\n**First part:** \\n\\n * Verify steady state:\\n   * verify the readiness of the cluster \\n   * deploy a process model (which contains a [simple model](https://github.com/camunda/zeebe-chaos/blob/main/go-chaos/internal/bpmn/one_task.bpmn))\\n * Chaos Action: \\n   * start a process instance (PI), with a service task\\n   * enable the SST partitioning\\n   * restart the cluster\\n   * verify the readiness\\n   * verify that job is activatable\\n   * complete the job (in consequence the PI)\\n   \\n**Second part:**\\n\\n* Chaos Action:\\n    * start a process instance (PI), with a service task\\n    * disable the SST partitioning\\n    * restart the cluster\\n    * verify the readiness\\n    * verify that job is activatable\\n    * complete the job (in consequence the PI)\\n\\n\\n### Expected\\n\\nWhen operating a cluster, I can enable the SST partitioning without an impact on executing existing process instances. Existing PIs should still be executable and completable.\\n\\n### Actual\\n\\nAs linked above I used again our [benchmark/setup](https://github.com/camunda/camunda/tree/main/benchmarks/setup) scripts to set up a cluster.\\n\\n#### First Part: Verify Steady state\\nTo verify the readiness and run all actions I used the [zbchaos](https://github.com/camunda/zeebe-chaos/tree/zbchaos-v1.0.0) tool.\\n\\nVerifying readiness is fairly easy with zbchaos.\\n\\n```shell\\n$ zbchaos verify readiness -v\\nConnecting to zell-chaos\\nRunning experiment in self-managed environment.\\nPod zell-chaos-zeebe-0 is in phase Pending, and not ready. Wait for some seconds.\\n[...]\\nPod zell-chaos-zeebe-0 is in phase Running, and not ready. Wait for some seconds.\\nPod zell-chaos-zeebe-0 is in phase Running, and not ready. Wait for some seconds.\\nAll Zeebe nodes are running.\\n```\\n\\nWe then deploy the mentioned simple process model:\\n```shell\\n$ zbchaos deploy process -v\\nConnecting to zell-chaos\\nRunning experiment in self-managed environment.\\nPort forward to zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\nSuccessfully created port forwarding tunnel\\nDeploy file bpmn/one_task.bpmn (size: 2526 bytes).\\nDeployed process model bpmn/one_task.bpmn successful with key 2251799813685249.\\nDeployed given process model , under key 2251799813685249!\\n```\\n\\n#### First Part: Chaos Action\\n\\nAs the first step in the chaos action we create a process instance. \\n\\n```shell\\n$ zbchaos verify instance-creation -v\\nConnecting to zell-chaos\\nRunning experiment in self-managed environment.\\nPort forward to zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\nSuccessfully created port forwarding tunnel\\nSend create process instance command, with BPMN process ID \'benchmark\' and version \'-1\' (-1 means latest) [variables: \'\', awaitResult: false]\\nCreated process instance with key 2251799813685251 on partition 1, required partition 1.\\nThe steady-state was successfully verified!\\n```\\n\\nNext, we enable the SST partitioning in our broker configuration, we can do this in the `values.yaml` file and run a `helm update`.\\n\\n```shell\\n$ diff ../default/values.yaml values.yaml \\n85a86\\n>     zeebe.broker.experimental.rocksdb.enableSstPartitioning: \\"true\\"\\n```\\n\\n```shell\\n$ make update\\nhelm upgrade --namespace zell-chaos zell-chaos zeebe-benchmark/zeebe-benchmark -f values.yaml\\nRelease \\"zell-chaos\\" has been upgraded. Happy Helming!\\nNAME: zell-chaos\\nLAST DEPLOYED: Mon May 15 15:54:24 2023\\nNAMESPACE: zell-chaos\\nSTATUS: deployed\\nREVISION: 2\\nNOTES:\\n# Zeebe Benchmark\\n\\nInstalled Zeebe cluster with:\\n\\n * 3 Brokers\\n * 2 Gateways\\n\\nThe benchmark is running with:\\n\\n * Starter replicas=0\\n * Worker replicas=0\\n * Publisher replicas=0\\n * Timer replicas=0\\n```\\n\\n> **Note**\\n> Changing the configmap doesn\'t restart pods! We need to delete all Zeebe pods, to apply the configuration.\\n\\n```shell\\n$ k delete pod -l app=camunda-platform\\npod \\"zell-chaos-zeebe-0\\" deleted\\npod \\"zell-chaos-zeebe-1\\" deleted\\npod \\"zell-chaos-zeebe-2\\" deleted\\npod \\"zell-chaos-zeebe-gateway-7bbdf9fd58-8j7d6\\" deleted\\npod \\"zell-chaos-zeebe-gateway-7bbdf9fd58-dl97j\\" deleted\\n```\\n\\nNext we can use `zbchaos verify readiness` again to await the readiness of the cluster.\\n```\\nAll Zeebe nodes are running.\\n```\\n\\nTaking a look at the logs of the broker we can also see that the broker configuration was correctly set:\\n\\n```\\n   \\\\\\"disableWal\\\\\\" : true,\\\\n      \\\\\\"enableSstPartitioning\\\\\\" : true\\\\n    }\\n```\\n\\n> **Note**\\n> Right now zbchaos can\'t complete an job (missing feature). We use zbctl for that, we need to port-forward to the gateway in order to send the commands.\\n\\n```shell\\n$ k port-forward svc/zell-chaos-zeebe-gateway 26500\\nForwarding from 127.0.0.1:26500 -> 26500\\nForwarding from [::1]:26500 -> 26500\\n\\n```\\n\\nActivating the right job.\\n```shell\\n$ zbctl --insecure activate jobs benchmark-task\\n{\\n  \\"jobs\\":  [\\n    {\\n      \\"key\\":  \\"2251799813685256\\",\\n      \\"type\\":  \\"benchmark-task\\",\\n      \\"processInstanceKey\\":  \\"2251799813685251\\",\\n      \\"bpmnProcessId\\":  \\"benchmark\\",\\n      \\"processDefinitionVersion\\":  1,\\n      \\"processDefinitionKey\\":  \\"2251799813685249\\",\\n      \\"elementId\\":  \\"task\\",\\n      \\"elementInstanceKey\\":  \\"2251799813685255\\",\\n      \\"customHeaders\\":  \\"{}\\",\\n      \\"worker\\":  \\"zbctl\\",\\n      \\"retries\\":  3,\\n      \\"deadline\\":  \\"1684173544716\\",\\n      \\"variables\\":  \\"{}\\"\\n    }\\n  ]\\n}\\n```\\nCompleting the job and the PI.\\n```shell\\n$ zbctl complete job 2251799813685256 --insecure\\nCompleted job with key \'2251799813685256\' and variables \'{}\'\\n```\\n\\n#### Second Part:\\n\\nCreate again a process instance `$ zbchaos verify instance-creation`\\n\\n```\\nCreated process instance with key 2251799813685263 on partition 1, required partition 1.\\nThe steady-state was successfully verified!\\n```\\n\\nDisabling the configuration again, and running the update.\\n```shell\\n$ diff default/values.yaml zell-chaos/values.yaml \\n85a86\\n>     zeebe.broker.experimental.rocksdb.enableSstPartitioning: \\"false\\"\\n\\n\\n$ make update \\nhelm upgrade --namespace zell-chaos zell-chaos zeebe-benchmark/zeebe-benchmark -f values.yaml\\nRelease \\"zell-chaos\\" has been upgraded. Happy Helming!\\nNAME: zell-chaos\\nLAST DEPLOYED: Mon May 15 20:00:53 2023\\n...\\n\\n$ k delete pod -l app=camunda-platform\\n$ zbchaos verify readiness\\nAll Zeebe nodes are running.\\n```\\n\\nAgain the job completion worked without problems (skipping here the port-forward and activate output)\\n\\n```\\n$ zbctl complete job 2251799813685268 --insecure\\nCompleted job with key \'2251799813685268\' and variables \'{}\'\\n```\\n\\n:white_check_mark: The experiment was successful. \\n\\n#### Further investigation\\n\\n![](general-after2.png)\\n\\nWhen running the experiment I also observed the metrics of the cluster and was not able to see any differences in the snapshot file counts, which we would expect on the SST partitioning (there should be more files).\\n\\nBefore the experiment:\\n![](snapshot-before.png)\\n\\nAfter the experiment, we still see that for each partition we have around ~6 files.\\n![](snapshot-after2.png)\\n\\nIn order to make sure whether the options have been applied correctly I investigated the RocksDB log files and option files.\\n\\nIn the current LOG file, we can see the current options printed, which is indeed the disabled partitioner. Since this is the default as well it is not a proof yet.\\n\\n```\\n2023/05/15-18:01:46.223234 139711509092096 [/column_family.cc:621] --------------- Options for column family [default]:\\n2023/05/15-18:01:46.223237 139711509092096               Options.comparator: leveldb.BytewiseComparator\\n2023/05/15-18:01:46.223239 139711509092096           Options.merge_operator: None\\n2023/05/15-18:01:46.223241 139711509092096        Options.compaction_filter: None\\n2023/05/15-18:01:46.223242 139711509092096        Options.compaction_filter_factory: None\\n2023/05/15-18:01:46.223244 139711509092096  Options.sst_partitioner_factory: None\\n```\\n\\nWhat we can see in the runtime folder of the partition is that there exist two Options files, an older one `OPTIONS-000014`, and a newer one `OPTIONS-000023`.\\n\\nThe older one contains the expected configuration for the SST partitioning:\\n\\n```shell\\n$ cat OPTIONS-000014 \\n[CFOptions \\"default\\"]\\n...\\n  sst_partitioner_factory={id=SstPartitionerFixedPrefixFactory;length=8;}\\n```\\n\\nThe most recent options file has the configuration set to null.\\n```shell\\n\\n$ cat OPTIONS-000023\\n[CFOptions \\"default\\"]\\n...\\nsst_partitioner_factory=nullptr\\n\\n```\\n\\nWe can see that the current snapshot only copied the most recent options file:\\n\\n```shell\\n$ ll ../snapshots/188-4-230-244\\ntotal 56\\ndrwxr-xr-x 2 root root 4096 May 15 18:13 ./\\ndrwxr-xr-x 3 root root 4096 May 15 18:13 ../\\n...\\n-rw-r--r-- 2 root root 7015 May 15 18:01 OPTIONS-000023\\n-rw-r--r-- 1 root root   92 May 15 18:13 zeebe.metadata\\n```\\n\\n### Conclusion\\n\\nWe were able to toggle the SST partitioning flag without problems back and forth. We were able to make still progress on an existing process instance, which we wanted to prove.\\n\\nNevertheless, we need to prove this once more for multiple process instances (100-1000 PIs), which cause or forces compaction of the SST files. Right now I\'m not 100% convinced whether this experiment was enough, but it was a good first step."},{"id":"/2023/04/06/gateway-termination","metadata":{"permalink":"/zeebe-chaos/2023/04/06/gateway-termination","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-04-06-gateway-termination/index.md","source":"@site/blog/2023-04-06-gateway-termination/index.md","title":"Gateway Termination","description":"In today\'s chaos day, we wanted to experiment with the gateway and resiliency of workers.","date":"2023-04-06T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"resiliency","permalink":"/zeebe-chaos/tags/resiliency"}],"readingTime":7.24,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Gateway Termination","date":"2023-04-06T00:00:00.000Z","categories":["chaos_experiment","gateway"],"tags":["availability","resiliency"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/15/SST-Partitioning-toggle"},"nextItem":{"title":"Recursive call activity","permalink":"/zeebe-chaos/2023/02/23/Recursive-call-activity"}},"content":"In today\'s chaos day, we wanted to experiment with the gateway and resiliency of workers.\\n\\nWe have seen in recent weeks some issues within our benchmarks when gateways have been restarted,\\nsee [zeebe#11975](https://github.com/camunda/camunda/issues/11975).\\n\\nWe did a similar experiment [in the past](../2022-02-15-Standalone-Gateway-in-CCSaaS/index.md),\\ntoday we want to focus on self-managed ([benchmarks with our helm charts](https://helm.camunda.io/)).\\nIdeally, we can automate this as well soon.\\n\\nToday [Nicolas](https://github.com/npepinpe) joined me on the chaos day :tada: \\n\\n**TL;DR;** We were able to show that the workers (clients) can reconnect after a gateway is shutdown :white_check_mark:\\nFurthermore, we have discovered a potential performance issue on lower load, which impacts process execution latency ([zeebe#12311](https://github.com/camunda/camunda/issues/12311)). \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe will use our [Zeebe benchmark helm charts](https://github.com/camunda/zeebe-benchmark-helm) to set up the test cluster, and\\nour helper scripts [here](https://github.com/camunda/camunda/tree/main/benchmarks/setup).\\n\\n### Setup:\\n\\nWe will run with the default benchmark configuration, which means:\\n\\n * three brokers\\n * three partitions\\n * replication count three\\n * two gateways\\n\\nWe will run the benchmark with a low load, 10 process instances per second created and completed. For that,\\nwe deploy one starter and worker. This reduces the blast radius and allows us to observe more easily how the workers\\nbehave when a gateway is restarted.\\n\\nDuring the experiment, we will use our [grafana dashboard](https://github.com/camunda/camunda/tree/main/monitor/grafana) to\\nobserve to which gateway the worker will connect and which gateway we need to stop/restart.\\n\\n\\n```shell\\nLAST DEPLOYED: Thu Apr  6 10:21:27 2023\\nNAMESPACE: zell-chaos\\nSTATUS: deployed\\nREVISION: 1\\nNOTES:\\n# Zeebe Benchmark\\n\\nInstalled Zeebe cluster with:\\n\\n * 3 Brokers\\n * 2 Gateways\\n\\nThe benchmark is running with:\\n\\n * Starter replicas=1\\n * Worker replicas=1\\n * Publisher replicas=0\\n * Timer replicas=0\\n```\\n\\n### Expected\\n\\n\\nWhen we terminate a gateway to which the worker has connected, **we expect** that the worker connects to the different\\nreplica and starts completing jobs again.\\n\\nThe performance drop is expected to be not significant, or at least should recover fast.\\n\\n### Actual\\n\\nWe will run the experiment in two ways, first via terminating the gateway (using [zbchaos](https://github.com/camunda/zeebe-chaos/releases/tag/zbchaos-v1.0.0))\\nand later via scaling down the gateway deployment to one replica. \\n\\nWe want to verify whether this makes any difference, since terminating will cause Kubernetes to recreate immediately the pod.\\n\\n#### Termination\\n\\nBefore we start the experiment we check our current deployed state:\\n```shell\\n$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012860-zk72q     0/1     Completed   0          7m24s\\nelasticsearch-master-0                      1/1     Running     0          45m\\nelasticsearch-master-1                      1/1     Running     0          45m\\nelasticsearch-master-2                      1/1     Running     0          45m\\nleader-balancer-28012860-7cwmd              0/1     Completed   0          7m25s\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          45m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          45m\\nzell-chaos-zeebe-0                          1/1     Running     0          45m\\nzell-chaos-zeebe-1                          1/1     Running     0          45m\\nzell-chaos-zeebe-2                          1/1     Running     0          45m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-vs28f   1/1     Running     0          45m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          45m\\n\\n```\\n\\nVia our Grafana dashboard (and the gRPC metrics) we are able to track to which gateway the worker connects to:\\n\\n![grpc](grpc.png)\\n\\nIt is `zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f`.\\nVia zbchaos we can easily terminate the gateway (it will always take the first in the pod list).\\n\\n```shell\\n$ zbchaos terminate gateway \\n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1680772377704 false false true false false 30 false -1 benchmark 30  }\\nTerminated zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f\\n```\\n\\nAfter terminating we can see that a new gateway pod has started.\\n```sh\\n$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012860-zk72q     0/1     Completed   0          13m\\nelasticsearch-master-0                      1/1     Running     0          52m\\nelasticsearch-master-1                      1/1     Running     0          52m\\nelasticsearch-master-2                      1/1     Running     0          52m\\nleader-balancer-28012860-7cwmd              0/1     Completed   0          13m\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          52m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          52m\\nzell-chaos-zeebe-0                          1/1     Running     0          52m\\nzell-chaos-zeebe-1                          1/1     Running     0          52m\\nzell-chaos-zeebe-2                          1/1     Running     0          52m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          33s\\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          52m\\n```\\n\\nIn the metrics, we can see that due to the restart, the throughput slightly dropped, but recovered pretty fast. The worker\\nwas able to connect to the different gateway. :white_check_mark:\\n\\n![restart](restart.png)\\n\\n> **Note**\\n> \\n> _In the panel `Pod Restarts` on the top right, we don\'t see any restarts and that is something\\nwe should always be aware of that the _metrics are just samples of data_. If a pod, like a gateway, restarts fast enough\\nand the metric collect interval is higher (per default we have ~30 s (?)) then you might not see a change._\\n\\n\\n\\n#### Scale down\\n\\nAs described earlier we wanted to verify whether it makes a difference if we scale down the replica instead of terminating/restarting\\nit, which causes restarting a new pod (which might get the same IP).\\n\\nFor scaling down Nicolas found this annotation: `controller.kubernetes.io/pod-deletion-cost`\\n\\n[That annotation allows giving hints to the schedule which pod to turn-down, because another pod might have a higher cost \\nto be deleted (this is of course best-effort).](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#pod-deletion-cost)\\n\\nThis means we edit one pod and gave the following annotation:\\n\\n```yaml\\nannotations:\\n  controller.kubernetes.io/pod-deletion-cost:  \\"-1\\"\\n```\\n\\nWe have similarly chosen the pod as we have seen above, based on the gRPC metrics.\\n\\nChecking the running pods and editing the correct gateway:\\n```shell\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          6m26s\\nelasticsearch-master-0                      1/1     Running     0          59m\\nelasticsearch-master-1                      1/1     Running     0          59m\\nelasticsearch-master-2                      1/1     Running     0          59m\\nleader-balancer-28012875-sctwq              0/1     Completed   0          6m26s\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          59m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          59m\\nzell-chaos-zeebe-0                          1/1     Running     0          59m\\nzell-chaos-zeebe-1                          1/1     Running     0          59m\\nzell-chaos-zeebe-2                          1/1     Running     0          59m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          8m29s\\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          59m\\n```\\n\\n\\nWhen I did the following I was wondering why it didn\'t scale down the deployment, one pod was recreated.\\n```shell\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k scale replicaset zell-chaos-zeebe-gateway-7bbdf9fd58 --replicas=1\\nWarning: spec.template.spec.containers[0].env[16].name: duplicate name \\"ZEEBE_LOG_LEVEL\\"\\nreplicaset.apps/zell-chaos-zeebe-gateway-7bbdf9fd58 scaled\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS            RESTARTS   AGE\\ncamunda-platform-curator-28012875-tntdv     0/1     Completed         0          6m53s\\nelasticsearch-master-0                      1/1     Running           0          60m\\nelasticsearch-master-1                      1/1     Running           0          60m\\nelasticsearch-master-2                      1/1     Running           0          60m\\nleader-balancer-28012875-sctwq              0/1     Completed         0          6m53s\\nstarter-cb69c447f-l2zbh                     1/1     Running           0          60m\\nworker-cb7f7c469-qvqqv                      1/1     Running           0          60m\\nzell-chaos-zeebe-0                          1/1     Running           0          60m\\nzell-chaos-zeebe-1                          1/1     Running           0          60m\\nzell-chaos-zeebe-2                          1/1     Running           0          60m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   0/1     PodInitializing   0          7s\\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running           0          8m56s\\n```\\n\\n\\n> **Note:**\\n> \\n> During the experiment I learned that when you have deployed a [deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), you need to scale down the deployment, not the [ReplicaSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/).\\n> Otherwise your Kubernetes deployment controller will recreate the ReplicaSet in the next reconcile loop, which means you\\n> will have again the same replicas as defined in the deployment.\\n\\n\\n\\nSo correct is to scale down the deployment (!), if you ever wonder.\\n\\n\\n```shell\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k edit pod zell-chaos-zeebe-gateway-7bbdf9fd58-pkj48\\npod/zell-chaos-zeebe-gateway-7bbdf9fd58-pkj48 edited\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          10m\\nelasticsearch-master-0                      1/1     Running     0          63m\\nelasticsearch-master-1                      1/1     Running     0          63m\\nelasticsearch-master-2                      1/1     Running     0          63m\\nleader-balancer-28012875-sctwq              0/1     Completed   0          10m\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          63m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          63m\\nzell-chaos-zeebe-0                          1/1     Running     0          63m\\nzell-chaos-zeebe-1                          1/1     Running     0          63m\\nzell-chaos-zeebe-2                          1/1     Running     0          63m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   1/1     Running     0          3m40s\\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          12m\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k scale deployment zell-chaos-zeebe-gateway --replicas=1\\nWarning: spec.template.spec.containers[0].env[16].name: duplicate name \\"ZEEBE_LOG_LEVEL\\"\\ndeployment.apps/zell-chaos-zeebe-gateway scaled\\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          10m\\nelasticsearch-master-0                      1/1     Running     0          64m\\nelasticsearch-master-1                      1/1     Running     0          64m\\nelasticsearch-master-2                      1/1     Running     0          64m\\nleader-balancer-28012875-sctwq              0/1     Completed   0          10m\\nstarter-cb69c447f-l2zbh                     1/1     Running     0          64m\\nworker-cb7f7c469-qvqqv                      1/1     Running     0          64m\\nzell-chaos-zeebe-0                          1/1     Running     0          64m\\nzell-chaos-zeebe-1                          1/1     Running     0          64m\\nzell-chaos-zeebe-2                          1/1     Running     0          64m\\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   1/1     Running     0          4m\\n```\\n\\nWith that, we have only one gateway pod left, and all traffic goes to that gateway. Based on the metrics\\nwe can see that the workers recovered everytime when we restarted/terminated or scaled down.\\n\\n![activate](activate.png)\\n\\n![general](general.png)\\n\\nThe experiment itself succeeded :muscle: :white_check_marks:\\n\\n\\n## Found Bugs\\n \\n\\n### Zbchaos print verbose logs\\n\\nI realized that we still have [the issue with zbchaos](https://github.com/camunda/zeebe-chaos/issues/323) which is printing verbose logs:\\n\\n```shell\\n$ zbchaos terminate gateway \\n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1680772377704 false false true false false 30 false -1 benchmark 30  }\\nTerminated zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f\\n```\\n\\n### Segment creation impact\\n\\nDuring checking the metrics together with Nicolas, we realized that even on low load (10 PI/s) we have high spikes\\nin our processing execution latency.\\n\\n![latency](latency.png)\\n\\nThe spikes are going up to 1-1.5 seconds, while the avg is at 0.06s. This happens every 6 minutes.\\n\\nWe can see that the commit latency is as well at the same time high, which might be an issue because of the high IO.\\n\\n![commit](commit.png)\\n\\nWe first expected that to be related to snapshotting, but snapshots happen much more often.\\n\\n![snapshot](snapshot-count.png)\\n\\nInterestingly is that it seems to be related to our segment creation (again), even if we have \\nasync segment creation in our journal built recently. We need to investigate this further within [zeebe#12311](https://github.com/camunda/camunda/issues/12311).\\n\\n![segment](segment.png)"},{"id":"/2023/02/23/Recursive-call-activity","metadata":{"permalink":"/zeebe-chaos/2023/02/23/Recursive-call-activity","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2023-02-23-Recursive-call-activity/index.md","source":"@site/blog/2023-02-23-Recursive-call-activity/index.md","title":"Recursive call activity","description":"Long time no see. Happy to do my first chaos day this year. In the last week have implemented interesting features, which I would like to experiment with.","date":"2023-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":4.03,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Recursive call activity","date":"2023-02-23T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Gateway Termination","permalink":"/zeebe-chaos/2023/04/06/gateway-termination"},"nextItem":{"title":"Message Correlation after Network Partition","permalink":"/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition"}},"content":"Long time no see. Happy to do my first chaos day this year. In the last week have implemented interesting features, which I would like to experiment with.\\n[Batch processing](https://github.com/camunda/camunda/issues/11416) was one of them.\\n\\n**TL;DR;** Chaos experiment failed. :boom: Batch processing doesn\'t seem to respect the configured limit, which causes issues with processing and influences the health of the system. We found a bug :muscle:\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nIn today\'s chaos experiment, we want to experiment with [Batch processing](https://github.com/camunda/camunda/issues/11416) and how it can handle error conditions, like deploying an endless recursive process model.\\n\\n![recursive process](call.png)\\n\\n### Expected\\n\\nWhen we deploy such a process model and create an instance of it, we expect that the execution is done endlessly. In normal process models with batch processing, the execution of a process instance is done until a wait state is reached. In this process model, there exists no wait state. To handle such cases, we have implemented a batch limit, which can be configured via [maxCommandsInBatch](https://github.com/camunda/camunda/blob/main/dist/src/main/config/broker.standalone.yaml.template#L695). This configuration is by default set to 100 commands. Meaning the stream processor will process 100 commands until it stops, to make room for other things.\\n\\nWe expect that our limit handling steps in during the execution and we can execute also other instances or, cancel the problematic process instance. Furthermore, we expect to stay healthy, we should be able to update our health check continuously.\\n\\n### Actual\\n\\nBefore we can start with our experiment we need to start our benchmark Zeebe cluster. This has become easier now since I have written the last post. Previously we had to use the scripts and Makefile in the [zeebe/benchmark sub-directory](https://github.com/camunda/camunda/tree/main/benchmarks/setup).\\n\\nWe have now provided new [Benchmark Helm charts](https://github.com/camunda/zeebe-benchmark-helm), based on our Camunda Platform Helm charts. They allow us to deploy a new zeebe benchmark setup via:\\n\\n```shell\\nkubectl create namespace zell-chaos # create a new namespace\\nkubens zell-chaos  # change context to a new namespace\\n# deploy zeebe benchmark cluster - without starter and worker\\nhelm install zell-chaos \\\\\\n    zeebe-benchmark/zeebe-benchmark \\\\\\n    --set starter.replicas=0 \\\\\\n    --set worker.replicas=0\\n```\\n\\n\\nTo deploy the model we can use [zbchaos v1.0.0](https://github.com/camunda/zeebe-chaos/releases/tag/zbchaos-v1.0.0).\\n\\n\\n```shell\\n$ zbchaos deploy process --processModelPath call.bpmn \\n{1 LEADER -1 call.bpmn 10  msg false 1 LEADER -1 2 LEADER -1 1677157340943 false false true false false 30 false -1 benchmark 30  }\\nDeployed given process model call.bpmn, under key 2251799813685249!\\n```\\n\\n*Note: Looks like we have some left-over debug logs, which we should remove.*\\n\\nTo create an instance we can use:\\n\\n```shell\\n$ zbchaos verify instance-creation --bpmnProcessId super\\n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1677157569058 false false true false false 30 false -1 super 30  }\\nThe steady state was successfully verified!\\n```\\n\\nAfter creating the instance we can observe the behavior of the Zeebe via [grafana](https://grafana.dev.zeebe.io/).\\n\\nWe can see that the processing starts immediately quite high and is continuously going on. \\n\\n![general](general.png)\\n\\n**We have two instances running, one on partition three and one on partition one.**\\n\\n_One interesting fact is that the topology request rate is also up to 0.400 per second, so potentially every 2.5 seconds we send a topology request to the gateway. But there is no application deployed that does this. [I have recently found out again](https://github.com/camunda/camunda/pull/11599#discussion_r1109846523), that we have the Zeebe client usage in the gateway to request the topology. Might be worth investigating whether this is an issue._\\n\\nAfter observing this cluster for a while we can see that after around five minutes the cluster fails. The processing for the partitions breaks down to 1/10 of what was processed before. A bit later it looks like it tries to come back but, failed again.\\n\\n![fail-general](fail-general.png)\\n\\n_We can see in the metrics that in between also the balancing was triggered. A feature we have as part of our Benchmark Helm charts._\\n\\nThe logs (at stack driver) doesn\'t give us many insights, [except that we see that nodes becoming unhealthy](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22zell-chaos%22%0Alabels.k8s-pod%2Fapp%3D%22camunda-platform%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fcomponent%3D%22zeebe-broker%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Finstance%3D%22zell-chaos%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fmanaged-by%3D%22Helm%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fname%3D%22zeebe%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fpart-of%3D%22camunda-platform%22;timeRange=2023-02-23T12:17:49.128812Z%2F2023-02-23T14:18:59.101Z;pinnedLogId=2023-02-23T13:13:40.945376476Z%2Fdr4gxdklsxtgx6h6;cursorTimestamp=2023-02-23T13:13:40.945376476Z?project=zeebe-io). Similar information we can see in the metrics, that followers are unhealthy.\\n\\n```shell\\nPartition-1 failed, marking it as unhealthy: Broker-2{status=HEALTHY}\\nDetected \'UNHEALTHY\' components. The current health status of components: [Partition-2{status=HEALTHY}, Partition-1{status=UNHEALTHY, issue=HealthIssue[message=null, throwable=null, cause=Broker-2-StreamProcessor-1{status=UNHEALTHY, issue=HealthIssue[message=actor appears blocked, throwable=null, cause=null]}]}, Partition-3{status=HEALTHY}]\\n```\\n\\nInteresting insights we can get in our new Batch processing metrics. We see that at the beginning we use our limit of 100 commands per batch, but soon as we start with the recursion we use an enormous high batch processing command count.\\n\\n![fail-batchprocessing.png](fail-batchprocessing.png)\\n\\nThe new sequence metric shows similar results, so there must be a problem with not respecting the limit.\\n\\n![sequencer](sequencer.png)\\n\\nWith this, I mark this chaos experiment as failed. We need to investigate this further and fix the related issue.:boom:\\n\\n## Found Bugs\\n\\n* [zbchaos logs debug message on normal usage](https://github.com/camunda/zeebe-chaos/issues/323)\\n* [Every 2.5 seconds we send a topology request, which is shown in the metrics](https://github.com/camunda/camunda/issues/11799)\\n* [Batch processing doesn\'t respect the limit](https://github.com/camunda/camunda/issues/11798)"},{"id":"/2022/08/31/Message-Correlation-after-Network-Partition","metadata":{"permalink":"/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2022-08-31-Message-Correlation-after-Network-Partition/index.md","source":"@site/blog/2022-08-31-Message-Correlation-after-Network-Partition/index.md","title":"Message Correlation after Network Partition","description":"In the last weeks, we made several changes in our core components, we introduce some new abstractions, and changed how we communicate between partitions.","date":"2022-08-31T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":9.97,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Message Correlation after Network Partition","date":"2022-08-31T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Recursive call activity","permalink":"/zeebe-chaos/2023/02/23/Recursive-call-activity"},"nextItem":{"title":"Bring Deployment distribution experiment back","permalink":"/zeebe-chaos/2022/08/02/deployment-distribution"}},"content":"In the last weeks, we made several changes in our core components, we introduce some new abstractions, and changed how we communicate between partitions.\\n\\nDue to these changes, we thought it might make sense to run some more chaos experiments in that direction and area since our benchmarks also recently found some interesting edge cases.\\n\\nToday we experimented with Message Correlation and what happens when a network partition disturbs the correlation process.\\n\\n**TL;DR;** The experiment was partially successful (after retry), we were able to publish messages during a network partition that have been correlated after the network partition. We need to verify whether we can also publish messages before a network partition and during the partition create the related instances.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nThe experiment is related to our previously described [deployment distribution experiment](../2022-08-02-deployment-distribution/index.md).\\n\\nWhen a user/client publishes a message, the message will be sent to a certain partition, based on the correlation key. There is some calculation going on related to the hashcode and the partition count.\\nThis calculation is deterministic in order to find later the message again if we reach a message catch event.\\n\\nA message can specify a time-to-live (TTL), [which allows buffering that message](https://docs.camunda.io/docs/components/concepts/messages/#message-buffering). If later a process instance is created and the TTL is not exceeded the message can be still correlated. The creation of process instances\\nhappens round-robin on the existing/available partitions (this is controlled by the gateway). When a process instance is created and reaches a message catch event it will be based on the correlation key search for a message on the expected partition. _Actually this happens based on subscriptions, for more details see the [docs](https://docs.camunda.io/docs/components/concepts/messages/#message-subscriptions) or the [ZEP-4](https://github.com/zeebe-io/enhancements/blob/master/ZEP004-wf-stream-processing.md#message-intermediate-catch-event)._ If the message still exists (TTL didn\'t expire) and this message [wasn\'t already correlated to the same process definition](https://docs.camunda.io/docs/components/concepts/messages/#message-cardinality) then it will be correlated.\\n\\nSince both partitions can be on different leader nodes this requires some network communication, which can be interrupted/disturbed.\\n\\n### Expected\\n\\nWe expect that if the network between the partition where the message was published and where the process instance was created is interrupted that no message correlation happens. But after the network recovers, we expect further that the message will be correlated and the process instance can continue.\\n\\n### Actual\\n\\nAs a setup, I installed our benchmarks, with Operate enabled.\\nThis allows us to also view the details in Operate.\\n\\n```shell\\n$ diff zeebe-values.yaml ../default/zeebe-values.yaml \\n5,8d4\\n<   identity:\\n<     auth:\\n<       enabled: false\\n< \\n28c24\\n<   containerSecurityContext:\\n---\\n>   podSecurityContext:\\n139c135\\n<   enabled: true\\n---\\n>   enabled: false\\n```\\n\\nDuring the experiment, it turned out that the `podSecurityContext` is outdated.\\n\\n#### Experiment Description\\n\\nSince we want to automate this experiment soon, or later I thought it would be a good idea to use the [create process instance with result](https://docs.camunda.io/docs/components/concepts/process-instance-creation/#create-and-await-results). We would start the following process:\\n\\n![msg-catch](msg-catch.png)\\n\\nBefore we start the process we need to publish the message to a certain partition and create a network partition between two partitions. After that, we can create the PI and verify that the message correlation shouldn\'t happen. Afterward, we would delete the network partition and verify that the process instance is completed.\\n\\n#### Details\\n\\nTo make the experiment easier to reproduce and allow us to experiment in different directions later as well I extend our new chaos cli (zbchaos), which I created during the last hack days. I will write a separate blog post about this tool soon.\\n\\n##### Message Publish\\n\\nI added a new feature ([PR #166](https://github.com/camunda/zeebe-chaos/pull/166)) that allows us to publish a message to a specific partition:\\n\\n```sh\\n$ ./zbchaos publish message -v --partitionId 3\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nSend message \'msg\', with correaltion key \'2\' (ASCII: 50) \\nMessage was sent and returned key 6755399441055796, which corresponds to partition: 3\\n```\\n\\n##### Extend Steady-state verification\\n\\nFor the steady-state verification, multiple enhancements have been added.\\n\\n1. Previously the `zbchaos` didn\'t allow us to create instances of specific models, which is now added as new feature ([PR #167](https://github.com/camunda/zeebe-chaos/pull/167)).\\n2. In order to await the process instance completion a new flag was added `--awaitResult`, which allows us to await the PI completeness.\\n3. To make sure that our message can be correlated we have to set the right correlationKey/value. This means we need to create instances with certain variables, which is now possible as well (`--variables`).\\n\\n```shell\\n./zbchaos verify steady-state -h\\nVerifies the steady state of the Zeebe system.\\nA process model will be deployed and process instances are created until the required partition is reached.\\n\\nUsage:\\n  zbchaos verify steady-state [flags]\\n\\nFlags:\\n      --awaitResult               Specify whether the completion of the created process instance should be awaited.\\n  -h, --help                      help for steady-state\\n      --partitionId int           Specify the id of the partition (default 1)\\n      --processModelPath string   Specify the path to a BPMN process model, which should be deployed and an instance should be created of.\\n      --variables string          Specify the variables for the process instance. Expect json string.\\n\\nGlobal Flags:\\n  -v, --verbose   verbose output\\n\\n```\\n\\n#### Execution of the Experiment\\n\\n```shell\\n$./zbchaos verify readiness -v\\nConnecting to zell-chaos\\nAll Zeebe nodes are running.\\n```\\n\\nAfter checking the readiness we can check what the current topology is:\\n```shell\\n$ ./zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3\\n0         |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n1         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n```\\nWe can see that the leaders are well distributed. I pick partition 3 as our message publish partition, and partition 1 as our partition for the process instance. Since we can\'t control really the round-robin mechanism, we need to create multiple messages and multiple process instances (for each partition). During our experiment, we will only look at the instance on partition one.\\n\\n```shell\\n$ ./zbchaos publish message -v --partitionId 3\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nSend message \'msg\', with correaltion key \'2\' (ASCII: 50) \\nMessage was sent and returned key 6755399441055745, which corresponds to partition: 3\\n[zell go-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ ./zbchaos publish message -v --partitionId 3\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nSend message \'msg\', with correaltion key \'2\' (ASCII: 50) \\nMessage was sent and returned key 6755399441055746, which corresponds to partition: 3\\n[zell go-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ ./zbchaos publish message -v --partitionId 3\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nSend message \'msg\', with correaltion key \'2\' (ASCII: 50) \\nMessage was sent and returned key 6755399441055747, which corresponds to partition: 3\\n```\\n\\nCreating the network partition:\\n\\n```shell\\n./zbchaos disconnect brokers --broker1PartitionId 3 --broker1Role LEADER --broker2PartitionId 1 --broker2Role LEADER -v\\n...\\nSuccessfully created port forwarding tunnel\\nFound Broker zell-chaos-zeebe-2 as LEADER for partition 3.\\nFound Broker zell-chaos-zeebe-0 as LEADER for partition 1.\\nExecute [\\"apt\\" \\"-qq\\" \\"update\\"] on pod zell-chaos-zeebe-2\\n...\\nDisconnect zell-chaos-zeebe-2 from zell-chaos-zeebe-0\\n...\\nDisconnect zell-chaos-zeebe-0 from zell-chaos-zeebe-2\\n```\\n\\nCreating the process instance and await the result:\\n\\n```shell\\n$ ./zbchaos verify steady-state --awaitResult --partitionId 1 --processModelPath ../msg-catch.bpmn --variables \'{\\"key\\":\\"2\\"}\'\\n\\n\\n```\\n\\nUnfortunately, I missed the verbose flag so we can\'t really see the output. But if failed later with:\\n\\n```shell\\nEncountered an error during process instance creation. Error: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request ProtocolRequest{id=422, subject=command-api-1, sender=10.0.20.4:26502, payload=byte[]{length=153, hash=1559826040}} to zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc:26501 timed out in PT15S\\nEncountered an error during process instance creation. Error: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request ProtocolRequest{id=441, subject=command-api-2, sender=10.0.20.4:26502, payload=byte[]{length=153, hash=-1786651527}} to zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc:26501 timed out in PT15S\\nEncountered an error during process instance creation. Error: rpc error: code = NotFound desc = Command \'CREATE_WITH_AWAITING_RESULT\' rejected with code \'NOT_FOUND\': Expected to find process definition with key \'2251799813685249\', but none found\\npanic: Expected to create process instance on partition 1, but timed out after 30s.\\n\\ngoroutine 1 [running]:\\ngithub.com/camunda/zeebe-chaos/go-chaos/cmd.glob..func10(0x247f740?, {0x1758c60?, 0x7?, 0x7?})\\n\\t/home/zell/goPath/src/github.com/camunda/zeebe-chaos/go-chaos/cmd/verify.go:97 +0x1c5\\ngithub.com/spf13/cobra.(*Command).execute(0x247f740, {0xc000426540, 0x7, 0x7})\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:876 +0x67b\\ngithub.com/spf13/cobra.(*Command).ExecuteC(0x24808c0)\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:990 +0x3bd\\ngithub.com/spf13/cobra.(*Command).Execute(...)\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:918\\ngithub.com/camunda/zeebe-chaos/go-chaos/cmd.Execute()\\n\\t/home/zell/goPath/src/github.com/camunda/zeebe-chaos/go-chaos/cmd/root.go:61 +0x25\\nmain.main()\\n\\t/home/zell/goPath/src/github.com/camunda/zeebe-chaos/go-chaos/main.go:8 +0x17\\n```\\nI retried it:\\n```shell\\n$ ./zbchaos verify steady-state --awaitResult --partitionId 1 --processModelPath ../msg-catch.bpmn --variables \'{\\"key\\":\\"2\\"}\' -v\\nConnecting to zell-chaos\\nSuccessfully created port forwarding tunnel\\nDeploy file ../msg-catch.bpmn (size: 2980 bytes).\\nDeployed process model ../msg-catch.bpmn successful with key 2251799813685249.\\nCreate process instance with defition key 2251799813685249 [variables: \'{\\"key\\":\\"2\\"}\', awaitResult: true]\\nEncountered an error during process instance creation. Error: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request ProtocolRequest{id=509, subject=command-api-1, sender=10.0.20.4:26502, payload=byte[]{length=153, hash=1559826040}} to zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc:26501 timed out in PT15S\\nCreate process instance with defition key 2251799813685249 [variables: \'{\\"key\\":\\"2\\"}\', awaitResult: true]\\nEncountered an error during process instance creation. Error: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request ProtocolRequest{id=530, subject=command-api-2, sender=10.0.20.4:26502, payload=byte[]{length=153, hash=-1786651527}} to zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc:26501 timed out in PT14.999S\\nCreate process instance with defition key 2251799813685249 [variables: \'{\\"key\\":\\"2\\"}\', awaitResult: true]\\nEncountered an error during process instance creation. Error: rpc error: code = NotFound desc = Command \'CREATE_WITH_AWAITING_RESULT\' rejected with code \'NOT_FOUND\': Expected to find process definition with key \'2251799813685249\', but none found\\npanic: Expected to create process instance on partition 1, but timed out after 30s.\\n\\ngoroutine 1 [running]:\\ngithub.com/camunda/zeebe-chaos/go-chaos/cmd.glob..func10(0x247f740?, {0x1758c60?, 0x8?, 0x8?})\\n\\t/home/zell/goPath/src/github.com/camunda/zeebe-chaos/go-chaos/cmd/verify.go:97 +0x1c5\\ngithub.com/spf13/cobra.(*Command).execute(0x247f740, {0xc00007e500, 0x8, 0x8})\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:876 +0x67b\\ngithub.com/spf13/cobra.(*Command).ExecuteC(0x24808c0)\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:990 +0x3bd\\ngithub.com/spf13/cobra.(*Command).Execute(...)\\n\\t/home/zell/goPath/pkg/mod/github.com/spf13/cobra@v1.5.0/command.go:918\\ngithub.com/camunda/zeebe-chaos/go-chaos/cmd.Execute()\\n\\t/home/zell/goPath/src/github.com/camunda/zeebe-chaos/go-chaos/cmd/root.go:61 +0x25\\nmain.main()\\n\\t/home/zell/goPath/src/github.com/camunda/zeebe-chaos/go-chaos/main.go:8 +0x17\\n```\\n\\nAnd got a similar exception. Taking a look at Operate we can see that process instances are created. It is likely that the await timed out since the message hasn\'t been correlated but the returned error is a bit unclear. Interesting is that on partition two the message is also not correlated.\\n\\n![operate](operate.png)\\n\\nRemoving the network partition:\\n\\n```shell\\n$ ./zbchaos connect brokers -v\\nConnecting to zell-chaos\\nExecute [\\"sh\\" \\"-c\\" \\"command -v ip\\"] on pod zell-chaos-zeebe-0\\n/usr/sbin/ip\\nExecute [\\"sh\\" \\"-c\\" \\"ip route | grep -m 1 unreachable\\"] on pod zell-chaos-zeebe-0\\nExecute [\\"sh\\" \\"-c\\" \\"ip route del unreachable 10.0.17.8\\"] on pod zell-chaos-zeebe-0\\nConnected zell-chaos-zeebe-0 again, removed unreachable routes.\\nExecute [\\"sh\\" \\"-c\\" \\"command -v ip\\"] on pod zell-chaos-zeebe-1\\nError on connection Broker: zell-chaos-zeebe-1. Error: Execution exited with exit code 127 (Command not found). It is likely that the broker was not disconnected or restarted in between.\\nExecute [\\"sh\\" \\"-c\\" \\"command -v ip\\"] on pod zell-chaos-zeebe-2\\nError on connection Broker: zell-chaos-zeebe-2. Error: Execution exited with exit code 127 (Command not found). It is likely that the broker was not disconnected or restarted in between.\\n``` \\n\\nIt looks like the Broker-2 was restarted in between, which is really the case if we check the `kubectl get pods`\\n```\\n[zell go-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\\nNAME                                        READY   STATUS      RESTARTS   AGE\\ncamunda-platform-curator-27698835-pwdjh     0/1     Completed   0          9m20s\\nelasticsearch-master-0                      1/1     Running     0          12m\\nelasticsearch-master-1                      1/1     Running     0          12m\\nelasticsearch-master-2                      1/1     Running     0          12m\\nzell-chaos-operate-64bbc6794d-vqtnc         1/1     Running     0          12m\\nzell-chaos-zeebe-0                          1/1     Running     0          12m\\nzell-chaos-zeebe-1                          1/1     Running     0          12m\\nzell-chaos-zeebe-2                          1/1     Running     0          2m48s\\nzell-chaos-zeebe-gateway-795f87fd64-c9mf4   1/1     Running     0          12m\\nzell-chaos-zeebe-gateway-795f87fd64-h5d9c   1/1     Running     0          12m\\nzell-chaos-zeebe-gateway-795f87fd64-nlr5v   1/1     Running     0          12m\\n```\\n\\nThe topology has also completely changed.\\n\\n```shell\\n$ ./zbchaos topology\\nNode      |Partition 1         |Partition 2         |Partition 3\\n0         |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)\\n1         |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)  |FOLLOWER (HEALTHY)\\n2         |FOLLOWER (HEALTHY)  |LEADER (HEALTHY)    |FOLLOWER (HEALTHY)\\n```\\n\\nAfter connecting again the instances haven\'t been executed. My guess is that the TTL was already reached.\\n\\n##### Rerun\\n\\nI will disconnect again partitions one and three and publish a message to partition three. Afterward I will connect them again and see whether the message is correlated.\\n\\n```shell\\n./zbchaos disconnect brokers --broker1PartitionId 3 --broker1Role LEADER --broker2PartitionId 1 --broker2Role LEADER -v\\n./zbchaos publish message -v --partitionId 3\\n./zbchaos publish message -v --partitionId 3\\n./zbchaos publish message -v --partitionId 3\\n./zbchaos connect brokers -v\\n```\\n\\nTake a look at Operate again:\\n\\n![operate2](operate2.png)\\n\\nWe can see that the experiment was successful, and the message has been correlated even if they are published during a network partition. :tada:\\n\\n## Found Bugs\\n\\n* I learned that the go zeebe client, doesn\'t set a default TTL which was interesting to find out (and somehow unexpected).\\n* The zbchaos uses always the same port for connecting to the kubernetes and zeebe cluster, which makes it impossible to run multiple commands. We should use random ports to make this possible."},{"id":"/2022/08/02/deployment-distribution","metadata":{"permalink":"/zeebe-chaos/2022/08/02/deployment-distribution","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2022-08-02-deployment-distribution/index.md","source":"@site/blog/2022-08-02-deployment-distribution/index.md","title":"Bring Deployment distribution experiment back","description":"We encountered recently a severe bug zeebe#9877 and I was wondering why we haven\'t spotted it earlier, since we have chaos experiments for it. I realized two things:","date":"2022-08-02T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":9.62,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Bring Deployment distribution experiment back","date":"2022-08-02T00:00:00.000Z","categories":["chaos_experiment","bpmn","deployment"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Message Correlation after Network Partition","permalink":"/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition"},"nextItem":{"title":"Standalone Gateway in CCSaaS","permalink":"/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS"}},"content":"We encountered recently a severe bug [zeebe#9877](https://github.com/camunda/camunda/issues/9877) and I was wondering why we haven\'t spotted it earlier, since we have chaos experiments for it. I realized two things:\\n\\n 1. The experiments only check for parts of it (BPMN resource only). The production code has changed, and a new feature has been added (DMN) but the experiments/tests haven\'t been adjusted.\\n 2. More importantly we disabled the automated execution of the deployment distribution experiment because it was flaky due to a missing standalone gateway in Camunda Cloud SaaS [camunda/zeebe-chaos#61](https://github.com/camunda/zeebe-chaos/issues/61). This is no longer the case, see [Standalone Gateway in CCSaaS](../2022-02-15-Standalone-Gateway-in-CCSaaS/index.md)\\n\\nOn this chaos day I want to bring the automation of this chaos experiment back to life. If I have still time I want to enhance the experiment. \\n\\n**TL;DR;** The experiment still worked, and our deployment distribution is still resilient against network partitions. It also works with DMN resources. I can enable the experiment again, and we can close [camunda/zeebe-chaos#61](https://github.com/camunda/zeebe-chaos/issues/61). Unfortunately, we were not able to reproduce [zeebe#9877](https://github.com/camunda/camunda/issues/9877) but we did some good preparation work for it.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nTo recap, when a deployment is created by a client it is sent to the partition one leader. Partition one is in charge of distributing the deployment to the other partitions. That means it will send the deployment to the other partition leaders, this is retried as long no ACK was received from the corresponding partition leader.\\n\\n![deploymentDistribution](deploymentDistribution.png)\\n\\nWe already have covered that in more detail in another chaos day you can read [here](../2021-01-26-deployments/index.md).\\n\\n### Expected\\n\\n![](deploymentDistributionExperiment.png)\\n\\nAs you can see in the image we will create an asymmetric network partition and disconnect the partition one leader from partition three. That means the sending to partition three will not be possible. We use here an asymmetric network partition, in order to reduce the probability to cause a leader change. The partition one leader is a follower of partition three and will still receive heartbeats.\\n\\nAfter disconnecting the leaders we deploy multiple process versions and after connecting the leaders again we expect that the deployments are distributed. It is expected that we can create instances of the last version on all partitions.\\n\\nWe will run the existing experiment against the latest minor version, to verify whether the experiment still works. When we enable the experiment again for automation it will be executed against SNAPSHOT automatically.\\n\\n### Actual\\n\\n#### Setup\\n\\nAs a first step, we created a new Production-S cluster, which has three partitions, three nodes (brokers), and two standalone gateways. The Zeebe version was set to 8.0.4 (latest minor).\\n\\nIt was a while since I used the [chaostoolkit](https://chaostoolkit.org/) which is the reason I had to reinstall it again, which is quite a simple see [here](https://chaostoolkit.org/reference/usage/install/).\\n\\nTL;DR:\\n```sh\\npython3 -m venv ~/.venvs/chaostk\\nsource  ~/.venvs/chaostk/bin/activate\\npip install -U chaostoolkit\\nchaos --version\\n```\\n\\n#### Executing chaos toolkit\\n\\nAs mentioned, the deployment distribution was not enabled for Production-S clusters, which is currently the only configuration we test via [Zeebe Testbench](https://github.com/zeebe-io/zeebe-cluster-testbench). We have to use the experiment that is defined under [production-l/deployment-distribution](https://github.com/camunda/zeebe-chaos/tree/master/chaos-workers/chaos-experiments/camunda-cloud/production-l/deployment-distribution), which is the same*.\\n\\n<sub>* That is not 100% true. During running the Production-l experiment I realized that it made some assumptions regarding the <a href=\\"https://github.com/camunda/zeebe-chaos/blob/master/chaos-workers/chaos-experiments/scripts/disconnect-leaders-one-way.sh#L19\\">partition count</a> which needs to be adjusted for the Production-S setup.</sub>\\n\\n```sh\\n chaos run production-l/deployment-distribution/experiment.json \\n[2022-08-02 09:35:25 INFO] Validating the experiment\'s syntax\\n[2022-08-02 09:35:25 INFO] Experiment looks valid\\n[2022-08-02 09:35:25 INFO] Running experiment: Zeebe deployment distribution\\n[2022-08-02 09:35:25 INFO] Steady-state strategy: default\\n[2022-08-02 09:35:25 INFO] Rollbacks strategy: default\\n[2022-08-02 09:35:25 INFO] Steady state hypothesis: Zeebe is alive\\n[2022-08-02 09:35:25 INFO] Probe: All pods should be ready\\n[2022-08-02 09:35:25 INFO] Steady state hypothesis is met!\\n[2022-08-02 09:35:25 INFO] Playing your experiment\'s method now...\\n[2022-08-02 09:35:25 INFO] Action: Enable net_admin capabilities\\n[2022-08-02 09:35:26 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2022-08-02 09:35:26 INFO] Pausing after activity for 180s...\\n[2022-08-02 09:38:26 INFO] Probe: All pods should be ready\\n[2022-08-02 09:38:33 INFO] Action: Create network partition between leaders\\n[2022-08-02 09:38:34 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2022-08-02 09:38:34 INFO] Action: Deploy different deployment versions.\\n[2022-08-02 09:38:40 INFO] Action: Delete network partition\\n[2022-08-02 09:38:42 INFO] Probe: Create process instance of latest version on partition one\\n[2022-08-02 09:38:43 INFO] Probe: Create process instance of latest version on partition two\\n[2022-08-02 09:38:44 INFO] Probe: Create process instance of latest version on partition three\\n[2022-08-02 09:38:44 INFO] Steady state hypothesis: Zeebe is alive\\n[2022-08-02 09:38:44 INFO] Probe: All pods should be ready\\n[2022-08-02 09:38:45 INFO] Steady state hypothesis is met!\\n[2022-08-02 09:38:45 INFO] Let\'s rollback...\\n[2022-08-02 09:38:45 INFO] No declared rollbacks, let\'s move on.\\n[2022-08-02 09:38:45 INFO] Experiment ended with status: completed\\n```\\n\\nBased on the tool output it looks like it succeed, to make sure it really worked, we will take a look at the logs in stackdriver.\\n\\nIn the following logs we can see that deployment distribution is failing for partition 3 and is retried, which is expected and what we wanted.\\n```\\n2022-08-02 09:38:53.114 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685347-3\'). Retrying\\n2022-08-02 09:38:53.115 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685347 was written on partition 3\\n2022-08-02 09:38:53.157 CEST zeebe Received new exporter state {elasticsearch=228, MetricsExporter=228} \\n2022-08-02 09:38:53.157 CEST zeebe Received new exporter state {elasticsearch=228, MetricsExporter=228}\\n2022-08-02 09:38:53.241 CEST zeebe Received new exporter state {elasticsearch=232, MetricsExporter=232}\\n2022-08-02 09:38:53.241 CEST zeebe Received new exporter state {elasticsearch=232, MetricsExporter=232}\\n2022-08-02 09:38:53.464 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685351-3\'). Retrying\\n2022-08-02 09:38:53.466 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685351 was written on partition 3\\n2022-08-02 09:38:54.216 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685353-3\'). Retrying\\n2022-08-02 09:38:54.218 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685353 was written on partition 3\\n2022-08-02 09:38:54.224 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685355-3\'). Retrying\\n2022-08-02 09:38:54.225 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685355 was written on partition 3\\n2022-08-02 09:38:55.055 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685359-3\'). Retrying\\n2022-08-02 09:38:55.057 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685359 was written on partition 3\\n2022-08-02 09:38:55.689 CEST zeebe Received new exporter state {elasticsearch=299, MetricsExporter=299}\\n2022-08-02 09:38:55.690 CEST zeebe Received new exporter state {elasticsearch=299, MetricsExporter=299}\\n2022-08-02 09:38:56.089 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685357-3\'). Retrying\\n2022-08-02 09:38:56.090 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685357 was written on partition 3\\n2022-08-02 09:38:56.272 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685363-3\'). Retrying\\n2022-08-02 09:38:56.273 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685363 was written on partition 3\\n2022-08-02 09:38:56.920 CEST zeebe Failed to receive deployment response for partition 3 (on topic \'deployment-response-2251799813685361-3\'). Retrying\\n2022-08-02 09:38:56.922 CEST zeebe Deployment DISTRIBUTE command for deployment 2251799813685361 was written on partition 3\\n2022-08-02 09:39:08.369 CEST zeebe Received new exporter state {elasticsearch=252, MetricsExporter=252}\\n```\\n\\nAt some point, the retry stopped, and we can see in the experiment output that we were able to start process instances on all partitions. This is great because it means the experiment was successfully executed and our deployment distribution is failure tolerant.\\n\\n#### Enhancement\\n\\nAs described earlier the current experiment deploys a BPMN process model only. It looks like this:\\n\\n![v1](multiVersionModel.png)\\n\\nIn order to make DMN part of the experiment, we change the service task to a [Business Rule task](https://docs.camunda.io/docs/components/modeler/bpmn/business-rule-tasks/). \\n\\n![v2](multiVersionModelv2.png)\\n\\nThe decision is really simple and just defines a static input and returns that as output.\\n\\n![decision](decision.png)\\n\\nWhen we run our experiment and create process instances on all partitions the DMN needs to be available otherwise the execution would fail. Currently, we can\'t specify a specific version of the DMN in the Business Rule Task (always the latest will be executed). Because of that, we will not deploy different DMN model versions, since it is currently not that easy to verify whether the right version was chosen. \\n\\nAfter adjusting the model and adjusting the script, we run the experiment again.\\n\\n```\\n$ chaos run production-l/deployment-distribution/experiment.json \\n[2022-08-02 11:05:12 INFO] Validating the experiment\'s syntax\\n[2022-08-02 11:05:12 INFO] Experiment looks valid\\n[2022-08-02 11:05:12 INFO] Running experiment: Zeebe deployment distribution\\n[2022-08-02 11:05:12 INFO] Steady-state strategy: default\\n[2022-08-02 11:05:12 INFO] Rollbacks strategy: default\\n[2022-08-02 11:05:12 INFO] Steady state hypothesis: Zeebe is alive\\n[2022-08-02 11:05:12 INFO] Probe: All pods should be ready\\n[2022-08-02 11:05:13 INFO] Steady state hypothesis is met!\\n[2022-08-02 11:05:13 INFO] Playing your experiment\'s method now...\\n[2022-08-02 11:05:13 INFO] Action: Enable net_admin capabilities\\n[2022-08-02 11:05:13 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2022-08-02 11:05:13 INFO] Pausing after activity for 180s...\\n[2022-08-02 11:08:14 INFO] Probe: All pods should be ready\\n[2022-08-02 11:08:14 INFO] Action: Create network partition between leaders\\n[2022-08-02 11:08:16 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2022-08-02 11:08:16 INFO] Action: Deploy different deployment versions.\\n[2022-08-02 11:08:25 INFO] Action: Delete network partition\\n[2022-08-02 11:08:27 INFO] Probe: Create process instance of latest version on partition one\\n[2022-08-02 11:08:27 INFO] Probe: Create process instance of latest version on partition two\\n[2022-08-02 11:08:28 INFO] Probe: Create process instance of latest version on partition three\\n[2022-08-02 11:08:29 INFO] Steady state hypothesis: Zeebe is alive\\n[2022-08-02 11:08:29 INFO] Probe: All pods should be ready\\n[2022-08-02 11:08:29 INFO] Steady state hypothesis is met!\\n[2022-08-02 11:08:29 INFO] Let\'s rollback...\\n[2022-08-02 11:08:29 INFO] No declared rollbacks, let\'s move on.\\n[2022-08-02 11:08:29 INFO] Experiment ended with status: completed\\n```\\n\\nIt succeeded as well.\\n\\nTaking a look at Operate we can see some incidents.\\n\\n![dmn-error](dmn-error.png)\\n\\nIt seems the process instance execution runs into the Business Rule Task, but the DMN resource was not available on the partition. \\n\\n![dmn-retry](dmn-retry.png)\\n\\nAfter retrying in Operate the incident was resolved, which means the DMN resource was distributed at that time.\\n\\nWe can adjust the experiment further to await the result of the process execution, but I will stop here and leave that for a later point.\\n\\n#### Reproduce our bug\\n\\nThe current experiment didn\'t reproduce the bug in [zeebe#9877](https://github.com/camunda/camunda/issues/9877), since the DMN resource has to be distributed multiple times. Currently, we create a network partition such that the distribution doesn\'t work at all. \\n\\n![](deploymentDistributionExperimentV2.png)\\n\\nIn order to reproduce our scenario, we can set the network partition in the other direction, such that the acknowledgment is not received by the leader one.\\n\\nAdjusting the experiment (script) like this:\\n\\n```diff\\n-retryUntilSuccess disconnect \\"$leader\\" \\"$leaderTwoIp\\"\\n+retryUntilSuccess disconnect \\"$leaderTwo\\" \\"$leaderIp\\"\\n```\\n\\nShould do the trick, but I was not yet able to reproduce the issue with 8.0.4. It seems we need to spend some more time reproducing the bug. But I think with today\'s changes we already did a good step in the right direction, and we can improve based on that. I will create a follow-up issue to improve our experiment.\\n\\n\\n## Further Work\\n\\nBased on today\'s outcome we can enable again the Deployment Distribution experiment for Production-S, such that is executed by Zeebe Testbench (our automation tooling). We can close [camunda/zeebe-chaos#61](https://github.com/camunda/zeebe-chaos/issues/61)\\n\\nWe should adjust our Chaos Worker implementation such that we also deploy DMN resources as we did in today\'s Chaos Day, since the scripts we changed aren\'t used in the automation.\\n\\n I will create a follow-up issue to improve our experiment, so we can reproduce the critical bug.\\n\\n## Found Bugs\\n\\n*none*"},{"id":"/2022/02/15/Standalone-Gateway-in-CCSaaS","metadata":{"permalink":"/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2022-02-15-Standalone-Gateway-in-CCSaaS/index.md","source":"@site/blog/2022-02-15-Standalone-Gateway-in-CCSaaS/index.md","title":"Standalone Gateway in CCSaaS","description":"We recently introduced the Zeebe Standalone Gateway in CCSaaS. Today I wanted to do a first simple","date":"2022-02-15T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.935,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Standalone Gateway in CCSaaS","date":"2022-02-15T00:00:00.000Z","categories":["chaos_experiment","gateway"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Bring Deployment distribution experiment back","permalink":"/zeebe-chaos/2022/08/02/deployment-distribution"},"nextItem":{"title":"High Snapshot Frequency","permalink":"/zeebe-chaos/2022/02/01/High-Snapshot-Frequency"}},"content":"We recently introduced the Zeebe Standalone Gateway in CCSaaS. Today I wanted to do a first simple \\nchaos experiment with the gateway, where we just restart one gateway. \\n\\nIdeally in the future we could enable some gateway chaos experiments again, which we currently only support for [helm](https://github.com/camunda/zeebe-chaos/tree/master/chaos-workers/chaos-experiments/helm).\\n\\n**TL;DR;** Our Camunda Cloud clusters can handle gateway restarts without issues. \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nThis experiment is a simple restart / kill pod experiment. We want to verify that our cluster\\ncan still make progress even if a gateway is restarted / killed in between. Currently, we are running two gateway replicas in the CCSaaS. \\n\\nIn order to start with our experiment we created a new CCSaaS cluster with a `Production - S` plan, and the latest version (1.3.4). \\nTo run some load on the cluster we used the cloud benchmark deployments, which you can find [here](https://github.com/camunda-cloud/zeebe/tree/main/benchmarks/setup/cloud-default).\\nThe load was rather low with ~ 100 PI/s.\\n\\n### Expected\\n\\n_Hypothesis: When restarting / killing a zeebe standalone gateway we expect only a small \\nimpact on current requests, new requests should be routed to the right gateway and the cluster can\\nmake progress._\\n\\n\\n### Actual\\n\\nAfter creating the cluster and starting the benchmark we checked the current resource usage, to find a cluster which does most of the work. It looks like that the requests are well distributed.\\n```shell\\n[zell zell-chaos/ cluster: ultrachaos ns:448f5091-8d15-4c01-a0ee-202437c09d83-zeebe]$ k top pod\\nNAME                                                     CPU(cores)   MEMORY(bytes)   \\n...      \\nzeebe-gateway-6c9f95b557-f2zbf                           294m         407Mi           \\nzeebe-gateway-6c9f95b557-gk57z                           202m         396Mi\\n```\\n\\nWe deleted the first pod `zeebe-gateway-6c9f95b557-f2zbf` and observed the metrics.\\n```\\n[zell zell-chaos/ cluster: ultrachaos ns:448f5091-8d15-4c01-a0ee-202437c09d83-zeebe]$ k delete pod zeebe-gateway-6c9f95b557-f2zbf\\npod \\"zeebe-gateway-6c9f95b557-f2zbf\\" deleted\\n```\\n\\nWe can see that a new gateway pod is created quite fast.\\n\\n```\\n[zell zell-chaos/ cluster: ultrachaos ns:448f5091-8d15-4c01-a0ee-202437c09d83-zeebe]$ kgpo\\nNAME                                                     READY   STATUS             RESTARTS   AGE\\n...\\nzeebe-gateway-6c9f95b557-flgz6                           0/1     Running            0          16s\\nzeebe-gateway-6c9f95b557-gk57z                           1/1     Running            0          156m\\n```\\n\\nAs expected we see no high impact due to the restart. \\n\\n![](restart.png)\\n\\nJust out of interest I deleted all gateway pods:\\n\\n```shell\\n$ k delete pod -l app.kubernetes.io/component=standalone-gateway\\npod \\"zeebe-gateway-6c9f95b557-flgz6\\" deleted\\npod \\"zeebe-gateway-6c9f95b557-gk57z\\" deleted\\n```\\n\\nWe can see that the throughput goes almost directly down, but recovers again. It took ~4 min until we\\nreached the normal state (normal throughput of 100 PI/s) again. But we can also see that it is only a short period\\nof time, where nothing happens. \\n\\n![](restart2.png)\\n\\nIdeally we would define some anti-affinity on the gateway pods, to reduce the risk of losing all gateways at once.\\n\\n## Result\\n\\nWe were able to verify and proof our hypothesis.\\n\\n> _Hypothesis: When restarting / killing a zeebe standalone gateway we expect only a small\\nimpact on current requests, new requests should be routed to the right gateway and the cluster can\\nmake progress._\\n\\nAs we saw above the resources were almost equally used, which is in our normal zeebe benchmarks not the case. \\nIn the benchmarks we use the helm charts and there is no ingress controller enabled, \\nso we have no good load balancing of the incoming requests.\\n\\nThe benefit of the standalone gateway now stands out: losing one is not too problematic than one broker with an embedded gateway,\\nsince a broker takes longer to restart and is likely to be an leader for a partition, which will then also cause a leader change. \\nFurthermore, we can scale the gateways independently. \\n\\n## Found Issues\\n\\n### Optimize resources\\n\\nDuring experiment with the CCSaaS cluster I observed that the Optimize importer was crashlooping due to this load (PI 100/s).\\n\\n```shell\\n[zell zell-chaos/ cluster: ultrachaos ns:448f5091-8d15-4c01-a0ee-202437c09d83-zeebe]$ kgpo\\nNAME                                                     READY   STATUS             RESTARTS   AGE\\n...\\noptimize-deployment-importer-archiver-65679b6449-pb7kz   0/1     CrashLoopBackOff   5          128m\\n...\\n```\\n\\nChecking the pod shows:\\n```shell\\n    State:          Waiting\\n      Reason:       CrashLoopBackOff\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    3\\n      Started:      Tue, 15 Feb 2022 14:25:36 +0100\\n      Finished:     Tue, 15 Feb 2022 14:26:33 +0100\\n    Ready:          False\\n```\\n\\nChecking the logs we can see that it runs continuously out of memory.\\n```shell\\n\\nZell\ufffc  13 minutes ago\\n13:32:48.493 [ZeebeImportScheduler-1] WARN  org.elasticsearch.client.RestClient - request [POST http://elasticsearch:9200/zeebe-record-process-instance/_search?routing=2&typed_keys=true&max_concurrent_shard_requests=5&ignore_unavailable=false&expand_wildcards=open&allow_no_indices=true&ignore_throttled=true&request_cache=false&search_type=query_then_fetch&batched_reduce_size=512&ccs_minimize_roundtrips=true] returned 1 warnings: [299 Elasticsearch-7.16.2-2b937c44140b6559905130a8650c64dbd0879cfb \\"[ignore_throttled] parameter is deprecated because frozen indices have been deprecated. Consider cold or frozen tiers in place of frozen indices.\\"]\\n13:32:49.104 [ImportJobExecutor-pool-ZeebeProcessInstanceImportService-0] WARN  org.elasticsearch.client.RestClient - request [HEAD http://elasticsearch:9200/optimize-process-instance-benchmark?ignore_throttled=false&ignore_unavailable=false&expand_wildcards=open%2Cclosed&allow_no_indices=false] returned 1 warnings: [299 Elasticsearch-7.16.2-2b937c44140b6559905130a8650c64dbd0879cfb \\"[ignore_throttled] parameter is deprecated because frozen indices have been deprecated. Consider cold or frozen tiers in place of frozen indices.\\"]\\njava.lang.OutOfMemoryError: Java heap space\\nDumping heap to java_pid8.hprof ...\\nHeap dump file created [611503401 bytes in 1.070 secs]\\nTerminating due to java.lang.OutOfMemoryError: Java heap space\\n```\\n\\n### Gateway metrics\\n\\nIt looks like that in our latest ccsm helm charts, we no longer export the gateway metrics which we should fix.\\n\\n### Gateway Anti-affinity\\n\\nCurrently, we have no anti-affinity defined for the gateway, which could cause on preemption to take down all gateways."},{"id":"/2022/02/01/High-Snapshot-Frequency","metadata":{"permalink":"/zeebe-chaos/2022/02/01/High-Snapshot-Frequency","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2022-02-01-High-Snapshot-Frequency/index.md","source":"@site/blog/2022-02-01-High-Snapshot-Frequency/index.md","title":"High Snapshot Frequency","description":"Today we wanted to experiment with the snapshot interval and verify that a high snapshot frequency will not impact our availability (#21).","date":"2022-02-01T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.52,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"High Snapshot Frequency","date":"2022-02-01T00:00:00.000Z","categories":["chaos_experiment","data"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Standalone Gateway in CCSaaS","permalink":"/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS"},"nextItem":{"title":"Handling of Big Variables","permalink":"/zeebe-chaos/2022/01/19/big-variables"}},"content":"Today we wanted to experiment with the snapshot interval and verify that a high snapshot frequency will not impact our availability ([#21](https://github.com/camunda/zeebe-chaos/issues/21)).\\n\\n\\n**TL;DR;** The chaos experiment succeeded :muscle: We were able to prove our hypothesis.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n### Snapshot Interval\\n\\nAs we can see in the [docs](https://docs.camunda.io/docs/self-managed/zeebe-deployment/operations/resource-planning/#snapshots) a snapshot is defined as:\\n\\n> A snapshot is a projection of all events that represent the current running state of the processes running on the partition. It contains all active data, for example, deployed processes, active process instances, and not yet completed jobs.\\n\\nPer default snapshots are taken every 5 minutes, by leaders and followers. If a follower is lagging behind (with replication) the leader will, after reaching a certain threshold, prefer to send the follower a snapshot instead of replicating X amount of records. We recently observed that this currently happens quite often, see [#8565](https://github.com/camunda-cloud/zeebe/issues/8565).\\n\\nThe snapshot interval can be changed via an environment variable: `ZEEBE_BROKER_DATA_SNAPSHOTPERIOD`\\n\\n### Expected\\n\\nWe expect that even if the snapshot interval is low (so the frequency of taking snapshot is high) we not run into any availability issues and the cluster should still be healthy. Lower snapshot interval might impact the performance, since taking a snapshot can take some time but other than that it shouldn\'t have any effect.\\n\\n### Actual\\n\\nAs usual, we run again two benchmarks to compare them. One base which has the [default benchmark configuration](https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks/setup/default) and one with a changed snapshot interval.\\n\\nFor the second benchmark we set the snapshot interval to one minute. Like this:\\n```\\nenv:\\n  ...\\n    - name: ZEEBE_BROKER_DATA_SNAPSHOTPERIOD\\n    value: \\"1m\\"\\n```\\n\\nThroughput wise we can see a small difference, but this might be more related that on the base benchmark one node is leader for all partitions.\\n\\n\\n| Base | Chaos |\\n|------|-------|\\n| ![](chaos-base-general.png) | ![](chaos-general.png) |\\n\\n\\nIn general the cluster with the small snapshot interval shows no negative effect. What we can see is that the install request rate increased. It seems to be currently have no affect, but it is likely that if more partitions are added it might become an issue.\\n\\n\\n| Base | Chaos |\\n|------|-------|\\n| ![](chaos-base-install-freq.png) | ![](chaos-install-freq.png) |\\n\\nFurther investigation needs to be done as part of [#8565](https://github.com/camunda-cloud/zeebe/issues/8565).\\n\\n\\n#### Smaller intervals\\n\\nThe smallest interval which Zeebe supports is `1m == 1 minute`. If we configure for example `1s`\\n\\n```\\nenv:\\n  ...\\n    - name: ZEEBE_BROKER_DATA_SNAPSHOTPERIOD\\n    value: \\"1s\\"\\n```\\n\\nWe see the following exception in the log and the broker fails to start.\\n\\n```\\njava.lang.IllegalArgumentException: Snapshot period PT1S needs to be larger then or equals to one minute.\\n```\\n\\n#### Bigger intervals\\n\\nIn order to verify how Zeebe reacts on a bigger snapshot interval we have set the interval to 30 minutes.\\n\\n```\\nenv:\\n  ...\\n    - name: ZEEBE_BROKER_DATA_SNAPSHOTPERIOD\\n    value: \\"30m\\"\\n```\\nIn general, it looked good. What we can see is that one node was restarted in between and took a while to come back. \\n\\n![](big-general.png)\\n\\n\\nThis is expected due to the high snapshot interval, but interesting to observe. The leader had no snapshot yet produced, which means it had to replicate all events to the restarted follower. Only if the follower catches up on all partitions its bootstrap process is complete, and it can mark itself as ready. As we see it can take a while if there is no snapshot available, since new records are incoming all the time. \\n\\nAfter the leader of partition two took a snapshot and the leader sent this snapshot to the follower, the follower were able to become ready.\\n\\nEven with a big snapshot interval we can see that as soon as a new snapshot is taken it is sent to the followers, which is suboptimal.\\n\\n![](big-install-freq.png)\\n\\nAn important thing to keep in mind when playing around with snapshots is the logstream/journal size. The journal is only compacted after taking a snapshot, if we take snapshot less frequent this means we clean up less frequent. The log can grow much bigger with big snapshot intervals.\\n\\n![](big-interval-log.png)\\n\\n\\n### Result\\n\\nThe chaos experiment succeeded :tada: We verified that a smaller snapshot interval has no negative impact on the cluster availability, at least for a small amount of partitions. \\n\\n## Found Bugs\\n\\n * Existing issue regarding the install requests [#8565](https://github.com/camunda-cloud/zeebe/issues/8565)"},{"id":"/2022/01/19/big-variables","metadata":{"permalink":"/zeebe-chaos/2022/01/19/big-variables","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2022-01-19-big-variables/index.md","source":"@site/blog/2022-01-19-big-variables/index.md","title":"Handling of Big Variables","description":"New Year;New Chaos","date":"2022-01-19T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.29,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Handling of Big Variables","date":"2022-01-19T00:00:00.000Z","categories":["chaos_experiment","bpmn","variables"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"High Snapshot Frequency","permalink":"/zeebe-chaos/2022/02/01/High-Snapshot-Frequency"},"nextItem":{"title":"Worker count should not impact performance","permalink":"/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance"}},"content":"New Year;:tada:New Chaos:monkey:\\n\\nThis time I wanted to experiment with \\"big\\" variables. Zeebe supports a `maxMessageSize` of 4 MB, which is quite big. In general, it should be clear that using big variables will cause performance issues, but today I also want to find out whether the system can handle big variables (~1 MB) at all. \\n\\n**TL;DR;** Our Chaos experiment failed! Zeebe and Camunda Cloud is not able to handle (per default) big variables (~1 MB) without issues.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nNormally we run our benchmarks with ~32 KB payload size. This time we want to try out a payload size of ~1 MB and verify whether the system can handle such payload sizes. The payload we use can be found [here](pathname://big_payload.json). \\n\\nThe benchmark setup, is similar to default Zeebe benchmarks you can find [here](https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks/setup/default). To make it work and fair we updated the starter and worker resources for both, base and the chaos cluster.\\n\\n```diff\\ndiff --git a/benchmarks/setup/default/starter.yaml b/benchmarks/setup/default/starter.yaml\\nindex 78c6e81dbb..d0404d4d3e 100644\\n--- a/benchmarks/setup/default/starter.yaml\\n+++ b/benchmarks/setup/default/starter.yaml\\n@@ -30,11 +30,11 @@ spec:\\n             value: \\"warn\\"\\n         resources:\\n           limits:\\n-            cpu: 250m\\n-            memory: 256Mi\\n+            cpu: 1G\\n+            memory: 2Gi\\n           requests:\\n-            cpu: 250m\\n-            memory: 256Mi\\n+            cpu: 1G\\n+            memory: 2Gi\\n ---\\n apiVersion: v1\\n kind: Service\\ndiff --git a/benchmarks/setup/default/worker.yaml b/benchmarks/setup/default/worker.yaml\\nindex cd6f5ffeb6..05b195291f 100644\\n--- a/benchmarks/setup/default/worker.yaml\\n+++ b/benchmarks/setup/default/worker.yaml\\n@@ -31,11 +31,11 @@ spec:\\n             value: \\"warn\\"\\n         resources:\\n           limits:\\n-            cpu: 500m\\n-            memory: 256Mi\\n+            cpu: 1G\\n+            memory: 1Gi\\n           requests:\\n-            cpu: 500m\\n-            memory: 256Mi\\n+            cpu: 1G\\n+            memory: 1Gi\\n```\\n\\n### Expected\\n\\nIt is expected that the performance will drop, we formulate the following hypothesis.\\n\\n**Hypothesis: With a bigger payload size of e.g. 1 MB, Zeebe should be still able to handle process instances, maybe under a degraded performance, but in general the availability must not suffer from such a payload size.**\\n\\n\\n### Actual\\n\\n#### Base\\n\\nWe started a base benchmark with ~32 KB to verify how it looks like normally.\\n\\n![base](base-general.png)\\n\\n\\n#### Small Payload\\n\\nIn order to verify how Zeebe handles different payload, we first started with a small payload ~130 bytes, which is part of the Starter application (called `small_payload.json`). \\n\\n![small-payload](small-payload.png)\\n\\nWe can see that the system handles such payload without any issues, and we can reach ~190 process instances per second (PI/s).\\n\\n#### Big Payload\\n\\nAfter running with a small payload, we changed the payload to a size of ~1 MB. This immediately broke the standalone gateways.\\n\\n![big-payload](big-payload-starter-gw-restarts.png)\\n\\nThe gateways went out of memory (OOM) in a loop. No processing was made in this time.\\n\\n##### Increasing Resources\\n\\nIn order to continue the experiment and to verify how Zeebe itself can handle it, we increased the gateway resources.\\n\\n```diff\\ndiff --git a/benchmarks/setup/default/zeebe-values.yaml b/benchmarks/setup/default/zeebe-values.yaml\\nindex 371ba538dc..7a11c10366 100644\\n--- a/benchmarks/setup/default/zeebe-values.yaml\\n+++ b/benchmarks/setup/default/zeebe-values.yaml\\n@@ -38,10 +38,10 @@ gateway:\\n   resources:\\n     limits:\\n       cpu: 1\\n-      memory: 512Mi\\n+      memory: 4Gi\\n     requests:\\n       cpu: 1\\n-      memory: 512Mi\\n+      memory: 4Gi\\n```\\n\\nBut this doesn\'t help. The gateway went no longer OOM, but it was still not able to handle the payload.\\n\\n![increase-res](big-payload-increase-res.png)\\n\\nWe can see that in a short period of time some events have been processed (small spike in the \\"Current Events\\" panel), but this stopped quite fast again. In the gateway logs there are endless warnings:\\n\\n```\\nWarning 2022-01-20 10:09:32.644 CET zeebe-cluster-helm \\"Stream Error\\"\\nWarning 2022-01-20 10:09:56.847 CET zeebe-cluster-helm \\"Stream Error\\"\\n```\\n\\nWith an underlying exception: `io.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place` \\n\\n<details>\\n<summary>Stacktrace</summary>\\n```\\nio.netty.handler.codec.http2.Http2Exception$StreamException: Stream closed before write could take place\\n\\tat io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:172) ~[netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$FlowState.cancel(DefaultHttp2RemoteFlowController.java:481) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2RemoteFlowController$1.onStreamClosed(DefaultHttp2RemoteFlowController.java:105) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection.notifyClosed(DefaultHttp2Connection.java:357) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.removeFromActiveStreams(DefaultHttp2Connection.java:1007) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection$ActiveStreams.deactivate(DefaultHttp2Connection.java:963) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:515) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2Connection$DefaultStream.close(DefaultHttp2Connection.java:521) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler.closeStream(Http2ConnectionHandler.java:613) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onRstStreamRead(DefaultHttp2ConnectionDecoder.java:444) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onRstStreamRead(Http2InboundFrameLogger.java:80) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.readRstStreamFrame(DefaultHttp2FrameReader.java:509) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:259) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:159) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:173) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:378) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:438) [netty-codec-http2-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:510) [netty-codec-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:449) [netty-codec-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:279) [netty-codec-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [netty-transport-classes-epoll-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.73.Final.jar:4.1.73.Final]\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.73.Final.jar:4.1.73.Final]\\n\\tat java.lang.Thread.run(Unknown Source)\\n```\\n</details>\\n\\nOn the client side we can see that the Zeebe cluster seems to be unavailable.\\n\\n### Camunda Cloud\\n\\nWe wanted to verify how Camunda Cloud and our standard Cluster plan (GA Hardware Plan) handles such a payload. But the result was the same.\\n\\n![cc-general](cc-general.png)\\n\\nThe processing stopped quite fast due to OOM of the gateway. We can see that operate is also not able to handle such load.\\n\\n![failed-op](failed-operate.png)\\n\\nIn our console overview we see that all services (exception Zeebe) went unhealthy\\n\\n![console-healthy](console-healthy.png)\\n\\n### Result\\n\\n> *Hypothesis: With a bigger payload size of e.g. 1 MB Zeebe, should be still able to handle process instances, maybe under a degraded performance but in general the availability must not suffer from such a payload size.*\\n\\n**We were not able to validate our hypothesis, which means our chaos experiment failed!** :boom:\\n\\n### Found Bugs\\n\\nWe opened the following bug issues:\\n\\n * Gateway can\'t handle bigger payload sizes [#8621](https://github.com/camunda-cloud/zeebe/issues/8621)\\n\\n# Outtakes\\n\\nInteresting issues I run into when doing the chaos experiment, could be count as TIL events and mentioning them might help others.\\n\\n## Message pack is not valid\\n\\nWhen I first generated the JSON payload, it was an array on root level, which is not supported by Zeebe. \\n\\nI spent sometime to understand why I see no progress in processing. Taking a look at the gateway logs we can see:\\n\\n`\\"Expected to handle gRPC request, but messagepack property was invalid: io.camunda.zeebe.msgpack.MsgpackPropertyException: Property \'variables\' is invalid: Expected document to be a root level object, but was \'ARRAY\'\\"`\\n\\nOn the client side (if the logging is turned on, starter needs info logging) we see:\\n\\n`INVALID_ARGUMENT: Property \'variables\' is invalid: Expected document to be a root level object, but was \'ARRAY\'`\\n\\n\\n## Configure the Starter payload\\n\\nIn order to use different JSON payload for the starter we support a configuration on the starter application (`-Dapp.starter.payloadPath`). I had a lot of *\\"fun\\"* to find out the right syntax:\\n\\n * -Dapp.starter.payloadPath=\\"bpmn/small_payload.json\\" - *DOESN\'T WORK*\\n * -Dapp.starter.payloadPath=\\"/bpmn/small_payload.json\\" - *DOESN\'T WORK*\\n * -Dapp.starter.payloadPath=/bpmn/small_payload.json - *DOESN\'T WORK*\\n * -Dapp.starter.payloadPath=bpmn/big_payload.json - *WORKS*\\n\\n\\nSo be aware don\'t use `\\"` and no `/` in front, otherwise you might get a `java.io.FileNotFoundException: \\"bpmn/small_payload.json\\" (No such file or directory)` in your starter deployment and wonder why you see no progress."},{"id":"/2021/11/24/Worker-count-should-not-impact-performance","metadata":{"permalink":"/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-11-24-Worker-count-should-not-impact-performance/index.md","source":"@site/blog/2021-11-24-Worker-count-should-not-impact-performance/index.md","title":"Worker count should not impact performance","description":"In this chaos day we experimented with the worker count, since we saw recently that it might affect the performance (throughput) negatively if there are more workers deployed. This is related to #7955 and #8244.","date":"2021-11-24T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":2.5,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Worker count should not impact performance","date":"2021-11-24T00:00:00.000Z","categories":["chaos_experiment","bpmn","performance"],"tags":["performance"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Handling of Big Variables","permalink":"/zeebe-chaos/2022/01/19/big-variables"},"nextItem":{"title":"Not produce duplicate Keys","permalink":"/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys"}},"content":"In this chaos day we experimented with the worker count, since we saw recently that it might affect the performance (throughput) negatively if there are more workers deployed. This is related to [#7955](https://github.com/camunda-cloud/zeebe/issues/7955) and [#8244](https://github.com/camunda-cloud/zeebe/issues/8244).\\n\\nWe wanted to prove, that even if we have more workers deployed the throughput of the process instance execution should not have an negative impact.\\n\\n**TL;DR;** We were not able to prove our hypothesis. Scaling of workers can have a negative impact on performance. Check out the [third chaos experiment](#third-chaos-experiment).\\n\\n\x3c!--truncate--\x3e\\n\\n## First Chaos Experiment\\n\\nWe run the first experiment with one partition, three brokers, one standalone gateway and one starter which creates 100 PI/s. In this experiment we deployed different zeebe [benchmarks](https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks) with 4, 8 and 16 workers.\\n\\n### Expected\\n\\nThe workers should be able to complete all created instances and if the workers are scaled the throughput should remain.\\n\\n### Actual\\n\\n#### 4 Worker\\n\\n![worker-4-p1-general.png](worker-4-p1-general.png)\\n\\n#### 8 Worker\\n\\n![worker-8-p1-general.png](worker-8-p1-general.png)\\n#### 16 Worker\\n\\n![worker-16-p1-general.png](worker-16-p1-general.png)\\n\\n##### Result \\n\\nWhat we can see is that if we increase the worker number it will decrease the throughput, this might be explained with the case that we sent more activation requests / commands which need to be handled by the Brokers. We can see that the back pressure is higher with 16 workers.\\n\\n## Second Chaos Experiment\\n\\nWe will repeat first experiment with some changes, to the partition count. We will now use three partitions.\\n\\n### Expected\\n\\nThe workers should be able to complete all created instances and if the workers are scaled the throughput should remain or be better.\\n\\n### Actual\\n\\n#### 4 Worker\\n\\n![worker-4-p3-general.png](worker-4-p3-general.png)\\n\\n#### 8 Worker\\n\\n![worker-8-p3-general.png](worker-8-p3-general.png)\\n\\n#### 16 Worker\\n\\n![worker-16-p3-general.png](worker-16-p3-general.png)\\n\\n##### Result \\n\\nIn this experiment we can see that all benchmarks reach almost the same throughput, since we not really stressing the system and have enough resources to work with. There is no backpressure at all. In the next experiment we will increase the load.\\n\\n## Third Chaos Experiment\\n\\nWe will repeat second experiment with some changes to the instance creation count. We will now start 300 process instances per second, with one starter.\\n\\n### Expected\\n\\nThe workers should be able to complete most of the instances, we would expect that with more workers we would be able to complete more instances and have no negative impact on the system.\\n\\n### Actual\\n\\n#### 4 Worker\\n\\n![worker-4-p3-300-general.png](worker-4-p3-300-general.png)\\n\\n#### 8 Worker\\n\\n![worker-8-p3-general.png](worker-8-p3-300-general.png)\\n\\n#### 16 Worker\\n\\n![worker-16-p3-general.png](worker-16-p3-300-general.png)\\n\\n##### Result \\n\\nBetween eight and four workers we see the expected difference, that more workers increases the throughput and we are able to complete more instances in a second. The result of 16 workers is completely unexpected. We observed that after short time frame the completion throughput completely droped, and only process instances are created. \\n\\nWe were able to reproduce this behavior, which shows the weakness again.\\n\\n![worker-16-p3-general-reproduce.png](worker-16-p3-300-general-reproduce.png)\\n\\nWe were not able to prove our hypothesis, that scaling of workers has no negative impact on performance.\\n\\n## Further Analysis\\n\\nWe created a bug issue to analyze and fix this weakness [#8267](https://github.com/camunda-cloud/zeebe/issues/8267)."},{"id":"/2021/11/11/Not-produce-duplicate-Keys","metadata":{"permalink":"/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-11-11-Not-produce-duplicate-Keys/index.md","source":"@site/blog/2021-11-11-Not-produce-duplicate-Keys/index.md","title":"Not produce duplicate Keys","description":"Due to some incidents and critical bugs we observed in the last weeks, I wanted to spent some time to understand the issues better and experiment how we could detect them. One of the issue we have observed was that keys were generated more than once, so they were no longer unique (#8129). I will describe this property in the next section more in depth.","date":"2021-11-11T00:00:00.000Z","tags":[{"inline":true,"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":5.425,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Not produce duplicate Keys","date":"2021-11-11T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Worker count should not impact performance","permalink":"/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance"},"nextItem":{"title":"Throughput on big state","permalink":"/zeebe-chaos/2021/10/29/Throughput-on-big-state"}},"content":"Due to some incidents and critical bugs we observed in the last weeks, I wanted to spent some time to understand the issues better and experiment how we could detect them. One of the issue we have observed was that keys were generated more than once, so they were no longer unique ([#8129](https://github.com/camunda-cloud/zeebe/issues/8129)). I will describe this property in the next section more in depth.\\n\\n\\n**TL;DR;** We were able to design an experiment which helps us to detect duplicated keys in the log. Further work should be done to automate such experiment and run it agains newer versions.\\n\\n\x3c!--truncate--\x3e\\n\\n## Unique Keys\\n\\nIn Zeebe each element must have an cluster wide unique key. This is a property we expect in several areas inside, but also outside of Zeebe (external services). We can call it an invariant we have to guarantee.\\n\\nIn order to have cluster wide unique key\'s we encode the partition id in the key\'s. You can check this [code](https://github.com/camunda-cloud/zeebe/blob/develop/protocol/src/main/java/io/camunda/zeebe/protocol/Protocol.java#L71-L73) for more details. Furthermore only the Leader (of a partition) are in charge and allowed to generate new keys. If a fail-over happens the new leader need to continue with generating new keys, he has to resume where the other Leader have left-of. \\n\\nThe last part was exactly the issue we had, see [#8129](https://github.com/camunda-cloud/zeebe/issues/8129). The new leader generated key\'s, which have been already generated by the previous leader. This caused inconsistency in our internal state, but also in external services like Operate.\\n\\n## Chaos Experiment\\n\\nThis time it is a bit different approach, since we know it will *fail* and we want to examine how we can detect the weakness.\\nIn order to understand how we can detect and prevent/verify that this will not happen again, I want to run an experiment against 1.2.0. Later we should try to automate this experiment and run it against newer versions.\\n\\nAs described [above](#unique-keys) the issue was caused due to a fail-over, to be more specific, the Follower has already reached the end of the log (replayed all events) and on fail-over he had nothing to replay. In order to reproduce this situation we will do the following experiment:\\n\\n  1. Deploy an simple process model, with start and end event.\\n  2. Start an instance and await the result `zbctl create instance \\"simpleProcess\\" --withResult --insecure`\\n  3. Wait a short period of time\\n  4. Restart the current Leader `k delete pod zell-chaos-zeebe-2`\\n  5. Wait until a new Leader is choosen\\n  6. Start an instance and await the result `zbctl create instance \\"simpleProcess\\" --withResult --insecure`\\n\\nAfter doing this we should copy the data from the new Leader and exermine the log (with [zdb](https://github.com/Zelldon/zdb)). Alternatively, we could also verify the exporter log in elastic.\\n\\nIn general it should be clear that if we take a look at the log the key can appear multiple times, but they should only reference one single entity/element.\\n\\nA key is normally generated when processing a command and written as follow up event the first time to the log. For example when we process the `ACTIVATE_ELEMENT` command we write the key of the new entity on `ELEMENT_ACTIVATED` to the log.\\nKeys are assigned for other types of entities as well, like process instance creations, deployments etc. Here it works similar.\\n\\nIf the follow-up event contains a key which doesn\'t occurre the first time, then we have a problem, and this is how we want to detect it.\\n\\n### Expected\\n\\nTo follow the chaos engineering approach we will define here a hypothesis, even if we know it might not true for 1.2.0. But later we can reuse it for newer version, because here it should apply.\\n\\n*We expect even if we processed all records and have a fail-over that the new leader should not generate the same keys again.*\\n\\n### Actual\\n\\nWe started a benchmark with version 1.2.0, three brokers and one partition to reduce the scope and followed the described experiment above.\\n\\n\\nThe second process instance creation returned a key (`2251799813685258`) which was incremented by one, compared to the other process instance (`2251799813685257`). Here we already knew that our experiment caused the issue. Because, multiple new entities are generated during processing of a process instance and we awaited the result, at the first process instance creation. This means we have generated already more than one key (on the old leader), which hasn\'t been picked up by the new leader.\\n\\nWe copyed the data from the new Leader and exermined the log via `zdb`. First we printed the complete log into a separate file as json:\\n\\n```sh\\n  zdb log print -p raft-partition/partitions/1 | jq > data.json\\n```\\n\\nThis makes it human readable and also processable by other tools, like `jq`. After checking the file we already found one duplicate.\\n\\nThe first key was part of index 12 in term one and the duplicate was part of index 20 in term 2 (new leader term)\\n\\n```json\\n {\\n      \\"index\\": 12,\\n      \\"term\\": 1,\\n      \\"entries\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"value\\": {\\n            \\"version\\": 1,\\n            \\"processDefinitionKey\\": 2251799813685249,\\n            \\"bpmnProcessId\\": \\"simpleProcess\\",\\n            \\"processInstanceKey\\": 2251799813685256,\\n            \\"parentElementInstanceKey\\": -1,\\n            \\"flowScopeKey\\": -1,\\n            \\"parentProcessInstanceKey\\": -1,\\n            \\"elementId\\": \\"simpleProcess\\",\\n            \\"bpmnElementType\\": \\"PROCESS\\"\\n          },\\n          \\"timestamp\\": 1636627776427,\\n          \\"position\\": 26,\\n          \\"valueType\\": \\"PROCESS_INSTANCE\\",\\n          \\"intent\\": \\"ACTIVATE_ELEMENT\\",\\n          \\"recordType\\": \\"COMMAND\\",\\n          \\"rejectionType\\": \\"NULL_VAL\\",\\n          \\"rejectionReason\\": \\"\\",\\n          \\"brokerVersion\\": \\"1.2.0\\",\\n          \\"sourceRecordPosition\\": 25,\\n          \\"key\\": 2251799813685256\\n        },\\n        {\\n          \\"partitionId\\": 1,\\n          \\"value\\": {\\n            \\"version\\": 1,\\n            \\"processDefinitionKey\\": 2251799813685249,\\n            \\"bpmnProcessId\\": \\"simpleProcess\\",\\n            \\"processInstanceKey\\": 2251799813685256,\\n            \\"variables\\": {}\\n          },\\n          \\"timestamp\\": 1636627776427,\\n          \\"position\\": 27,\\n          \\"valueType\\": \\"PROCESS_INSTANCE_CREATION\\",\\n          \\"intent\\": \\"CREATED\\",\\n          \\"recordType\\": \\"EVENT\\",\\n          \\"rejectionType\\": \\"NULL_VAL\\",\\n          \\"rejectionReason\\": \\"\\",\\n          \\"brokerVersion\\": \\"1.2.0\\",\\n          \\"sourceRecordPosition\\": 25,\\n          \\"key\\": 2251799813685257\\n        }\\n      ]\\n    },\\n```\\n\\n```json\\n {\\n      \\"index\\": 20,\\n      \\"term\\": 2,\\n      \\"entries\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"value\\": {\\n            \\"version\\": 1,\\n            \\"processDefinitionKey\\": 2251799813685249,\\n            \\"bpmnProcessId\\": \\"simpleProcess\\",\\n            \\"processInstanceKey\\": 2251799813685257,\\n            \\"parentElementInstanceKey\\": -1,\\n            \\"flowScopeKey\\": -1,\\n            \\"parentProcessInstanceKey\\": -1,\\n            \\"elementId\\": \\"simpleProcess\\",\\n            \\"bpmnElementType\\": \\"PROCESS\\"\\n          },\\n          \\"timestamp\\": 1636627887305,\\n          \\"position\\": 46,\\n          \\"valueType\\": \\"PROCESS_INSTANCE\\",\\n          \\"intent\\": \\"ACTIVATE_ELEMENT\\",\\n          \\"recordType\\": \\"COMMAND\\",\\n          \\"rejectionType\\": \\"NULL_VAL\\",\\n          \\"rejectionReason\\": \\"\\",\\n          \\"brokerVersion\\": \\"1.2.0\\",\\n          \\"sourceRecordPosition\\": 45,\\n          \\"key\\": 2251799813685257\\n        },\\n        {\\n          \\"partitionId\\": 1,\\n          \\"value\\": {\\n            \\"version\\": 1,\\n            \\"processDefinitionKey\\": 2251799813685249,\\n            \\"bpmnProcessId\\": \\"simpleProcess\\",\\n            \\"processInstanceKey\\": 2251799813685257,\\n            \\"variables\\": {}\\n          },\\n          \\"timestamp\\": 1636627887305,\\n          \\"position\\": 47,\\n          \\"valueType\\": \\"PROCESS_INSTANCE_CREATION\\",\\n          \\"intent\\": \\"CREATED\\",\\n          \\"recordType\\": \\"EVENT\\",\\n          \\"rejectionType\\": \\"NULL_VAL\\",\\n          \\"rejectionReason\\": \\"\\",\\n          \\"brokerVersion\\": \\"1.2.0\\",\\n          \\"sourceRecordPosition\\": 45,\\n          \\"key\\": 2251799813685258\\n        }\\n```\\n\\nSince reading and checking the complete file is a bit hard and error prone, we tried to exermine the data with `jq`. This worked quite well, we were able to detect the duplicates with an `jq` expression.\\n\\n```sh\\ncat data.json | jq -r \'.records[] | select(.entries  != null) |  .entries[] | select (.intent == \\"ELEMENT_ACTIVATED\\" or .intent == \\"CREATED\\") | .key\' data.json | sort | uniq -c -d | awk \'{print $2}\'\\n\\n2251799813685257\\n2251799813685258\\n2251799813685263\\n2251799813685264\\n```\\n\\nThis expression we can use for further experiments and for automating such."},{"id":"/2021/10/29/Throughput-on-big-state","metadata":{"permalink":"/zeebe-chaos/2021/10/29/Throughput-on-big-state","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-10-29-Throughput-on-big-state/index.md","source":"@site/blog/2021-10-29-Throughput-on-big-state/index.md","title":"Throughput on big state","description":"In this chaos day we wanted to prove the hypothesis that the throughput should not significantly change even if we have bigger state, see zeebe-chaos#64","date":"2021-10-29T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"}],"readingTime":3.145,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Throughput on big state","date":"2021-10-29T00:00:00.000Z","categories":["chaos_experiment","bpmn","processing"],"tags":["performance"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Not produce duplicate Keys","permalink":"/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys"},"nextItem":{"title":"Recovery (Fail Over) time","permalink":"/zeebe-chaos/2021/10/05/recovery-time"}},"content":"In this chaos day we wanted to prove the hypothesis that the throughput should not significantly change even if we have bigger state, see [zeebe-chaos#64](https://github.com/camunda/zeebe-chaos/issues/64)\\n\\nThis came up due observations from the [last chaos day](/2021-10-05-recovery-time/index.md). We already had a bigger investigation here [zeebe#7955](https://github.com/camunda-cloud/zeebe/issues/7955). \\n\\n**TL;DR;** We were not able to prove the hypothesis. Bigger state, more than 100k+ process instances in the state, seems to have an big impact on the processing throughput.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nSimilar to the [last chaos day](/2021-10-05-recovery-time/index.md) we set up three brokers, with one partition and replication factor three. \\n\\nAs first part of the experiment we start some amount of instances, afterwards we want to complete them with our workers. Based on the [last chaos day](/2021-10-05-recovery-time/index.md) we know what we can complete ~100 process instances with one worker and a capacity of `12`. An example worker configuration can be found [here](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/default/worker.yaml)\\n\\nWe changed the following:\\n\\n```sh\\n$ diff default/worker.yaml zell-chaos/worker.yaml \\n11c11\\n<   replicas: 12\\n---\\n>   replicas: 1\\n26c26\\n<               -Dapp.worker.capacity=120\\n---\\n>               -Dapp.worker.capacity=12\\n```\\n\\n### Experiment One\\n\\nAs first experiment we start with creating `100.000` instances, afterwards we start the worker.\\n\\nIn order to do that easily we can configure the [starter](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/default/starter.yaml) and reduce the rate to `100` and set the duration limit to `1000`. This means it will run for `1000` second and start each second `100` instances (which makes `100.000`).\\n\\n```shell\\n$ diff default/starter.yaml zell-chaos/starter.yaml \\n24,26c24,26\\n<               -Dapp.starter.rate=200\\n<               -Dapp.starter.durationLimit=0\\n---\\n>               -Dapp.starter.rate=100\\n>               -Dapp.starter.durationLimit=1000\\n```\\n\\n#### Expected\\n\\nWe expected that we can create `100.000` instances and can complete them with a rate of `~100`, as we have seen in other experiments.\\n\\n#### Actual\\n\\nWe were able to create all instances without any issues.\\n\\n![](exp1-100k-instances.png)\\n\\nWe were able to complete all instances without any issues and the throughput was ~100 completion per second.\\n\\n![](exp1-completion.png)\\n\\nWe can see that the snapshot was at some point ~650 MB big.\\n\\n![](exp1-state.png)\\n\\n\\n### Experiment Two\\n\\nAs second experiment we wanted to increase the state by factor `10`, which means `1.000.000` instances in the state and then start the worker.\\n\\nIn order to do that we changed the following in the starter configuration:\\n\\n```shell\\n$ diff default/starter.yaml zell-chaos/starter.yaml \\n24,26c24,26\\n<               -Dapp.starter.rate=200\\n<               -Dapp.starter.durationLimit=0\\n---\\n>               -Dapp.starter.rate=100\\n>               -Dapp.starter.durationLimit=10000\\n```\\n\\n#### Expected\\n\\nWe expect that we can create `1.000.000` instances and can complete them with a rate of `~100`, as we have seen in other experiments.\\n\\n#### Actual\\n\\nWe were **not** able to create all instances without issues.\\n\\n![](exp2-starting-general.png)\\n\\nWe see that at some point the throughput drops and backpressure seem to kick in. When this happens the state is only a bit bigger than in the previous experiment\\n\\n![](exp2-state.png)\\n\\nThe running instances seem to be around ~170K at this time.\\n\\n![](exp2-running.png)\\n\\n\\nWe can see that the processing queue increased at the same time and reached a critical point (over 150 records in the backlog).\\n\\n![](exp2-queue.png)\\n\\nThe overall latency seems to increase after this time (which makes sense)\\n\\n![](exp2-latency.png)\\n\\nWe haven\'t started the workers for this experiment, since the throughput already break before.\\n\\n### Experiment Three\\n\\nIn order to make sure that this is related to the state we run an experiment with the [simpleStarter](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/default/simpleStarter.yaml). This starter starts a process, which contains only a start and end event.\\n\\nWe let this starter run for more than one day and haven\'t experienced any issues on this one.\\n\\n![](exp3-general.png)\\n\\n## Result\\n\\nAs written above the throughput seem to break, after we reach a certain state size.\\n\\nIt might be just a trigger to get the system into stumblling, which means: after one thing takes a bit longer the processing queue gets longer and the processor is not able to catch up any more. This causes then backpressure to kick in etc.\\n\\nI think we need to further investigate this."},{"id":"/2021/10/05/recovery-time","metadata":{"permalink":"/zeebe-chaos/2021/10/05/recovery-time","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-10-05-recovery-time/index.md","source":"@site/blog/2021-10-05-recovery-time/index.md","title":"Recovery (Fail Over) time","description":"In the last quarter we worked on a new \\"feature\\" which is called \\"building state on followers\\". In short,","date":"2021-10-05T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":4.895,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Recovery (Fail Over) time","date":"2021-10-05T00:00:00.000Z","categories":["chaos_experiment","fail_over"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Throughput on big state","permalink":"/zeebe-chaos/2021/10/29/Throughput-on-big-state"},"nextItem":{"title":"Old-Clients","permalink":"/zeebe-chaos/2021/09/23/Old-Clients"}},"content":"In the last quarter we worked on a new \\"feature\\" which is called \\"building state on followers\\". In short,\\nit means that the followers apply the events to build there state, which makes regular snapshot \\nreplication unnecessary and allows faster role transition between Follower-to-Leader. In this chaos\\nday I wanted to experiment a bit with this property, we already did some benchmarks [here](https://github.com/camunda-cloud/zeebe/issues/7515).\\nToday, I want to see how it behaves with larger state (bigger snapshots), since this needed to be\\ncopied in previous versions of Zeebe, and the broker had to replay more than with the newest version.\\n\\nIf you want to now more about build state on followers check out the [ZEP](https://github.com/zeebe-io/enhancements/blob/master/ZEP007-build-state-on-followers.md)\\n\\n**TL;DR;** In our experiment we had almost no downtime, with version 1.2, the new leader was very fast able to pick up the next work (accept new commands). \\n\\n\x3c!--truncate--\x3e\\n\\n## First Chaos Experiment\\n\\nWe will run two benchmarks one with 1.1 version and one with 1.2, to compare the differences between \\nthe versions. We will run three brokers, with one partition and replication factor three. \\n\\nIn order to build up state we run the `starter` with a `durationLimit`, example cfg:\\n\\n```shell\\n            value: >-\\n              -Dapp.brokerUrl=zell-chaos-12-zeebe-gateway:26500\\n              -Dapp.starter.rate=100\\n              -Dapp.starter.durationLimit=1000\\n              -Dzeebe.client.requestTimeout=62000\\n              -XX:+HeapDumpOnOutOfMemoryError\\n```\\n\\nThis means that we run a rate of 100 PI/s creations over 1000 seconds. We expect at the end around \\n100.000 PI, which should be enough to simulate a \\"big state\\". \\n\\nAfter executing the starters we can see in the metrics the running instances:\\n\\n![instances](instances.png)\\n\\nAnd that the snapshot is around 600 to 700 MB.\\n\\n![snapshot](snapshot.png)\\n\\n### Expected\\n\\nWe expect that if we restart the current leader that a new leader is fast (under seconds) able to\\ntake over and continues the work. The version 1.2 should perform here much better than 1.1.\\n\\n### Actual\\n\\nJust normal bootstrap takes some time, on version 1.1:\\n\\n![base-start-up](base-start-up.png)\\n\\nFor version 1.2:\\n\\n![12-start-up](12-start-up.png)\\n\\nAfter running the starters for a certain duration and restarting the leader we can see that \\nthe processor recovery takes by *factor 10* longer on version 1.1. Unfortunately, we have not the\\nleader transition metric in that version to compare against.\\n\\n| **Version** | **1.1**  |  **1.2**  |\\n|---|---|---|\\n| Recovery  | [![base-recovery](base-recovery.png)](base-recovery.png) | [![12-recovery](12-recovery.png)](12-recovery.png) | \\n| General  | [![base-recovery-general](base-recovery-general.png)](base-recovery-general.png)  | [![12-recovery-general](12-recovery-general.png)](12-recovery-general.png)  | \\n\\n*Sorry for the small pictures*\\n\\nIn general what we have seen is that it is not so easy to compare if there is no longer load on the\\nsystem, which is the reason I did a second experiment with: A) \\"big state\\" and B) steady load.\\n\\n## Second Chaos Experiment\\n\\nSimilar setup to the first experiment, but additionally after the \\"big state\\" is reached a steady\\nload is put on the system. One starter with a rate of 100 PI/s and one worker completing some jobs.\\n\\nWith that setup we want to verify how it affects the system if now a leader change happens.\\n\\n### Expected\\n\\nSimilar to above expect that if we restart the current leader that a new leader is fast \\n(under seconds) able to take over and continues the work. The version 1.2 should perform here much\\nbetter than 1.1.\\n\\n### Actual\\n\\n\\n| **Version** | **1.1**  |  **1.2**  |\\n|---|---|---|\\n| Recovery  | [![base-general-state-and-throughput-recover-time.png](base-general-state-and-throughput-recover-time.png)](base-general-state-and-throughput-recover-time.png) | [![12-general-state-and-throughput-recover-time.png](12-general-state-and-throughput-recovery-time.png)](12-general-state-and-throughput-recovery-time.png) | \\n| General  | [![base-general-state-and-throughput-recover-general.png](base-general-state-and-throughput-recover-general.png)](base-general-state-and-throughput-recover-general.png)  | [![12-general-state-and-throughput-recover-general.png](12-general-state-and-throughput-recovery-general.png)](12-general-state-and-throughput-recovery-general.png)  |\\n\\nAfter running the experiment again, this time with load, we can see that the version 1.1 took almost\\n2 minutes! The newest Zeebe version (1.2), with building state on followers, took ~80 milliseconds!\\n\\nWe can see this much better also in the processing and throughput metrics on version 1.1 we have ~2\\nminutes gap.\\n\\n![base-general-state-and-throughput-recover-general-zoom.png](base-general-state-and-throughput-recover-general-zoom.png)\\n\\nThe exporters can recover a bit faster than the processing, but we are for a while not able to accept\\nany commands.\\n\\nIn version 1.2 on the other hand we are able to almost immediately continue with the processing, some\\nmetrics are not even able to show a gap in between, like the current events.\\n\\n![12-general-state-and-throughput-recover-general-zoom.png](12-general-state-and-throughput-recovery-general-zoom.png)\\n\\n# Result\\n\\nIn general, we were able to show that the new approach of building state on followers, gives us an\\nexcellent benefit in transitioning between Follower and Leader. Furthermore, it allows us to handle\\nmuch larger state, since this doesn\'t need to be replicated on a regular basis.\\n\\n# Found Bugs\\n\\n## Running Instances\\n\\nWhen experimenting with the clusters, building the state and deploying the steady load I \\naccidentally deployed to many workers. This caused to complete all existing running instances. The \\nissues here is that on the new leader the metric is zero, which results in a negative metric. \\n\\n![broken-metric](broken-metric.png)\\n\\nMore problematic is actually that if you than build state again, you might reach the zero and if you\\nobserve the cluster you can\'t be sure what the actual count of instances are. This makes the metric\\nkind of useless.\\n\\n![broken-metric](broken-metric-zero.png)\\n\\n## Performance\\n\\nDuring the experimenting it looked like that the performance of 1.2 degraded compared to 1.1. At the\\nend I had on each benchmark one starter with 100 PI/s and one worker with capacity 12.\\n\\nWith version 1.1 it looked like we reached ~100 PI/s created/completed\\n![base-general-state-and-throughput-recovery-general-perf.png](base-general-state-and-throughput-recovery-general-perf.png)\\n\\nWith version 1.2 we just reached ~30, which means it reduced by factor 3.\\n![12-general-state-and-throughput-recovery-general-perf.png](12-general-state-and-throughput-recovery-general-perf.png)\\n\\nI think we need to verify whether this is really the case.\\n\\n**Update:**\\n\\nI run again a benchmark for both versions, with one worker and one starter. It showed no significant \\ndifference on throughput.\\n\\n| **Version** | **1.1**  |  **1.2**  |\\n|---|---|---|\\n| Performance  | [![perf-11](perf-11.png)](perf-11.png) | [![perf-12](perf-12.png)](perf-12.png) |\\n\\nMy current assumption is that it was related to the previous build up state and switching between \\ndifferent worker configurations etc. Let us see whether we can observe this again.\\n\\n**Update 2:**\\n\\nThe second benchmark failed several days again, without any intervention. I investigated that issue further and it seem to be related to frequent install requests, which are sent by the leader. See for more information the related issue https://github.com/camunda-cloud/zeebe/issues/7955"},{"id":"/2021/09/23/Old-Clients","metadata":{"permalink":"/zeebe-chaos/2021/09/23/Old-Clients","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-09-23-Old-Clients/index.md","source":"@site/blog/2021-09-23-Old-Clients/index.md","title":"Old-Clients","description":"It has been awhile since the last post, I\'m happy to be back.","date":"2021-09-23T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":2.61,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Old-Clients","date":"2021-09-23T00:00:00.000Z","categories":["chaos_experiment","clients"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Recovery (Fail Over) time","permalink":"/zeebe-chaos/2021/10/05/recovery-time"},"nextItem":{"title":"Slow Network","permalink":"/zeebe-chaos/2021/07/06/Slow-Network"}},"content":"It has been awhile since the last post, I\'m happy to be back.\\n\\nIn today\'s chaos day we want to verify the hypothesis from [zeebe-chaos#34](https://github.com/camunda/zeebe-chaos/issues/34) that old \\nclients can\'t disrupt a running cluster.\\n\\nIt might happen that after upgrading your Zeebe to the newest shiny version, you might forget to \\nupdate some of your workers or starters etc. This should normally not an issue since Zeebe is \\nbackwards compatible, client wise since 1.x. But what happens when older clients are used. Old \\nclients should not have a negative effect on a running cluster.\\n\\n**TLDR** Older clients (0.26) have no negative impact on a running cluster (1.2), and clients after \\n1.x are still working with the latest version. \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe will run a simple setup with, three nodes and three partitions (replication factor 3). The \\nversion we use is the latest release candidate (1.2.0). Normally we run a load of 200 process \\ninstances per second (pi/s) on our benchmarks. This time we will put a load of 100 pi/s to get\\nsomething running and start an old starter (v0.26.x) with the same frequency. Later we will scale\\nthe old starter to see whether it makes any effect.\\n\\n### Expected\\n\\nWe expect that we can start and complete the 100 pi/s, since we can normally run 200 pi/s.\\n\\n### Actual\\n\\nThe cluster was first started with starters of the same version, and we saw a stable load of ~100 \\nprocess instances completed per second. After starting the old starters (with version 0.26.3), we\\ncan\'t observe any difference. \\n\\nInteresting is even when scaling the starters up to 10 replicas, which means 1000 PI creations per \\nsecond, it doesn\'t seem to make any effect. *Side note:* The \\n[starters](https://github.com/camunda-cloud/zeebe/tree/develop/benchmarks/project) have been\\nmodified, such they only start instances without deploying the model.\\n\\n![old26-general](old26-general.png)\\n\\nThe drops we see in the processing are related to restart\'s.\\n\\nThe gateway and grpc metrics doesn\'t indicate that more requests are sent. \\n\\n![old26-grpc](old26-grpc.png)\\n![old26-gateway](old26-gateway.png)\\n\\nIf we take a look in the clients log, we can see that the request are failing because the RPC Method names have been changed between 0.26 and 1.0. \\n\\n```shell\\njava.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNIMPLEMENTED: Method not found: gateway_protocol.Gateway/CreateWorkflowInstance\\n\\t...\\nCaused by: io.grpc.StatusRuntimeException: UNIMPLEMENTED: Method not found: gateway_protocol.Gateway/CreateWorkflowInstance\\n\\t...\\n```\\n\\nIt seems this kind of \\"old\\" requests can be blocked quite early in the request chain to make no effect. \\n\\nIn order to experiment a bit further I created a starter image with version 1.0 to see whether this still works with our newest release candidate 1.2.\\n\\nWe can see in the metrics right after starting the starter that the throughput goes up and we can reach our 200 pi/s.\\n\\n![old10-general](old10-general.png)\\n\\nWe run the benchmark overnight, and we haven\'t seen any issues. Be aware that the throughput is calculated over the 24h which makes is lower than 200.\\n\\n![general](general.png)\\n\\nFurthermore, taking a look at the resource consumption, especially at the gateway, gives no evidence that something wrong is going on.\\n\\n![res](res.png)\\n\\n### Result\\n\\nWe were able to confirm the hypothesis written in [zeebe-chaos#34](https://github.com/camunda/zeebe-chaos/issues/34), that an old client can\'t disrupt a running cluster. \\n\\n## Found Bugs\\n\\nNone this time :)"},{"id":"/2021/07/06/Slow-Network","metadata":{"permalink":"/zeebe-chaos/2021/07/06/Slow-Network","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-07-06-Slow-Network/index.md","source":"@site/blog/2021-07-06-Slow-Network/index.md","title":"Slow Network","description":"On a previous Chaos Day we played around with ToxiProxy , which allows injecting failures on the network level. For example dropping packages, causing latency etc.","date":"2021-07-06T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":5.905,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Slow Network","date":"2021-07-06T00:00:00.000Z","categories":["chaos_experiment","broker","network","leader"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Old-Clients","permalink":"/zeebe-chaos/2021/09/23/Old-Clients"},"nextItem":{"title":"Full Disk Recovery","permalink":"/zeebe-chaos/2021/06/08/Full-Disk"}},"content":"On a previous [Chaos Day](/2020-10-06-toxi-proxy/index.md) we played around with [ToxiProxy](https://github.com/Shopify/toxiproxy) , which allows injecting failures on the network level. For example dropping packages, causing latency etc.\\n\\nLast week [@Deepthi](https://github.com/deepthidevaki) mentioned to me that we can do similar things with [tc](https://man7.org/linux/man-pages/man8/tc.8.html), which is a built-in linux command. Today I wanted to experiment with latency between leader and followers using `tc`.\\n\\n**TL;DR;** The experiment failed; With adding 100ms network delay to the Leader we broke the complete processing throughput. :boom:\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe want to experiment with network latency and what kind of effect has a slow network on the cluster. \\n\\n### Hypothesis\\n\\nWe expect that we can handle certain network latency, due to our heartbeat and election time timeouts. After, reaching the deadlines we expect fail overs and followers which are lagging behind.\\n\\nThis means under a certain threshold we should be able to still process user commands, with slightly delay but without real issues. After reaching the deadline and the fail overs, it should be possible to continue, since we will add only to one node the delay.\\n\\n### Experiment\\n\\n#### TC\\n\\nTC is a built in linux command, the manpage summarizes it as `tc - show / manipulate traffic control settings`.\\n\\nIn order to [add delay to a network interface](https://netbeez.net/blog/how-to-use-the-linux-traffic-control/), we can run the following:\\n\\n```shell\\ntc qdisc add dev eth0 root netem delay 200ms\\n```\\n\\nMore details ([taken from the blog post](https://netbeez.net/blog/how-to-use-the-linux-traffic-control/)):\\n```\\nqdisc: modify the scheduler (aka queuing discipline)\\nadd: add a new rule\\ndev eth0: rules will be applied on device eth0\\nroot: modify the outbound traffic scheduler (aka known as the egress qdisc)\\nnetem: use the network emulator to emulate a WAN property\\ndelay: the network property that is modified\\n200ms: introduce delay of 200 ms\\n```\\n\\nAdding this kind of rule means that we add a delay to all outgoing connections, which are going over this network interface.\\n\\n#### Actual\\n\\nIn order to reduce the blast radius we will run the experiment with one partition on a three broker cluster (replication factor 3).\\n\\n##### Steady State\\n\\n As we can see in the benchmark we are able to reach in avg. ~77 process instance creation and completions per second.\\n\\n![base](general.png)\\n\\nThe process instance execution time (from start to end) is under 1 second.\\n![base](latency.png)\\n\\nThe commit latency is about 100 ms.\\n![base](commit-lat.png)\\n\\nWe can see that one of the followers is a bit lagging behind, but not too far.\\n\\n![base](raft-follower.png)\\n\\n##### 100 ms\\n\\nIn the first iteration of the experiment we added a 100 ms delay to the leaders outgoing traffic. \\n\\n```sh\\n[zell zell-chaos-day/ cluster: zeebe-cluster ns:zell-chaos-day]$ k exec -it zell-chaos-day-zeebe-gateway-579c76978f-npz2k -- zbctl status --insecure\\nCluster size: 3\\nPartitions count: 1\\nReplication factor: 3\\nGateway version: 1.1.0-SNAPSHOT\\nBrokers:\\n  Broker 0 - zell-chaos-day-zeebe-0.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26501\\n    Version: 1.1.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy  Broker 1 - zell-chaos-day-zeebe-1.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26501\\n    Version: 1.1.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy  Broker 2 - zell-chaos-day-zeebe-2.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26501\\n    Version: 1.1.0-SNAPSHOT\\n    Partition 1 : Leader, Healthy\\n```\\n\\nBased on the `zbctl` output, or the grafana dashboard we can find out who is the leader. Note that the output of `zbctl` looks still a bit broken, related issue [#6692](https://github.com/camunda-cloud/zeebe/issues/6692).\\n\\nWith the following commands we can add the delay of 100 ms to the leaders outgoing traffic.\\n```sh\\n[zell zell-chaos-day/ cluster: zeebe-cluster ns:zell-chaos-day]$ k exec -it zell-chaos-day-zeebe-2 -- bash\\nroot@zell-chaos-day-zeebe-2:/usr/local/zeebe# tc qdisc add dev eth0 root netem delay 100ms\\nroot@zell-chaos-day-zeebe-2:/usr/local/zeebe# tc -s qdisc\\nqdisc noqueue 0: dev lo root refcnt 2 \\n Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) \\n backlog 0b 0p requeues 0\\nqdisc netem 8001: dev eth0 root refcnt 2 limit 1000 delay 100.0ms\\n Sent 11635496 bytes 10086 pkt (dropped 0, overlimits 0 requeues 0) \\n backlog 339773b 77p requeues 0\\n```\\n\\nAlmost immediately we see a drop in our general section of the Grafana Dashboard.\\n\\n![base](100ms-general.png)\\n\\nThe backpressure increased significantly. As expected the commit latency increased.\\n\\n![base](100ms-commit-lat.png)\\n\\nThe processing latency as well.\\n\\n![base](100ms-latency.png)\\n\\nIt was unexpected that the throughput breaks down so much. We can see in the send request that a lot of the requests are ended with timeouts or with resource exhausted.\\n\\n![base](100ms-grpc.png)\\n![base](100ms-gateway.png)\\n\\nInteresting is that no worker is able to activate nor complete any job. This cause increasing of the running process instances, so the state is growing.\\n\\n![base](100ms-running-proc.png)\\n\\nTaking a look at the raft metrics we see that this already caused some heartbeat misses, but no leader change.\\n\\n![base](100ms-heartbeats.png)\\n\\n\\n![base](100ms-follower.png)\\n\\nThe follower is now lagging far more behind. We can see in the logs that the Leader tries to send `InstallRequests`, but these are also timing out.\\n\\n```shell\\nRaftServer{raft-partition-partition-1} - InstallRequest{currentTerm=1, leader=2, index=1173627, term=1, version=1, chunkId=HeapByteBuffer{position=0, remaining=10, limit=10, capacity=10, mark=java.nio.HeapByteBuffer[pos=0 lim=10 cap=10], hash=1744147670}, nextChunkId=HeapByteBuffer{position=0, remaining=7, limit=7, capacity=7, mark=java.nio.HeapByteBuffer[pos=0 lim=7 cap=7], hash=1283029304}, data=HeapByteBuffer{position=0, remaining=10626817, limit=10626817, capacity=10626817, mark=java.nio.HeapByteBuffer[pos=0 lim=10626817 cap=10626817], hash=1083445787}, initial=false, complete=false} to 1 failed: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Request ProtocolRequest{id=489616, subject=raft-partition-partition-1-install, sender=zell-chaos-day-zeebe-2.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26502, payload=byte[]{length=10647731, hash=1655826849}} to zell-chaos-day-zeebe-1.zell-chaos-day-zeebe.zell-chaos-day.svc.cluster.local:26502 timed out in PT5S\\"\\n```\\n\\nThe follower is starting regularly elections, but is not able to overturn the existing leader.\\n\\n\\n\\nIn general, in the Gateway logs we can see the start of the delay injection quite good. \\n\\n```shell\\nD 2021-07-06T10:05:51.263195Z Received REACHABILITY_CHANGED for broker 2, do nothing.\\nD 2021-07-06T10:05:53.264480Z Received REACHABILITY_CHANGED for broker 1, do nothing.\\nD 2021-07-06T10:05:54.184981Z Received REACHABILITY_CHANGED for broker 2, do nothing.\\nD 2021-07-06T10:05:54.213904Z Received REACHABILITY_CHANGED for broker 1, do nothing.\\nD 2021-07-06T10:05:54.361408Z Received REACHABILITY_CHANGED for broker 0, do nothing.\\nD 2021-07-06T10:05:54.986270Z Received REACHABILITY_CHANGED for broker 0, do nothing.\\nD 2021-07-06T10:05:56.617134Z Received REACHABILITY_CHANGED for broker 2, do nothing.\\nD 2021-07-06T10:05:57.072707Z Expected to handle gRPC request, but request timed out between gateway and broker\\nD 2021-07-06T10:05:57.126207Z Expected to handle gRPC request, but request timed out between gateway and broker\\nD 2021-07-06T10:05:57.157683Z Expected to handle gRPC request, but request timed out between gateway and broker\\n```\\n\\nSimilar logs we see on the broker side.\\n\\nIn general, we can say the experiment failed. The cluster was not able to run our normal workload. It seem to behave quite bad, but there were no leader change at all.\\n\\n##### 250 ms\\n\\nIn order to verify whether 250 ms, will cause a leader election we reconfigured the delay. It looked quite similar, performance wise, but the heartbeats for one of the followers increased. Still it was not enough to cause a leader change.\\n\\nAfter several minutes (~30) of running this configuration we were able to observe that one of the other followers missed heartbeats as well. This finally caused a leader change.\\n\\n\\n![base](250ms-raft.png)\\n\\nThe throughput came back, it was similar to what is was before.\\n\\n![base](250ms-general.png)\\n\\nThe processing execution latency was higher than usual.\\n\\n![base](250ms-latency.png)\\n\\nSimilar to the commit latency, interesting to see such an affect caused by a follower with network issues.\\n\\n![base](250ms-commit-lat.png)\\n\\n### Result\\n\\nAs written before the experiment failed, the hypothesis was not met. We were not able to add latency to the network, which is much lower than our deadlines (heartbeat timeout is 2.5 seconds), without causing any harm to our processing throughput/latency.\\n\\nStill we had some interesting observations we can use for our next experiments and insight which we need to consider on setting up Zeebe in environment where the network might be unreliable/slow."},{"id":"/2021/06/08/Full-Disk","metadata":{"permalink":"/zeebe-chaos/2021/06/08/Full-Disk","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-06-08-Full-Disk/index.md","source":"@site/blog/2021-06-08-Full-Disk/index.md","title":"Full Disk Recovery","description":"On this chaos day we wanted to experiment with OOD recovery and ELS connection issues. This is related to the following issues from our hypothesis backlog: zeebe-chaos#32 and zeebe-chaos#14. This time @Nico joined me.","date":"2021-06-08T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":7.125,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Full Disk Recovery","date":"2021-06-08T00:00:00.000Z","categories":["chaos_experiment","broker","disk"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Slow Network","permalink":"/zeebe-chaos/2021/07/06/Slow-Network"},"nextItem":{"title":"Time travel Experiment","permalink":"/zeebe-chaos/2021/05/25/Reset-Clock"}},"content":"On this chaos day we wanted to experiment with OOD recovery and ELS connection issues. This is related to the following issues from our hypothesis backlog: [zeebe-chaos#32](https://github.com/camunda/zeebe-chaos/issues/32) and [zeebe-chaos#14](https://github.com/camunda/zeebe-chaos/issues/14). This time [@Nico](https://github.com/korthout) joined me.\\n\\n**TL;DR** The experiment was successful :muscle: and we found several things in the dashboard which we can improve :)\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment \\n\\nWith this experiment we want to verify that Zeebe can recover after OOD, which was caused by not exporting to ELS. For that we want to disconnect Zeebe and ELS first and see how it behaves. Afterwards we connect the services again and expect a recovery of the system.\\n\\nAs usual, we have set up a normal benchmark cluster with three nodes, three partitions and replication factor three. We run 200 PI/s and 12 workers against that cluster.\\n\\n### Expected\\n\\nWe expect the following properties:\\n\\n * at the beginning the system is stable (we can start instances without issues)\\n * after disconnecting ELS we start to fill the disk, since we can\'t export (which means we can\'t compact)\\n * after reaching the disk limits, Zeebe doesn\'t accept any commands anymore\\n * after connecting ELS, Zeebe should start to export again (compacting should be possible again)\\n * after come below the limit, Zeebe should accept commands again\\n\\n\\n#### Network disconnect to ELS\\n\\nIn order to disconnect the Brokers with ELS, we wanted to reuse one of our network disconnect scripts, e.g. [disconnect-leaders.sh](https://github.com/camunda/zeebe-chaos/blob/master/chaos-experiments/scripts/disconnect-leaders.sh). This resolves the IP\'s of the brokers and creates an unreachable route via the `ip` tool at the given brokers.\\n\\nWe copied that and adjusted it to our needs:\\n\\n\x3c!-- {% raw %} --\x3e\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\nelasticName=\\"elasticsearch-master\\"\\n\\nbroker0=$(getBroker 0)\\nbroker2=$(getBroker 2)\\nbroker1=$(getBroker 1)\\n\\nelastic0Ip=$(kubectl get pod \\"$elasticName-0\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\nelastic1Ip=$(kubectl get pod \\"$elasticName-1\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\nelastic2Ip=$(kubectl get pod \\"$elasticName-2\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$elastic0Ip\\"\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$elastic1Ip\\"\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$elastic2Ip\\"\\n\\n\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$elastic0Ip\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$elastic1Ip\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$elastic2Ip\\"\\n\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$elastic0Ip\\"\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$elastic1Ip\\"\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$elastic2Ip\\"\\n```\\n\x3c!-- {% endraw %} --\x3e\\n\\n*Small note in order to run this against the benchmark cluster you need to set the following environment variables:*\\n\\n```shell\\nexport NAMESPACE=$(kubens -c) # the namespace where the resources are located\\nexport CHAOS_SETUP=helm # indicates that the installation is done via helm, necessary since we use different labels in the helm charts and in CC.\\n```\\n\\n\x3c!-- {% raw %} --\x3e\\nRunning the script above didn\'t work as expected. We still saw records being exported. The issue was that we need to use the service IP instead of the IP\'s of the elastic search pods. The Broker uses only the service to connect with ELS. In order to get the IP we can use this `kubectl get services elasticsearch-master --template \\"{{.spec.clusterIP}}\\"`. \\n\\n\\nThe service will take care of the request routing, which means we just need to block one IP. This helps to simplify the disconnect script.\\n\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\nelasticName=\\"elasticsearch-master\\"\\n\\nbroker0=$(getBroker 0)\\nbroker2=$(getBroker 2)\\nbroker1=$(getBroker 1)\\n\\nelasticServiceIp=$(kubectl get services elasticsearch-master --template \\"{{.spec.clusterIP}}\\")\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$elasticServiceIp\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$elasticServiceIp\\"\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$elasticServiceIp\\"\\n```\\n\x3c!-- {% endraw %} --\x3e\\n\\n### Actual\\n\\nIn this section we will describe how we experienced the chaos experiment and what we observed. \\n\\n#### First Try\\nWe run the disconnect script and were able to observe that the exporting stopped. \\n\\n![elastic-disconnect](elastic-disconnect.png)\\n\\nAs expected we were no longer able to compact, which cause an increasing of log segments.\\n\\n![increase-segments](increase-segments.png)\\n\\nWe realized that our current disk size might be too big (it would take a while until we fill it), so we decided to setup a new benchmark with smaller size and different watermarks.\\n\\n```yaml\\n # ...\\n    env:\\n    - name: ZEEBE_BROKER_DATA_DISKUSAGECOMMANDWATERMARK\\n      value: \\"0.6\\"\\n    - name: ZEEBE_BROKER_DATA_DISKUSAGEREPLICATIONWATERMARK\\n      value: \\"0.8\\"\\n\\n\\n# PVC\\npvcAccessMode: [\\"ReadWriteOnce\\"]\\npvcSize: 16Gi\\npvcStorageClassName: ssd\\n```\\n\\nWe thought that we might have different performance, because of such a smaller disk size, but this was not the case. We were able to reach the same level, might be worth to think about reducing the disk sizes more in our benchmarks.\\n\\n![base](next-try-base.png)\\n\\n#### Second Try\\n\\n![base1](next-try-base1.png)\\n\\n##### Disconnecting\\n\\nAfter running our disconnect script we can immediately see that the exporting is stopping.\\n\\n![drop-base](next-try-drop-base.png)\\n\\nInteresting is the elastic section where we see one of our new panels in actions which shows the failure rate. There are two dots, which show the 100% failure rate.\\n\\n![drop-elastic](next-try-drop-elastic-section.png)\\n\\nAfter reaching our disk watermark we can see that the processing stops, and we no longer accept commands.\\n\\n![drop-base-2](next-try-drop-base-2.png)\\n\\nThe cluster turns to unhealthy, which is expected.\\n\\nInteresting is that we have no metrics at all on the gateway side after reaching the watermark.\\n\\n![drop-gw](next-try-drop-gw.png)\\n\\nWe were able to verify that snapshots are still taken, but no compaction.\\n\\n![drop-snapshot](next-try-drop-snapshot.png)\\n\\nNo segments are deleted during this time.\\n\\n![drop-segments](next-try-drop-segments.png)\\n\\nIf we take a look at the processing section we can see that the exporters lag way behind, which of course makes sense.\\n\\n![drop-processing](next-try-drop-processing.png)\\n\\n##### Connecting\\n\\nLuckily we were able to reuse on of our already written reconnect scripts for this experiment, see [connect-leaders.sh](https://github.com/camunda/zeebe-chaos/blob/master/chaos-experiments/scripts/connect-leaders.sh).\\n\\nAfter removing the ip route (connecting the Brokers with ELS again) we can see that it immediately starts to export again.\\n\\n![connect-base](next-try-connect-base.png)\\n\\nWhen we went under the disk watermarks the processing started again and we accepted new commands.\\n\\n### Result\\n\\n![connect-base2](next-try-connect-base2.png)\\n\\n**The experiment was successful, our system was able to recover after an elastic network outage and handled it properly.** :white_check_mark: :muscle:\\n\\nWe noted several issues with the Dashboard, during the chaos experiment observation. For example the Brokers, which went OOD, never went back to the `Healthy` state again.\\n\\n![connect-healthy](next-try-connect-healthy.png)\\n\\nFurthermore, the not exported panel seems to be broken, depending on the selected time frame.\\n\\n![connect-not-exported](next-try-connect-not-exported.png)\\n\\nThere have been also other issues with the panels and sections which we should take a look at. I have listed them below.\\n\\n### Possible Improvements\\n\\nWe observed several issues with the grafana dashboard which I wrote down here. I will create issues or PR\'s to resolve them.\\n\\n#### General Metric Section\\n\\nIf we take a look at the screenshot where we reach our disk watermarks, and the processing stops, the backpressure metrics are not correctly updated. We would expect that the backpressure shows 100%, since all requests are rejected.\\n\\n![drop-base-2](next-try-drop-base-2.png)\\n\\nAfter the cluster actually becomes healthy again (it accepts new commands) it is not shown as healthy in the panels. The metrics seems not to be updated.\\n\\nAnother possible improvement would be to make it more visible that the exporting stopped. One idea is to split the processing into exporting and processing, so having two graphs might help.\\n\\n#### Processing Section\\n\\nDepending on the time frame the panel `Number of records not exported`, seems to show quite high values.\\n\\n![connect-not-exported](next-try-connect-not-exported.png)\\n\\nIf we take a look at other metrics, this doesn\'t make any sense. If we had such a backlog we wouldn\'t expect to compact to one segment for example. Furthemore the tables on the right side show numbers which are quite close to each other.\\n\\n#### Elastic Metrics Section\\n\\nWe probably can improve the failure panel, such that it shows a graph, and the limit is not set to 130%.\\n\\n![drop-elastic](next-try-drop-elastic-section.png)\\n\\n#### GRPC Metric Section\\n\\nThe panel `gRPC requests` should have an dec ordering on the tooltip and should be renamed to `Total gRPC requests` it was a bit confusing to us.\\n\\n#### Throughput Section\\n\\nThe StreamProcessor vs Exporter panel has no data.\\n\\n#### Snapshot Section\\n\\nWe saw that snapshots are created, but the Snapshot replication panel show no data. It seem to be broken (again?).\\n\\n#### Resources Section\\n\\nThe JVM panel has the legend on the right side, which makes it hard to see the metrics if you have multiple windows open on one screen."},{"id":"/2021/05/25/Reset-Clock","metadata":{"permalink":"/zeebe-chaos/2021/05/25/Reset-Clock","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-05-25-Reset-Clock/index.md","source":"@site/blog/2021-05-25-Reset-Clock/index.md","title":"Time travel Experiment","description":"Recently we run a Game day where a lot of messages with high TTL have been stored in the state. This was based on an earlier incident, which we had seen in production. One suggested approach to resolve that incident was to increase the time, such that all messages are removed from the state. This and the fact that summer and winter time shifts can cause in other systems evil bugs, we wanted to find out how our system can handle time shifts. Phil joined me as participant and observer. There was a related issue which covers this topic as well, zeebe-chaos#3.","date":"2021-05-25T00:00:00.000Z","tags":[{"inline":true,"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":8.205,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Time travel Experiment","date":"2021-05-25T00:00:00.000Z","categories":["chaos_experiment","broker","time"],"tags":["data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Full Disk Recovery","permalink":"/zeebe-chaos/2021/06/08/Full-Disk"},"nextItem":{"title":"Corrupted Snapshot Experiment Investigation","permalink":"/zeebe-chaos/2021/04/29/Corrupted-Snapshot"}},"content":"[Recently we run a Game day](https://confluence.camunda.com/display/ZEEBE/Game+Day+18.05.2021) where a lot of messages with high TTL have been stored in the state. This was based on an earlier incident, which we had seen in production. One suggested approach to resolve that incident was to increase the time, such that all messages are removed from the state. This and the fact that summer and winter time shifts can cause in other systems evil bugs, we wanted to find out how our system can handle time shifts. [Phil](https://github.com/saig0) joined me as participant and observer. There was a related issue which covers this topic as well, [zeebe-chaos#3](https://github.com/camunda/zeebe-chaos/issues/3).\\n\\n**TL;DR;** Zeebe is able to handle time shifts back and forth, without observable issues. Operate seems to dislike it.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nAs part of the experiment we had to define what we expect, if we change the time. In order to keep it simple we decided to experiment with one-hour move forward and backwards. We wanted to run the experiment first against our normal benchmark cluster and afterwards against a Production - S Cluster on INT. Furthermore, we decided to test the time shift only on leaders, for now.\\n\\n### Expected\\n\\n*If we move the time one-hour forward we expect:*\\n\\n * in general, timers should be triggered (snapshot, message TTL, job timeouts, timers etc.)\\n * the system should operate normal, means zeebe and operate should be healthy and continue working\\n\\n*If we move the time one-hour backwards we expect:*\\n\\n * timers should not be triggered, until the deadline + 1 hour is reached\\n * the system should operate normal\\n * with operate we were not sure whether it has issues with exported records on the same time\\n\\nBefore we started, with running the experiment, we had to find out how we can change the time in a docker container.\\n\\n### Changing Time\\n\\n***Note:** If you\'re not interested in how we change the time you can jump to the [next section](#experiment-on-benchmark-cluster)*\\n\\nIf you search for it, you find quite quickly answers in how to change the time in a docker container. For example, we found this [answer](https://serverfault.com/a/898842/283059), which was quite useful.\\n\\nIn order to apply this, we first need to make sure that we have the right [linux capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html) to change the system time. For that we need the `SYS_TIME` capability. In our benchmarks this is quite easy to do, we just need to change [this line](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/default/zeebe-values.yaml#L16) and add `SYS_TIME`.\\n\\n```yaml\\npodSecurityContext:\\n  capabilities:\\n        add: [\\"NET_ADMIN\\", \\"SYS_TIME\\"]\\n```\\n\\nAfter changing this, we can set the time via [`date -s`](https://man7.org/linux/man-pages/man1/date.1.html). Since we were not sure whether this really works for our java process, we started a [`jshell`](https://docs.oracle.com/javase/9/tools/jshell.htm#JSWOR-GUID-C337353B-074A-431C-993F-60C226163F00) to verify that. *Note:* the jshell is only available if you use an container image with jdk. This is available, if you build your own zeebe docker image via [zeebe/createBenchmark.sh](https://github.com/camunda-cloud/zeebe/blob/develop/createBenchmark.sh).\\n\\nChanging the time:\\n```sh\\nroot@zell-phil-chaos-zeebe-1:/usr/local/zeebe# date\\nTue May 25 10:29:33 UTC 2021\\nroot@zell-phil-chaos-zeebe-1:/usr/local/zeebe# date +%T -s \\"09:29:00\\"\\n09:29:00\\nroot@zell-phil-chaos-zeebe-1:/usr/local/zeebe# date\\nTue May 25 09:29:01 UTC 2021\\nroot@zell-phil-chaos-zeebe-1:/usr/local/zeebe# jshell\\nPicked up JAVA_TOOL_OPTIONS: -XX:MaxRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -Xlog:gc*:file=/usr/local/zeebe/data/gc.log:time:filecount=7,filesize=8M\\nMay 25, 2021 9:29:03 AM java.util.prefs.FileSystemPreferences$1 run\\nINFO: Created user preferences directory.\\n|  Welcome to JShell -- Version 11.0.11\\n|  For an introduction type: /help intro\\n\\njshell> new Date()\\n$1 ==> Tue May 25 09:29:08 UTC 2021\\n```\\n\\nAfter we found out how we can actually change the time we moved forward and run the described chaos experiment.\\n\\n### Experiment on Benchmark Cluster\\n\\nAs usual, we have set up a normal benchmark cluster with three nodes, three partitions and replication factor three. We run 200 PI/s and 12 workers against that cluster.\\n\\n#### Move Time Forward\\n\\nAfter setting up the cluster we had to find out who is the Leader and picked the one who is Leader for the most of the partitions.\\n\\n```shell\\n$ k exec -it zell-phil-chaos-zeebe-0 -- zbctl status --insecure\\nBrokers:\\n  Broker 0 - zell-phil-chaos-zeebe-0.zell-phil-chaos-zeebe.zell-phil-chaos.svc.cluster.local:26501\\n    Version: 1.1.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy\\n    Partition 2 : Leader, Healthy\\n    Partition 3 : Leader, Healthy\\n    ....\\n```\\n\\n**On Broker 0 we increased the time by one hour.** After doing this we observed the metrics, but we haven\'t noticed any issues. We verified logs in stackdriver, but no errors were thrown.\\n![inc-1-hour-general](inc-1-hour-general.png)\\n\\nWe noticed, looking at the metrics, that the snapshot was triggered when we moved the time forward. This was what we actually expected. Plus also new scheduled snapshot have been triggered and created.\\n![inc-1-hour-snapshot](inc-1-hour-snapshot.png)\\n\\nOn the elastic exporter we can see that flushing has happened earlier than usual, because we increased the time. This was also expected.  \\n![inc-1-hour-export](inc-1-hour-export.png)\\n\\n**Experiment succeeded** :heavy_check_mark:\\n\\n#### Move Time Backwards\\n\\nIn order to run the experiment again, with moving the timer backwards, we set up a new benchmark cluster. This time we run the experiment on Broker 1, since he was the leader of the most partitions.\\n\\n```sh\\nBroker 1 - zell-phil-chaos-zeebe-1.zell-phil-chaos-zeebe.zell-phil-chaos.svc.cluster.local:26501\\n  Version: 1.1.0-SNAPSHOT\\n  Partition 1 : Follower, Healthy\\n  Partition 2 : Leader, Healthy\\n  Partition 3 : Leader, Healthy\\n```\\n\\nNo general issues have been detected, as expected no longer snapshots were taken.\\n![dec-1-hour-general](dec-1-hour-general.png)\\n\\nWe have run the benchmark for a bit longer, to see whether the snapshot will be triggered later, which was indeed the case.\\n![dec-1-hour-snapshot-later](dec-1-hour-snapshot-later.png)\\n\\nWe could also observe how the journal segments increased until we took the next snapshot.\\n![dec-1-hour-snapshot-segments](dec-1-hour-snapshot-segments.png)\\n\\n**Experiment succeeded** :heavy_check_mark:\\n\\n### Experiment on INT\\n\\nAfter running the experiment against our benchmark clusters we were confident to run it against a Production S cluster on INT.\\nWe have set up a Production S cluster in our chaos cluster and run the [cloud benchmark](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/setup/newCloudBenchmark.sh) against it. It starts starter and worker against that Production S cluster, turned out not with the same load (luckily this doesn\'t matter for this experiment). The starter and workers are deployed in the Zeebe Team gke cluster.\\n\\n#### How to change the time on INT\\n\\nOn INT, it is not that simple to get the `SYS_TIME` capability, which we need to change the time. Luckily we already have experience, with getting the necessary capability we need to have.\\nOn a previous chaos day we have added `NET_ADMIN` capability to a running zeebe container in order to experiment with network partitioning, you can read about that [here](/2021-03-23-camunda-cloud-network-partition/index.md).\\n\\nThe following patch adds the `SYS_TIME` cap to our linux container.\\n```yaml\\nspec:\\n  template:\\n    spec:\\n      containers:\\n        - name: \\"zeebe\\"\\n          securityContext:\\n            capabilities:\\n              add:\\n                - \\"SYS_TIME\\"\\n```\\n\\nThe following script applies the patch to our Zeebe cluster.\\n\\n```shell\\n#!/bin/bash\\nset -euo pipefail\\n\\nscriptPath=$( cd \\"$(dirname \\"${BASH_SOURCE[0]}\\")\\" ; pwd -P )\\nsource utils.sh\\n\\nnamespace=$(getNamespace)\\n\\nCLUSTERID=${namespace%-zeebe}\\n\\nkubectl patch zb \\"$CLUSTERID\\" --type merge --patch=\'{\\"spec\\":{\\"controller\\":{\\"reconcileDisabled\\":true}}}\'\\nkubectl patch statefulset zeebe -n \\"$namespace\\" --patch \\"$(cat $scriptPath/sys_time_patch.yaml)\\"\\nkubectl delete pod -l \\"$(getLabel)\\" -n \\"$namespace\\"\\n```\\n\\n#### Move Time Forward on INT\\n\\nAfter we patched our resources we can now change the time as before.\\n\\n```shell\\nroot@zeebe-0:/usr/local/zeebe# date\\nTue May 25 09:55:33 UTC 2021\\nroot@zeebe-0:/usr/local/zeebe# date +%T -s \\"10:55:00\\"\\n10:55:00\\nroot@zeebe-0:/usr/local/zeebe# date\\nTue May 25 10:55:01 UTC 2021\\n```\\n\\nIn the general section of our Grafana Dashboard everything looks fine. We can\'t see any issues here, except that the exporting is much less than the processing.\\n![int-inc-1-hour-general](int-inc-1-hour-general.png)\\n\\nWe took a closer look at the processing panels and saw that the exporter lag a lot behind, which causes Operate lagging behind and that fewer segments are deleted.\\n![int-inc-1-hour-exporting](int-inc-1-hour-exporting.png)\\n\\nOn moving the time forward we see as expected the snapshotting has been triggered.\\n![int-inc-1-hour-snapshot](int-inc-1-hour-snapshot.png)\\n\\nStackdriver shows no errors for the Zeebe service. But later, in operate we observed that no new data was imported after 11:50, we moved the time at 11:55 forward. In the logs we found the following exceptions, which need to be investigated further.\\n\\n```shell\\nException occurred when importing data: io.camunda.operate.exceptions.PersistenceException: Update request failed for [operate-import-position-1.0.0_] and id [1-process-instance] with the message [Elasticsearch exception [type=version_conflict_engine_exception, reason=[1-process-instance]: version conflict, required seqNo [1681], primary term [1]. current document has seqNo [1688] and primary term [1]]].\\n```\\n\\nThe exception in more detail:\\n```java\\nio.camunda.operate.exceptions.PersistenceException: Update request failed for [operate-import-position-1.0.0_] and id [2-job] with the message [Elasticsearch exception [type=version_conflict_engine_exception, reason=[2-job]: version conflict, required seqNo [1765], primary term [1]. current document has seqNo [1771] and primary term [1]]].\\n\\tat io.camunda.operate.util.ElasticsearchUtil.executeUpdate(ElasticsearchUtil.java:271) ~[operate-els-schema-1.0.0.jar!/:?]\\n\\tat io.camunda.operate.zeebeimport.ImportPositionHolder.recordLatestLoadedPosition(ImportPositionHolder.java:100) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\tat io.camunda.operate.zeebeimport.ImportJob.call(ImportJob.java:86) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\tat io.camunda.operate.zeebeimport.RecordsReader.lambda$scheduleImport$1(RecordsReader.java:217) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\tat java.util.concurrent.FutureTask.run(Unknown Source) [?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]\\n\\tat java.lang.Thread.run(Unknown Source) [?:?]\\nCaused by: org.elasticsearch.ElasticsearchStatusException: Elasticsearch exception [type=version_conflict_engine_exception, reason=[2-job]: version conflict, required seqNo [1765], primary term [1]. current document has seqNo [1771] and primary term [1]]\\n\\tat org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:176) ~[elasticsearch-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1933) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:1910) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1667) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1624) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1594) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat org.elasticsearch.client.RestHighLevelClient.update(RestHighLevelClient.java:1061) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\tat io.camunda.operate.util.ElasticsearchUtil.executeUpdate(ElasticsearchUtil.java:266) ~[operate-els-schema-1.0.0.jar!/:?]\\n\\t... 7 more\\n\\tSuppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://elasticsearch:9200], URI [/operate-import-position-1.0.0_/_update/2-job?refresh=true&timeout=1m], status line [HTTP/1.1 409 Conflict]\\n{\\"error\\":{\\"root_cause\\":[{\\"type\\":\\"version_conflict_engine_exception\\",\\"reason\\":\\"[2-job]: version conflict, required seqNo [1765], primary term [1]. current document has seqNo [1771] and primary term [1]\\",\\"index_uuid\\":\\"7jsKYxw7RxWQhba-UIG5Wg\\",\\"shard\\":\\"0\\",\\"index\\":\\"operate-import-position-1.0.0_\\"}],\\"type\\":\\"version_conflict_engine_exception\\",\\"reason\\":\\"[2-job]: version conflict, required seqNo [1765], primary term [1]. current document has seqNo [1771] and primary term [1]\\",\\"index_uuid\\":\\"7jsKYxw7RxWQhba-UIG5Wg\\",\\"shard\\":\\"0\\",\\"index\\":\\"operate-import-position-1.0.0_\\"},\\"status\\":409}\\n\\t\\tat org.elasticsearch.client.RestClient.convertResponse(RestClient.java:326) ~[elasticsearch-rest-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestClient.performRequest(RestClient.java:296) ~[elasticsearch-rest-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestClient.performRequest(RestClient.java:270) ~[elasticsearch-rest-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1654) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1624) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1594) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat org.elasticsearch.client.RestHighLevelClient.update(RestHighLevelClient.java:1061) ~[elasticsearch-rest-high-level-client-7.12.1.jar!/:7.12.1]\\n\\t\\tat io.camunda.operate.util.ElasticsearchUtil.executeUpdate(ElasticsearchUtil.java:266) ~[operate-els-schema-1.0.0.jar!/:?]\\n\\t\\tat io.camunda.operate.zeebeimport.ImportPositionHolder.recordLatestLoadedPosition(ImportPositionHolder.java:100) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\t\\tat io.camunda.operate.zeebeimport.ImportJob.call(ImportJob.java:86) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\t\\tat io.camunda.operate.zeebeimport.RecordsReader.lambda$scheduleImport$1(RecordsReader.java:217) ~[operate-qa-importer-1.0.0.jar!/:?]\\n\\t\\tat java.util.concurrent.FutureTask.run(Unknown Source) [?:?]\\n\\t\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]\\n\\t\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]\\n\\t\\tat java.lang.Thread.run(Unknown Source) [?:?]\\n```\\n\\n**Experiment failed** :x: \\n\\nOperate was not operating normal, the exceptions were not expected.\\n\\nIt seems that changing the time on INT, causes some unexpected problems for Operate. We need to investigate that further and resolve them before we can continue here and make that an automated test. Furthermore, we need to investigate how problematic it is that our exporting lags behind, which is related to [zeebe#6747](https://github.com/camunda-cloud/zeebe/issues/6747), and how we can resolve that.\\n\\nIn general, we saw that Zeebe has no real issues with time shifts and that timers can be triggered by changing the underlying system time. Still we should make sure that our containers are running on UTC time nodes (which we do), such that we avoid issues with daylight saving time."},{"id":"/2021/04/29/Corrupted-Snapshot","metadata":{"permalink":"/zeebe-chaos/2021/04/29/Corrupted-Snapshot","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-04-29-Corrupted-Snapshot/index.md","source":"@site/blog/2021-04-29-Corrupted-Snapshot/index.md","title":"Corrupted Snapshot Experiment Investigation","description":"A while ago we have written an experiment, which should verify that followers are not able to become leader, if they have a corrupted snapshot. You can find that specific experiment here. This experiment was executed regularly against Production-M and Production-S Camunda Cloud cluster plans. With the latest changes, in the upcoming 1.0 release, we changed some behavior in regard to detect snapshot corruption on followers.","date":"2021-04-29T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":7.195,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Corrupted Snapshot Experiment Investigation","date":"2021-04-29T00:00:00.000Z","categories":["chaos_experiment","broker","snapshots"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Time travel Experiment","permalink":"/zeebe-chaos/2021/05/25/Reset-Clock"},"nextItem":{"title":"BPMN meets Chaos Engineering","permalink":"/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering"}},"content":"A while ago we have written an experiment, which should verify that followers are not able to become leader, if they have a corrupted snapshot. You can find that specific experiment [here](https://github.com/camunda/zeebe-chaos/tree/master/chaos-experiments/helm/snapshot-corruption). This experiment was executed regularly against Production-M and Production-S Camunda Cloud cluster plans. With the latest changes, in the upcoming 1.0 release, we changed some behavior in regard to detect snapshot corruption on followers. \\n\\n**NEW** If a follower is restarted and has a corrupted snapshot it will detect it on bootstrap and will refuse to\\nstart related services and crash. This means the pod will end in a crash loop, until this is manually fixed.\\n\\n**OLD** The follower only detects the corrupted snapshot on becoming leader when opening the database. On the restart of a follower this will not be detected.\\n\\nThe behavior change caused to fail our automated chaos experiments, since we corrupt the snapshot on followers and on a later experiment we restart followers. For this reason we had to disable the execution of the snapshot corruption experiment, see related issue\\n[zeebe-io/zeebe-cluster-testbench#303](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/303).\\n\\nIn this chaos day we wanted to investigate whether we can improve the experiment and bring it back. For reference, I also opened a issue to discuss the current corruption detection approach [zeebe#6907](https://github.com/camunda-cloud/zeebe/issues/6907)\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nThis time we look at an already existing experiment. I will run our normal setup and execute the experiment (each step) manually and observe what happens.\\n\\n### Experiment\\n\\n```json\\n{\\n    \\"version\\": \\"0.1.0\\",\\n    \\"title\\": \\"Zeebe can recover from corrupted snapshots\\",\\n    \\"description\\": \\"Zeebe should be able to detect and recover from corrupted snapshot\\",\\n    \\"contributions\\": {\\n        \\"reliability\\": \\"high\\",\\n        \\"availability\\": \\"high\\"\\n    },\\n    \\"steady-state-hypothesis\\": {\\n        \\"title\\": \\"Zeebe is alive\\",\\n        \\"probes\\": [\\n            {\\n                \\"name\\": \\"All pods should be ready\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-readiness.sh\\",\\n                    \\"timeout\\": 900\\n                }\\n            },\\n            {\\n                \\"name\\": \\"Should be able to create process instances on partition 3\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-steady-state.sh\\",\\n                    \\"arguments\\": \\"3\\",\\n                    \\"timeout\\": 900\\n                }\\n            }\\n        ]\\n    },\\n    \\"method\\": [\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Corrupt snapshots on followers\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"corruptFollowers.sh\\",\\n                \\"arguments\\": \\"3\\"\\n            }\\n        },\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Terminate leader of partition 3\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"shutdown-gracefully-partition.sh\\",\\n                \\"arguments\\": [ \\"Leader\\", \\"3\\" ]\\n            }\\n        }\\n    ],\\n    \\"rollbacks\\": []\\n}\\n```\\n\\nAs written before we have our normal benchmark setup and I will run the referenced scripts manually and observe the behavior via grafana. The panels below show the base or steady state.\\n![before-general](before-general.png)\\n![before-snap](before-snap.png)\\n\\n\\nThe first script we will run, corrupts for a certain partition the snapshots of all followers. It does it via just simply deleting some `*.sst` files.\\n\\n```shell\\n[zell scripts/ cluster: zeebe-cluster ns:zell-chaos]$ ./corruptFollowers.sh 3\\n+ ...\\n+ leader=zell-chaos-zeebe-1\\n+ followers=\'zell-chaos-zeebe-0\\nzell-chaos-zeebe-2\'\\n+ ...\\n+ kubectl -n zell-chaos exec zell-chaos-zeebe-0 -- ./corrupting.sh 3\\n+ partition=3\\n+ partitionDir=data/raft-partition/partitions/3\\n+ snapshotDir=(\\"$partitionDir\\"/snapshots/*)\\n++ find data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619 -name \'*.sst\' -print -quit\\n+ fileName=data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619/000110.sst\\n+ rm data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619/000110.sst\\n+ ...\\n+ kubectl -n zell-chaos exec zell-chaos-zeebe-2 -- ./corrupting.sh 3\\n+ partition=3\\n+ partitionDir=data/raft-partition/partitions/3\\n+ snapshotDir=(\\"$partitionDir\\"/snapshots/*)\\n++ find data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619 -name \'*.sst\' -print -quit\\n+ fileName=data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619/000112.sst\\n+ rm data/raft-partition/partitions/3/snapshots/520492-1-1532470-1531619/000112.sst\\n```\\n\\nThe second script just terminated the Leader for the referenced partition.\\n\\n```shell\\n[zell scripts/ cluster: zeebe-cluster ns:zell-chaos]$ ./terminate-partition.sh \\"Leader\\" 3\\npod \\"zell-chaos-zeebe-1\\" deleted\\n```\\n\\nOn the first try we had the \\"luck\\" that there was a snapshot replication in between, such that the follower was able to become Leader, since it had again a valid snapshot. \\n\\n![luck-general](luck-general.png)\\n![luck-replication](luck-replication.png)\\n\\nOn the second try we were actually able to reproduce the behavior we want to see. We have corrupted the snapshot and restarted the Leader and the partition can make only progress - if the previous leader comes back. This is because the others have a corrupted snapshot and can\'t take over.\\n\\n![no-luck-restart-general](no-luck-restart-general.png)\\n![no-luck-restart](no-luck-restart.png)\\n\\nIn all cases above we were able to make progress. The issue now arises when one of the affected follower is restarted. For our experiment I restarted Broker-2, which was at this point in time follower for Partition 3. After I restarted the follower it first looked like the partition one was completely down.\\n\\n![follower-restart-partition-1](follower-restart-partition-1.png)\\n\\nAfter serveral minutes the system recovered and continued.\\n\\n![follower-restart-partition-1-cont](follower-restart-partition-1-cont.png)\\n\\nVia Grafana but also via `kubectl` we can see that the pod doesn\'t become ready again.\\n\\n```shell\\nzell-chaos-zeebe-0                          1/1     Running     0          13m\\nzell-chaos-zeebe-1                          1/1     Running     0          17m\\nzell-chaos-zeebe-2                          0/1     Running     0          5m4s\\nzell-chaos-zeebe-gateway-854dd5dd5c-b8cpl   1/1     Running     0          45m\\n```\\n\\n```shell\\n  Warning  Unhealthy               4m59s               kubelet                  Readiness probe failed: HTTP probe failed with statuscode: 503\\n  Warning  Unhealthy               9s (x30 over 5m9s)  kubelet                  Readiness probe failed: Get http://10.0.29.26:9600/ready: dial tcp 10.0.29.26:9600: connect: connection refused\\n```\\n\\nIn camunda cloud this will end in a crash loop, because we restart pods after 15 minutes if they are not ready in time. It is interesting to see that it seems not to restart by itself, **which I had expected**. After checking the logs we can also see why.\\n\\n```md\\nI 2021-04-29T11:20:44.205851Z RaftServer{raft-partition-partition-1} - Server join completed. Waiting for the server to be READY \\n**W 2021-04-29T11:20:44.220338Z Cannot load snapshot in /usr/local/zeebe/data/raft-partition/partitions/3/snapshots/1387684-4-4014493-4014014. The checksum stored does not match the checksum calculated.** \\nD 2021-04-29T11:20:44.737724Z Loaded disk segment: 33 (raft-partition-partition-3-33.log) \\nD 2021-04-29T11:20:44.738579Z Found segment: 33 (raft-partition-partition-3-33.log) \\nD 2021-04-29T11:20:45.024028Z Loaded disk segment: 34 (raft-partition-partition-3-34.log) \\nD 2021-04-29T11:20:45.024688Z Found segment: 34 (raft-partition-partition-3-34.log) \\nE 2021-04-29T11:20:45.025826Z Bootstrap Broker-2 [6/13]: cluster services failed with unexpected exception. \\nI 2021-04-29T11:20:45.038260Z Closing Broker-2 [1/5]: subscription api \\nD 2021-04-29T11:20:45.040467Z Closing Broker-2 [1/5]: subscription api closed in 1 ms \\nI 2021-04-29T11:20:45.040926Z Closing Broker-2 [2/5]: command api handler \\nD 2021-04-29T11:20:45.042116Z Closing Broker-2 [2/5]: command api handler closed in 1 ms \\nI 2021-04-29T11:20:45.042524Z Closing Broker-2 [3/5]: command api transport \\nD 2021-04-29T11:20:46.163435Z Created segment: JournalSegment{id=2, index=1556849} \\nI 2021-04-29T11:20:47.065203Z Stopped \\nD 2021-04-29T11:20:47.066007Z Closing Broker-2 [3/5]: command api transport closed in 2023 ms \\nI 2021-04-29T11:20:47.066552Z Closing Broker-2 [4/5]: membership and replication protocol \\nE 2021-04-29T11:20:47.067664Z Closing Broker-2 [4/5]: membership and replication protocol failed to close. \\nI 2021-04-29T11:20:47.069050Z Closing Broker-2 [5/5]: actor scheduler \\nD 2021-04-29T11:20:47.069503Z Closing actor thread ground \'Broker-2-zb-fs-workers\' \\nD 2021-04-29T11:20:47.071276Z Closing actor thread ground \'Broker-2-zb-fs-workers\': closed successfully \\nD 2021-04-29T11:20:47.071642Z Closing actor thread ground \'Broker-2-zb-actors\' \\nD 2021-04-29T11:20:47.073287Z Closing actor thread ground \'Broker-2-zb-actors\': closed successfully \\nD 2021-04-29T11:20:47.074101Z Closing Broker-2 [5/5]: actor scheduler closed in 4 ms \\nI 2021-04-29T11:20:47.074468Z Closing Broker-2 succeeded. Closed 5 steps in 2036 ms. \\n**E 2021-04-29T11:20:47.074845Z Failed to start broker 2!** \\nI 2021-04-29T11:20:47.078669Z \\n\\nError starting ApplicationContext. To display the conditions report re-run your application with \'debug\' enabled. \\n**E 2021-04-29T11:20:47.093828Z Application run failed** \\nI 2021-04-29T11:20:47.120419Z Shutting down ExecutorService \'applicationTaskExecutor\' \\nI 2021-04-29T11:20:47.132016Z RaftServer{raft-partition-partition-1}{role=FOLLOWER} - No heartbeat from null in the last PT2.926S (calculated from last 2926 ms), sending poll requests \\nI 2021-04-29T11:20:47.260582Z RaftServer{raft-partition-partition-1} - Found leader 0 \\nI 2021-04-29T11:20:47.262407Z RaftServer{raft-partition-partition-1} - Setting firstCommitIndex to 1507899. RaftServer is ready only after it has committed events upto this index \\nI 2021-04-29T11:20:47.263428Z RaftPartitionServer{raft-partition-partition-1} - Successfully started server for partition PartitionId{id=1, group=raft-partition} in 10098ms \\nD 2021-04-29T11:21:05.128572Z Created segment: JournalSegment{id=3, index=1570617} \\nD 2021-04-29T11:21:26.376398Z Created segment: JournalSegment{id=4, index=1584886} \\nI 2021-04-29T11:21:28.129677Z RaftPartitionServer{raft-partition-partition-2} - Successfully started server for partition PartitionId{id=2, group=raft-partition} in 51572ms \\n```\\n\\nWe can see in the logs that the corruption is detected and the broker seems to stop, but actually it doesn\'t.\\nAfter `Failed to start broker 2` the process should normally end. It looks like the thread for the other raft partitions are still running and continuing. This is a bug which I reported in one of my last chaos days,\\nsee [camunda-cloud/zeebe#6702](https://github.com/camunda-cloud/zeebe/issues/6702).\\n\\nWith this current behavior we could easily fix the snapshot corruption experiments, since we just need a separate clean up step. It could look like this:\\n\\n```shell\\n k exec -it zell-chaos-zeebe-2 -- rm -r data/raft-partition/partitions/3 # remove partition three data\\n k delete pod zell-chaos-zeebe-2 # restart broker 2\\n```\\n\\nI tried this and it worked, after some minutes the Broker-2 was able to come back.\\n\\n![success-after-fix.png](success-after-fix.png)\\n\\nWhy does it work you may ask. If we remove the corrupted partition data from the follower and restart it, then it will join the cluster with an empty state. The Leader for that partition will immediately replicate the snapshot for that partition, such that the follower is up to date again. This allows the follower then to bootstrap without issues again.\\n\\nOne problem with this approach we have if we actually fix the bug above, where we\'re not shutting down, then we have the issue that we might not be able to access the data, since the pod is not up. I need to do some more research to solve this, but one possible solution I can think of would be to patch the *StatefulSet*, such that we can claim the PV via multiple pods. This would allow us to start a separate POD in order to access the data and delete it.\\n\\n## Found Bugs\\n\\n * Broker is not shutting down properly [camunda-cloud/zeebe#6702](https://github.com/camunda-cloud/zeebe/issues/6702)"},{"id":"/2021/04/03/bpmn-meets-chaos-engineering","metadata":{"permalink":"/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-04-03-bpmn-meets-chaos-engineering/index.md","source":"@site/blog/2021-04-03-bpmn-meets-chaos-engineering/index.md","title":"BPMN meets Chaos Engineering","description":"On the first of April (2021) we ran our Spring Hackday at Camunda. This is an event where the developers at camunda come together to work on projects they like or on new ideas/approaches they want to try out. This time we (Philipp and me) wanted to orchestrate our Chaos Experiments with BPMN. If you already know how we automated our chaos experiments before, you can skip the next section","date":"2021-04-03T00:00:00.000Z","tags":[{"inline":true,"label":"tools","permalink":"/zeebe-chaos/tags/tools"}],"readingTime":7.615,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"BPMN meets Chaos Engineering","date":"2021-04-03T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["tools"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Corrupted Snapshot Experiment Investigation","permalink":"/zeebe-chaos/2021/04/29/Corrupted-Snapshot"},"nextItem":{"title":"Set file immutable","permalink":"/zeebe-chaos/2021/03/30/set-file-immutable"}},"content":"On the first of April (2021) we ran our Spring Hackday at Camunda. This is an event where the developers at camunda come together to work on projects they like or on new ideas/approaches they want to try out. This time we ([Philipp](https://github.com/saig0) and [me](https://github.com/zelldon)) wanted to orchestrate our Chaos Experiments with BPMN. If you already know how we automated our chaos experiments before, you can skip the next section\\nand jump directly to the [Hackday Project section](#hackday-project).\\n\\nIn order to understand this blogpost make sure that you have a little understanding of Zeebe, Camunda Cloud and Chaos Engineering. Read the following resources to get a better understanding.\\n\\n * [Get Started with Camund cloud](https://docs.camunda.io/docs/guides/)\\n * [Quickstart Guide](https://docs.camunda.io/docs/product-manuals/clients/cli-client/get-started)\\n * [Camunda Cloud](https://camunda.com/de/products/cloud/)\\n * [Zeebe Process Engine](https://docs.camunda.io/docs/product-manuals/zeebe/zeebe-overview/)\\n * [BPMN 2.0](https://www.omg.org/spec/BPMN/2.0/About-BPMN/)\\n * [Principles of Chaos](https://principlesofchaos.org/)\\n\\n\x3c!--truncate--\x3e\\n\\n## Previous Chaos Automation \\n\\nIn the previous Chaos Day summaries I described that we use [ChaosToolkit](https://chaostoolkit.org/) to run our chaos experiments. The chaos experiments have as prerequisite that an Zeebe cluster is already running, on which they should be executed. ChaosToolkit needs/uses a specific DSL to describe and execute Chaos Experiments. An example experiment looks like the following:\\n\\n```json\\n{\\n    \\"version\\": \\"0.1.0\\",\\n    \\"title\\": \\"Zeebe follower restart non-graceful experiment\\",\\n    \\"description\\": \\"Zeebe should be fault-tolerant. Zeebe should be able to handle followers terminations.\\",\\n    \\"contributions\\": {\\n        \\"reliability\\": \\"high\\",\\n        \\"availability\\": \\"high\\"\\n    },\\n    \\"steady-state-hypothesis\\": {\\n        \\"title\\": \\"Zeebe is alive\\",\\n        \\"probes\\": [\\n            {\\n                \\"name\\": \\"All pods should be ready\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-readiness.sh\\",\\n                    \\"timeout\\": 900\\n                }\\n            },\\n            {\\n                \\"name\\": \\"Should be able to create workflow instances on partition 1\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-steady-state.sh\\",\\n                    \\"arguments\\": \\"1\\",\\n                    \\"timeout\\": 900\\n                }\\n            }\\n        ]\\n    },\\n    \\"method\\": [\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Terminate follower of partition 1\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"terminate-partition.sh\\",\\n                \\"arguments\\": [ \\"Follower\\", \\"1\\"]\\n            }\\n        }\\n    ],\\n    \\"rollbacks\\": []\\n}\\n```\\n\\nThis JSON describes a chaos experiment where a follower of partition one is terminated, and where we expect that we can create a new process instance before and after this termination. The follower termination should not affect our steady state.\\n\\nIn the JSON structure we can see the defined steady state of the Zeebe Cluster and the method/action which should be executed (the chaos which should be injected). The defined steady state is verified at the beginning of the experiment and at the end, after the methods are executed. The execution logic is quite simple. You can also define rollback actions, which should be executed if the experiment fails. Timeouts can be defined for each action and probe. Since the `chaosToolkit` is written in Python you can reference python modules/functions, which should be called during execution. Additionally, it supports bash scripts, which we normally use. Unfortunately bash scripts are sometimes not easy to understand and to maintain. This is one of the reason why we already thought more than once to replace the `chaosToolkit` with something different.\\n\\nThe `chaosToolkit` has more features and extensions, but these are not used by us. \\n\\n### List of Chaos Experiments\\n\\nThe experiment above is just one experiment of our continuous growing collection of chaos experiments, which we have already defined. There exist chaos experiments for the helm charts, but also for camunda cloud, for each cluster plan separately. You can find them [here](https://github.com/camunda/zeebe-chaos/tree/master/chaos-experiments).\\n\\n### Automated Chaos Experiments\\n\\nChaos experiments need to be executed continously, not only once. For that we have build an automated pipeline, which runs the chaos experiments every night or if requested. We did that with help of the [Zeebe Cluster Testbench](https://github.com/zeebe-io/zeebe-cluster-testbench), we call it just `testbench`. The `testbench` creates for each cluster plan, in camunda cloud, a Zeebe cluster and runs the corresponding experiments against these clusters. The process model looks quite simple.\\n\\n![chaos-test](chaos-test.png)\\n\\nIt is executed via a [zbctl chaos worker](https://github.com/zeebe-io/zeebe-cluster-testbench/tree/develop/core/chaos-workers), which is part of the `testbench`. The `chaos worker` polls for new jobs at the `testbench`. On new jobs it executes, based on the cluster plan, against the given/created Zeebe cluster the chaos experiments, via the `chaostoolkit`.\\n\\nIn general this was a good first solution, which is quite extensible since we just needed to add new experiments in the [zeebe-chaos](https://github.com/camunda/zeebe-chaos) repository and on the next run the experiments are executed, without any further adjustments. \\n\\n### Challenges\\n\\nStill, we had some challenges with this approach. \\n\\n#### Additional Dependency\\n\\nWith the `chaosToolkit` we had an additional dependency. If you want to implement/write new chaos experiments you need to set up the `chaosToolkit` on your machine, with the correct Python etc. In general the setup guide was straight forward, but still it was something you need to have. It made the adoption harder.\\n\\n#### Root Cause Analysis\\n\\nDue to our setup it was a bit more challenging todo the root cause analysis.\\n\\n![ChaosOutput](ChaosOutput.png)\\n\\nWe run a `zbctl` worker in a docker image, which picks up the `chaos` typed jobs. An `zbctl` worker will complete jobs with the output of the called handler script. This means that everything, which you want to log, needs to be logged in a separate file. The `chaosToolkit` will print its output into an own log file. The output of the bash scripts, which are executed by the `chaosToolkit`, will also end in that `chaosToolkit.log` file. I tried to visualize this a bit with the image above.\\n\\n![chaos-test](failed-chaos-experiment-testbench.png)\\n\\nIf the chaos worker completes a job, the process instance in `testbench` is continued. If a chaos experiment fails, then the job is still completed normally, but with an error result. In the process instance execution this means that a different path is taken. The `testbench` will write a slack notification to a specific channel, such that the current Zeebe medic can look at it. \\n\\n![run-test](run-test-in-camunda-cloud.png)\\n\\nAfter the notification the medic needs to find out which experiment has failed, this is part of the payload of the completed job at least, but he also needs to find out why the experiment failed. For this root cause analysis he has to check the log of the `chaostoolkit`, which is stored somewhere in the chaos worker pod (`data/chaostoolkit.log` it is an ever growing log).\\n\\n## Hackday Project\\n\\nWith our Hackday Project we had two goals:\\n\\n 1. lower the bar for team adoption \\n 2. make root cause analysis easier\\n    \\nFor that we wanted to replace `chaosToolkit` with a BPMN Process, which should be executed by Zeebe. We wanted to stick with the experiment description (the `chaosToolkit`/openchaos DSL) for our chaos experiments.\\n\\nWe modeled two processes. One root process, which reads for a given cluster plan all experiments and runs then each experiment. This is done via a [multi instance](https://docs.camunda.io/docs/0.25/product-manuals/zeebe/bpmn-workflows/multi-instance/multi-instance/) [call activity](https://docs.camunda.io/docs/reference/bpmn-workflows/call-activities/call-activities/).\\n\\n![ChaosOutput](chaosToolkit.png)\\n\\nThe other process model is used for the real chaos experiment execution. As the `chaosToolkit` execution itself was quite simple, the resulting BPMN model is as well. All activities are\\nsequential multi instances, since we can have multiple probes/actions for the steady state, but also for the injection of chaos. On the root level of the process we have an interrupting [event sub process](https://docs.camunda.io/docs/reference/bpmn-workflows/event-subprocesses/event-subprocesses/) to timeout the chaos experiment if the experiment takes to long.\\n\\n![ChaosExperiment](chaosExperiment.png)\\n\\nAs payload of the process instances we have the defined chaos experiment in JSON, which we have seen earlier. In this JSON we have all information we need to orchestrate this experiment.\\n\\nWe have implemented two Kotlin workers, one to read all experiment JSON files and one to execute the bash scripts, which are referenced in the chaos experiment descriptions. You can find the code [here](https://github.com/camunda/zeebe-chaos/tree/master/chaos-model/chaos-worker), it is just 100 lines long.\\n\\n### Results\\n\\nWe orchestrated the follower/leader termination and graceful shutdown experiments via Camunda Cloud and the created process models. The experiments have been executed against another Zeebe cluster successfully.\\n\\n![success](success-run.png)\\n\\nTo see how we improved the observability, we provoked an experiment to fail. Operate shows use via an incident that an experiment failed, and exactly at which step. \\n\\n![failure](fail-run.png)\\n\\nWe can even see the standard output and error output of the executed script in operate, without searching different log files.\\n\\n![failuremsg](fail-run-output.png)\\n\\nAn timeout of an experiment will look like this:\\n\\n![timeout](timeout-run.png)\\n\\nAs we can see the observability improved here a lot. We are able to understand why it failed based on the error message, since the complete error output is printed, and we can see the progress of the process instance on running the chaos experiment. \\n\\nWith these models we were able to completely replace the `chaosToolkit` usage, so in the end we can remove an additional dependency.\\n\\n### Further Work\\n\\nNext step would be to integrate this in `testbench`, such that we can replace the old `chaos worker`.\\n\\nFurthermore, we plan to replace step by step the new worker, which calls the scripts, by workers which have the script logic inside. For example with workers written in go or kotlin. This should improve the adoption and maintainability further.\\n\\nFor simplicity and to make progress we modeled quite generic process models, which are feed with the chaos experiment DSL. In the future we can also think of modeling the chaos experiments directly as BPMN model.\\n\\n**Thanks to [Peter](https://github.com/pihme) for giving the impulse and his awesome work on `testbench` and [Philipp](https://github.com/saig0) which worked with me on this Hackday project.**"},{"id":"/2021/03/30/set-file-immutable","metadata":{"permalink":"/zeebe-chaos/2021/03/30/set-file-immutable","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-03-30-set-file-immutable/index.md","source":"@site/blog/2021-03-30-set-file-immutable/index.md","title":"Set file immutable","description":"This chaos day was a bit different. Actually I wanted to experiment again with camunda cloud and verify that our high load chaos experiments are now working with the newest cluster plans, see zeebe-cluster-testbench#135.","date":"2021-03-30T00:00:00.000Z","tags":[],"readingTime":6.5,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Set file immutable","date":"2021-03-30T00:00:00.000Z","categories":["chaos_experiment","filesystem","immutable"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"BPMN meets Chaos Engineering","permalink":"/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering"},"nextItem":{"title":"Camunda Cloud network partition","permalink":"/zeebe-chaos/2021/03/23/camunda-cloud-network-partition"}},"content":"This chaos day was a bit different. Actually I wanted to experiment again with camunda cloud and verify that our high load chaos experiments are now working with the newest cluster plans, see [zeebe-cluster-testbench#135](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/135). \\nUnfortunately I found out that our test chaos cluster was in a way broken, that we were not able to create new clusters. Luckily this was fixed at the end of the day, thanks to @immi :) \\n\\nBecause of these circumstances I thought about different things to experiment with, and I remembered that in the [last chaos day](/2021-03-23-camunda-cloud-network-partition/index.md) we worked with patching running deployments, in order to add more capabilities.\\nThis allowed us to create ip routes and experiment with the zeebe deployment distribution. During this I have read the [capabilities list of linux](https://man7.org/linux/man-pages/man7/capabilities.7.html), and found out that we can mark files as immutable, which might be interesting for a chaos experiment.\\n\\nIn this chaos day I planned to find out how marking a file immutable affects our brokers and I made the hypothesis that: *If a leader has a write error, which is not recoverable, it will step down and another leader should take over.* I put this in our hypothesis backlog ([zeebe-chaos#52](https://github.com/camunda/zeebe-chaos/issues/52)). \\n\\nIn order to really run this kind of experiment I need to find out whether marking a file immutable will cause any problems and if not how I can cause write errors such that affects the broker.\\nUnfortunately it turned out that immutable files will not cause issues on already opened file channels, but I found some other bugs/issues, which you can read below.\\n\\nIn the next chaos days I will search for a way to cause write errors proactively, so we can verify that our system can handle such issues.\\n\\n\x3c!--truncate--\x3e\\n\\n### Immutable File\\n\\nIn order to [mark a file as immutable](https://delightlylinux.wordpress.com/2012/12/11/file-immutable-attribute/) we can use the command `chattr +i`. For that we need a specific capability called `LINUX_IMMUTABLE`. Since we were at this time not able to create new clusters I tested it with the helm charts, where it is anyway easier to give us these capabilities.\\n\\nWe just need to add in our *values* files the following:\\n\\n```shell\\npodSecurityContext:\\n  capabilities:\\n        add: [ \\"LINUX_IMMUTABLE\\" ]\\n```\\n\\nSince we want to experiment with leaders I need to get the current topology from the gateway, here I found an issue on our latest zbctl build and the human output ([camunda-cloud/zeebe#6692](https://github.com/camunda-cloud/zeebe/issues/6692)). This is already fixed, thanks to [Miguel](https://github.com/MiguelPires) :rocket:!\\n\\nAfter finding the correct leader we can execute on the corresponding broker/pod following commands to mark a file as immutable.\\n\\n```shell\\n[zell ns:zell-chaos]$ k exec -it zell-chaos-zeebe-1 -- bash # opens an interactive shell on zell-chaos-zeebe-1\\nroot@zell-chaos-zeebe-1# cd data/raft-partition/partitions/1/\\nroot@zell-chaos-zeebe-1# chattr +i raft-partition-partition-1-1.log # marks the log file immutable\\nroot@zell-chaos-zeebe-1# lsattr raft-partition-partition-1-1.log\\n----i---------e---- raft-partition-partition-1-1.log\\n```\\n\\nWith the last command `lsattr` we print out the file attributes, where we see that the immutable flag is set (the `i` in the output).\\nAfter setting the log file to immutable nothing really happens. I checked again the manual page and read that this not affects already created file channels.\\n\\n```shell\\nCHATTR(1)                           General Commands Manual                          CHATTR(1)\\n\\nNAME\\n       chattr - change file attributes on a Linux file system\\n\\n      ...\\n      i      A  file with the \'i\' attribute cannot be modified: it cannot be deleted or renamed, no link can be created to this file, most of the file\'s metadata can not be modified, and the\\n              file can not be opened in write mode.  Only the superuser or a process possessing the CAP_LINUX_IMMUTABLE capability can set or clear this attribute.\\n      ...\\n      \\n      BUGS AND LIMITATIONS\\n       The \'c\', \'s\',  and \'u\' attributes are not honored by the ext2, ext3, and ext4 filesystems as implemented in the current mainline Linux kernels.  Setting \'a\' and \'i\' attributes will not\\n       affect the ability to write to already existing file descriptors.\\n```\\n\\nSo we actually are not able to use it to cause any write errors on a running leader, but interesting was what happened later when the pod was restarted. The pod was not able to restart, since the log was still immutable. We can see the following bootstrap sequence and the related errors:\\n\\n```shell\\nD 2021-03-30T09:11:02.433467Z Found segment: 1 (raft-partition-partition-2-1.log) \\nI 2021-03-30T09:11:02.499079Z RaftServer{raft-partition-partition-2} - Transitioning to FOLLOWER \\nI 2021-03-30T09:11:02.500847Z RaftPartitionServer{raft-partition-partition-1} - Starting server for partition PartitionId{id=1, group=raft-partition} \\nI 2021-03-30T09:11:02.506876Z RaftServer{raft-partition-partition-2} - Server join completed. Waiting for the server to be READY \\nE 2021-03-30T09:11:02.508400Z Bootstrap Broker-1 [6/13]: cluster services failed with unexpected exception. \\nI 2021-03-30T09:11:02.523239Z Closing Broker-1 [1/5]: subscription api \\nD 2021-03-30T09:11:02.525497Z Closing Broker-1 [1/5]: subscription api closed in 2 ms \\nI 2021-03-30T09:11:02.526484Z Closing Broker-1 [2/5]: command api handler \\nD 2021-03-30T09:11:02.528108Z Closing Broker-1 [2/5]: command api handler closed in 1 ms \\nI 2021-03-30T09:11:02.528740Z Closing Broker-1 [3/5]: command api transport \\nI 2021-03-30T09:11:03.519309Z RaftServer{raft-partition-partition-2} - Found leader 2 \\nI 2021-03-30T09:11:03.521376Z RaftServer{raft-partition-partition-2} - Setting firstCommitIndex to 2. RaftServer is ready only after it has committed events upto this index \\nI 2021-03-30T09:11:03.522206Z RaftPartitionServer{raft-partition-partition-2} - Successfully started server for partition PartitionId{id=2, group=raft-partition} in 1171ms \\nI 2021-03-30T09:11:04.553825Z Stopped \\nD 2021-03-30T09:11:04.555166Z Closing Broker-1 [3/5]: command api transport closed in 2026 ms \\nI 2021-03-30T09:11:04.556177Z Closing Broker-1 [4/5]: membership and replication protocol \\nI 2021-03-30T09:11:04.558282Z RaftServer{raft-partition-partition-2} - Transitioning to INACTIVE \\nE 2021-03-30T09:11:04.558408Z Closing Broker-1 [4/5]: membership and replication protocol failed to close. \\nI 2021-03-30T09:11:04.560776Z Closing Broker-1 [5/5]: actor scheduler \\nD 2021-03-30T09:11:04.561558Z Closing actor thread ground \'Broker-1-zb-fs-workers\' \\nD 2021-03-30T09:11:04.563600Z Closing segment: JournalSegment{id=1, version=1, index=1} \\nD 2021-03-30T09:11:04.563881Z Closing actor thread ground \'Broker-1-zb-fs-workers\': closed successfully \\nD 2021-03-30T09:11:04.564448Z Closing actor thread ground \'Broker-1-zb-actors\' \\nD 2021-03-30T09:11:04.566157Z Closing actor thread ground \'Broker-1-zb-actors\': closed successfully \\nD 2021-03-30T09:11:04.567716Z Closing Broker-1 [5/5]: actor scheduler closed in 6 ms \\nI 2021-03-30T09:11:04.568366Z Closing Broker-1 succeeded. Closed 5 steps in 2045 ms. \\nE 2021-03-30T09:11:04.568908Z Failed to start broker 1! \\nI 2021-03-30T09:11:04.574482Z \\n\\nError starting ApplicationContext. To display the conditions report re-run your application with \'debug\' enabled. \\nE 2021-03-30T09:11:04.595919Z Application run failed \\nI 2021-03-30T09:11:04.627321Z Shutting down ExecutorService \'applicationTaskExecutor\' \\n```\\n\\nThe following exception occurred on opening the log:\\n\\n```java\\njava.util.concurrent.CompletionException: io.zeebe.journal.JournalException: java.nio.file.FileSystemException: /usr/local/zeebe/data/raft-partition/partitions/1/raft-partition-partition-1-1.log: Operation not permitted\\n\\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]\\n\\tat java.lang.Thread.run(Unknown Source) ~[?:?]\\nCaused by: io.zeebe.journal.JournalException: java.nio.file.FileSystemException: /usr/local/zeebe/data/raft-partition/partitions/1/raft-partition-partition-1-1.log: Operation not permitted\\n\\tat io.zeebe.journal.file.SegmentedJournal.openChannel(SegmentedJournal.java:468) ~[zeebe-journal-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n\\tat io.zeebe.journal.file.SegmentedJournal.loadSegments(SegmentedJournal.java:490) ~[zeebe-journal-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n```\\n\\nAfter this exception on bootstrap the broker tries to close itself, and we see an error on closing a step `2021-03-30 11:11:04.558 CEST Closing Broker-1 [4/5]: membership and replication protocol failed to close.` This seems to be caused by a NPE.\\n```shell\\n\\njava.lang.NullPointerException: null\\n\\tat io.atomix.raft.partition.impl.RaftPartitionServer.stop(RaftPartitionServer.java:141) ~[atomix-cluster-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n\\tat io.atomix.raft.partition.RaftPartition.closeServer(RaftPartition.java:165) ~[atomix-cluster-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n\\tat io.atomix.raft.partition.RaftPartition.close(RaftPartition.java:155) ~[atomix-cluster-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]\\n```\\n\\nThe problem now here is that the broker never comes back. It is not restarted, which is a bit confusing. Furthermore it hasn\'t retried on opening, which is also unexpected, since it might be a temporary exception. \\n\\nWe can see in stackdriver that the new leader is not able to connect, which is expected. But we also see that the other Broker never comes back which is unexpected!\\n\\n```shell\\n\\nW 2021-03-30T09:11:05.006825Z RaftServer{raft-partition-partition-2} - AppendRequest{term=2, leader=2, prevLogIndex=2, prevLogTerm=2, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message \\nW 2021-03-30T09:11:05.256830Z RaftServer{raft-partition-partition-2} - AppendRequest{term=2, leader=2, prevLogIndex=2, prevLogTerm=2, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message \\nW 2021-03-30T09:11:08.450683Z RaftServer{raft-partition-partition-3} - AppendRequest{term=2, leader=2, prevLogIndex=1, prevLogTerm=1, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message \\nW 2021-03-30T09:11:12.897492Z RaftServer{raft-partition-partition-1} - AppendRequest{term=2, leader=2, prevLogIndex=1, prevLogTerm=1, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message \\nW 2021-03-30T09:11:25.950363Z RaftServer{raft-partition-partition-3} - AppendRequest{term=2, leader=2, prevLogIndex=1, prevLogTerm=1, entries=0, commitIndex=2} to 1 failed: java.util.concurrent.CompletionException: io.atomix.cluster.messaging.MessagingException$NoRemoteHandler: No remote message handler registered for this message\\n```\\n\\nWe need to investigate this issue [camunda-cloud/zeebe#6702](https://github.com/camunda-cloud/zeebe/issues/6702) further, and I need to find out how we can cause write errors proactively.\\n\\n#### Found Bugs\\n\\n * Int Console was not able to create new clusters in ultrachaos \\n * Zbctl human output looks broken [#6692](https://github.com/camunda-cloud/zeebe/issues/6692)\\n * Broker is not correctly shutdown [#6702](https://github.com/camunda-cloud/zeebe/issues/6702)"},{"id":"/2021/03/23/camunda-cloud-network-partition","metadata":{"permalink":"/zeebe-chaos/2021/03/23/camunda-cloud-network-partition","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-03-23-camunda-cloud-network-partition/index.md","source":"@site/blog/2021-03-23-camunda-cloud-network-partition/index.md","title":"Camunda Cloud network partition","description":"This time Deepthi was joining me on my regular Chaos Day.","date":"2021-03-23T00:00:00.000Z","tags":[],"readingTime":7.18,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Camunda Cloud network partition","date":"2021-03-23T00:00:00.000Z","categories":["chaos_experiment","camunda_cloud","network"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Set file immutable","permalink":"/zeebe-chaos/2021/03/30/set-file-immutable"},"nextItem":{"title":"Fault-tolerant processing of process instances","permalink":"/zeebe-chaos/2021/03/09/cont-workflow-instance"}},"content":"This time [Deepthi](https://github.com/deepthidevaki) was joining me on my regular Chaos Day. :tada:\\n\\n[In the second last chaos day](/2021-02-23-automate-deployments-dist/index.md) I created an automated chaos experiment, which verifies that the deployments are distributed after a network partition. Later it turned out that this doesn\'t work for camunda cloud, only for our helm setup. [The issue](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/237) was that on our camunda cloud zeebe clusters we had no [NET_ADMIN](https://man7.org/linux/man-pages/man7/capabilities.7.html) capability to create ip routes (used for the network partitions). After discussing with our SRE\'s they proposed a good way to overcome this. On running chaos experiments, which are network related, we will patch our target cluster to add this capability. This means we don\'t need to add such functionality in camunda cloud and the related zeebe operate/controller. Big thanks to [Immi](https://github.com/hisImminence) and [David](https://github.com/Faffnir) for providing this fix.\\n\\n\\n**TL;DR;**\\n\\nWe were able to enhance the deployment distribution experiment and run it in the camunda cloud via testbench. We have enabled the experiment for Production M and L cluster plans. We had to adjust the rights for the testbench service account to make this work.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe already had a [prepared chaos experiment](https://github.com/camunda/zeebe-chaos/blob/master/chaos-experiments/helm/deployment-distribution/experiment.json), but we needed to enhance that. Deepthi was so kind to create [PR](https://github.com/camunda/zeebe-chaos/pull/50) for that.\\n\\n### Enhancement\\nThe changes contain a new step before creating the network partition on the deployment distribution experiment, see [here](https://github.com/camunda/zeebe-chaos/blob/master/chaos-experiments/camunda-cloud/production-l/deployment-distribution/experiment.json#L25-L35).\\n\\n```json\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Enable net_admin capabilities\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"apply_net_admin.sh\\"\\n            },\\n            \\"pauses\\": {\\n                \\"after\\": 180\\n            }\\n        }\\n```\\n\\nThe `apply_net_admin.sh` contains the following code:\\n\\n```shell\\n\\n#!/bin/bash\\nset -euo pipefail\\n\\nscriptPath=$( cd \\"$(dirname \\"${BASH_SOURCE[0]}\\")\\" ; pwd -P )\\nsource utils.sh\\n\\nnamespace=$(getNamespace)\\n\\nCLUSTERID=${namespace%-zeebe}\\n\\nkubectl patch zb \\"$CLUSTERID\\" --type merge --patch=\'{\\"spec\\":{\\"controller\\":{\\"reconcileDisabled\\":true}}}\'\\nkubectl patch statefulset zeebe -n \\"$namespace\\" --patch \\"$(cat $scriptPath/net_admin_patch.yaml)\\"\\n```\\n\\nIt disables reconciliation for the target zeebe cluster and applies the following patch, which adds the `NET_ADMIN` capability:\\n\\n```yaml\\nspec:\\n  template:\\n    spec:\\n      containers:\\n        - name: \\"zeebe\\"\\n          securityContext:\\n            capabilities:\\n              add:\\n                - \\"NET_ADMIN\\"\\n```\\n\\nBig thanks to [Immi](https://github.com/hisImminence) and [David](https://github.com/Faffnir) for providing this fix.\\n\\nAfter we applied the patch, we need to make sure that the all pods are restarted and have the requested change. This is the reason we wait after the action for some minutes. This was the easiest way for us to think of, but ideally we find a better way here to make sure the patch was applied and we can continue.\\n\\n### Verification\\n\\nWe run this experiment with a Production L cluster (v1.0.0-alpha2) and it succeeded. This is quite nice, because this also contains already the rewritten [deployment distribution](https://github.com/camunda-cloud/zeebe/issues/6173) logic.\\n\\nTo verify whether the experiment really does something we checked the metrics and the logs. In the metrics we were not really able to tell that there was a network partition going on. There were no heart beats missing. The reason for that was that in the experiment the Leader for partition 3 (Node 1) and Leader for partition 1 (Node 0) have been disconnected. If we check the [partition distribution](https://github.com/camunda-cloud/zeebe/blob/develop/benchmarks/docs/scripts/partitionDistribution.sh) we can see that they have no partitions in common. \\n\\n```shell\\n$ ./partitionDistribution.sh 6 8 3\\nDistribution:\\nP\\\\N|\\tN 0|\\tN 1|\\tN 2|\\tN 3|\\tN 4|\\tN 5\\nP 0|\\tL  |\\tF  |\\tF  |\\t-  |\\t-  |\\t-  \\nP 1|\\t-  |\\tL  |\\tF  |\\tF  |\\t-  |\\t-  \\nP 2|\\t-  |\\t-  |\\tL  |\\tF  |\\tF  |\\t-  \\nP 3|\\t-  |\\t-  |\\t-  |\\tL  |\\tF  |\\tF  \\nP 4|\\tF  |\\t-  |\\t-  |\\t-  |\\tL  |\\tF  \\nP 5|\\tF  |\\tF  |\\t-  |\\t-  |\\t-  |\\tL  \\nP 6|\\tL  |\\tF  |\\tF  |\\t-  |\\t-  |\\t-  \\nP 7|\\t-  |\\tL  |\\tF  |\\tF  |\\t-  |\\t-  \\n\\nPartitions per Node:\\nN 0: 4\\nN 1: 5\\nN 2: 5\\nN 3: 4\\nN 4: 3\\nN 5: 3\\n```\\n\\nFortunately we saw in the logs that the Node 1, was retrying to send the deployments and at some point it succeeds.\\n```shell\\n2021-03-23 13:11:54.163 CET\\nFailed to receive deployment response for partitions [2, 4, 7, 8] (topic \'deployment-response-2251799813685304\'). Retrying\\n2021-03-23 13:11:54.164 CET\\nPushed deployment 2251799813685304 to all partitions.\\n2021-03-23 13:11:54.164 CET\\nDeployment CREATE command for deployment 2251799813685304 was written on partition 2\\n2021-03-23 13:11:54.164 CET\\nDeployment CREATE command for deployment 2251799813685304 was written on partition 4\\n2021-03-23 13:11:54.164 CET\\nDeployment CREATE command for deployment 2251799813685304 was written on partition 8\\n2021-03-23 13:11:54.165 CET\\nDeployment CREATE command for deployment 2251799813685304 was written on partition 7\\n2021-03-23 13:11:54.175 CET\\nDeployment 2251799813685304 distributed to all partitions successfully.\\n2021-03-23 13:11:54.763 CET\\nFailed to receive deployment response for partitions [2, 4, 7, 8] (topic \'deployment-response-2251799813685306\'). Retrying\\n2021-03-23 13:11:54.764 CET\\nPushed deployment 2251799813685306 to all partitions.\\n2021-03-23 13:11:54.764 CET\\nDeployment CREATE command for deployment 2251799813685306 was written on partition 4\\n2021-03-23 13:11:54.764 CET\\nDeployment CREATE command for deployment 2251799813685306 was written on partition 2\\n2021-03-23 13:11:54.765 CET\\nDeployment CREATE command for deployment 2251799813685306 was written on partition 8\\n2021-03-23 13:11:54.765 CET\\nDeployment CREATE command for deployment 2251799813685306 was written on partition 7\\n2021-03-23 13:11:54.773 CET\\nDeployment 2251799813685306 distributed to all partitions successfully.\\n```\\n\\n### Testbench\\n\\nAfter the experiment has succeeded and we had verified the execution we run this again on testbench.\\n\\nWe saw a non completing chaos worker after checking the chaostoolkit logs, we saw that it was still in the phase of disconnecting the nodes, which was the same issue as before [#237](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/237).\\n\\n```shell\\n[2021-03-23 12:40:24 INFO] [activity:161] Action: Enable net_admin capabilities\\n[2021-03-23 12:40:24 DEBUG] [process:53] Running: [\'/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/apply_net_admin.sh\']\\n[2021-03-23 12:40:24 WARNING] [process:66] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n[2021-03-23 12:40:24 DEBUG] [__init__:142] Data encoding detected as \'ascii\' with a confidence of 1.0\\n[2021-03-23 12:40:24 DEBUG] [activity:180]   => succeeded with \'{\'status\': 1, \'stdout\': \'\', \'stderr\': \'Error from server (Forbidden): zeebeclusters.cloud.camunda.io \\"cc108db7-768c-45cc-a6c5-098dc28f260c\\" is forbidden: User \\"system:serviceaccount:zeebe-chaos:zeebe-chaos-sa\\" cannot get resource \\"zeebeclusters\\" in API group \\"cloud.camunda.io\\" at the cluster scope\\\\n\'}\'\\n[2021-03-23 12:40:24 INFO] [activity:198] Pausing after activity for 180s...\\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on \'activity\'\\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on \'activity\'\\n[2021-03-23 12:43:24 INFO] [activity:161] Probe: All pods should be ready\\n[2021-03-23 12:43:24 DEBUG] [process:53] Running: [\'/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/verify-readiness.sh\']\\n[2021-03-23 12:43:24 DEBUG] [__init__:142] Data encoding detected as \'ascii\' with a confidence of 1.0\\n[2021-03-23 12:43:24 DEBUG] [__init__:142] Data encoding detected as \'ascii\' with a confidence of 1.0\\n[2021-03-23 12:43:24 DEBUG] [activity:180]   => succeeded with \'{\'status\': 0, \'stdout\': \'pod/zeebe-0 condition met\\\\npod/zeebe-1 condition met\\\\npod/zeebe-2 condition met\\\\npod/zeebe-3 condition met\\\\npod/zeebe-4 condition met\\\\npod/zeebe-5 condition met\\\\n\', \'stderr\': \\"+ source utils.sh\\\\n++ CHAOS_SETUP=cloud\\\\n++ getNamespace\\\\n+++ kubectl config view --minify --output \'jsonpath={..namespace}\'\\\\n++ namespace=cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\\\n++ echo cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\\\n+ namespace=cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\\\n+ \'[\' cloud == cloud \']\'\\\\n+ kubectl wait --for=condition=Ready pod -l app.kubernetes.io/app=zeebe --timeout=900s -n cc108db7-768c-45cc-a6c5-098dc28f260c-zeebe\\\\n\\"}\'\\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on \'activity\'\\n[2021-03-23 12:43:24 DEBUG] [__init__:355] No controls to apply on \'activity\'\\n[2021-03-23 12:43:24 INFO] [activity:161] Action: Create network partition between leaders\\n[2021-03-23 12:43:24 DEBUG] [process:53] Running: [\'/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/disconnect-leaders.sh\']\\n```\\n\\nIn the logs we also saw that the patch didn\'t worked as expected with the used service account, so we need to fix here the permission and after that it should hopefully work. :crossed_fingers:\\n\\n```shell\\n\'Error from server (Forbidden): zeebeclusters.cloud.camunda.io \\"XXX\\" is forbidden: User \\"system:serviceaccount:zeebe-chaos:zeebe-chaos-sa\\" cannot get resource \\"zeebeclusters\\" in API group \\"cloud.camunda.io\\" at the cluster scope\\\\n\'\\n```\\n\\nAfter checking with Immi, we were sure that we need to adjust the [serviceaccount roles](https://github.com/zeebe-io/zeebe-cluster-testbench/blob/develop/core/chaos-workers/deployment/service-account/zeebe-chaos-role.yaml#L9). After changing the apiGroups to `[\\"*\\"]` the experiments are running in testbench and the patch can be applied. We can now see in the log the following:\\n\\n```shell\\n[2021-03-23 14:21:24 DEBUG] [process:53] Running: [\'/home/chaos/zeebe-chaos/chaos-experiments/camunda-cloud/../scripts/apply_net_admin.sh\']\\n[2021-03-23 14:21:25 DEBUG] [__init__:142] Data encoding detected as \'ascii\' with a confidence of 1.0\\n[2021-03-23 14:21:25 DEBUG] [activity:180]   => succeeded with \'{\'status\': 0, \'stdout\': \'zeebecluster.cloud.camunda.io/cc108db7-768c-45cc-a6c5-098dc28f260c patched\\\\nstatefulset.apps/zeebe patched\\\\n\', \'stderr\': \'\'}\'\\n```\\n\\nThanks for participating [Deepthi](https://github.com/deepthidevaki).\\n\\n#### Found Bugs\\n\\n##### Re-connecting might fail\\n\\nWe realized during testing the experiment that the re-connecting might fail, because the pod can be rescheduled and then a ip route can\'t be delete since it no longer exist. [This is now fixed](https://github.com/camunda/zeebe-chaos/blob/master/chaos-experiments/scripts/connect-leaders.sh#L45-L48). We check for existence of the command `ip`, if this doesn\'t exist we know the pod was restarted and we ignore it.\\n\\n\\n*Before:*\\n\\n```shell\\nfunction connect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route del unreachable \\"$targetIp\\"\\n\\n}\\n```\\n\\n*After:*\\n\\n```shell\\n\\nfunction connect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n if command -v ip\\n then\\n     kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route del unreachable \\"$targetIp\\"\\n fi\\n}\\n```"},{"id":"/2021/03/09/cont-workflow-instance","metadata":{"permalink":"/zeebe-chaos/2021/03/09/cont-workflow-instance","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-03-09-cont-workflow-instance/index.md","source":"@site/blog/2021-03-09-cont-workflow-instance/index.md","title":"Fault-tolerant processing of process instances","description":"Today I wanted to add another chaos experiment, to increase our automated chaos experiments collection. This time we will deploy a process model (with timer start event), restart a node and complete the process instance via zbctl.","date":"2021-03-09T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":5.98,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Fault-tolerant processing of process instances","date":"2021-03-09T00:00:00.000Z","categories":["chaos_experiment","broker","processing"],"tags":["availability","data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Camunda Cloud network partition","permalink":"/zeebe-chaos/2021/03/23/camunda-cloud-network-partition"},"nextItem":{"title":"Automating Deployment Distribution Chaos Experiment","permalink":"/zeebe-chaos/2021/02/23/automate-deployments-dist"}},"content":"Today I wanted to add another chaos experiment, to increase our automated chaos experiments collection. This time we will deploy a process model (with timer start event), restart a node and complete the process instance via `zbctl`.\\n\\n**TL;DR;**\\n\\nI was able to create the chaos toolkit experiment. It shows us that we are able to restore our state after fail over, which means we can trigger timer start events to create process instances even if they have been deployed before fail-over. Plus we are able to complete these instances.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFor testing, I have run our normal setup of three nodes, three partitions and replication factor three in our zeebe gke cluster.\\nLater I want to automate the experiment against the production cluster plans.\\n\\n### Expected\\n\\nWe want to verify whether the processing of process instances continues even if we restart a leader in between. For that we do the following experiment:\\n\\n1. Verify Steady State: All Pods are ready\\n  2. Introduce Chaos:\\n      1. *Action:* Deploy process with timer start event (`PT1M`)\\n      2. *Action:* Restart leader of partition one\\n      3. *Probe:* We can activate and complete an job of a specific type\\n3. Verify Steady State: All Pods are ready\\n\\n**Note:** *We know that timer start events currently only scheduled on partition one, which means it is enough to just restart the leader of partition one for our experiment.* We use this property to reduce the blast radius. Later we could introduce an intermediate timer catch event and start many workflow instances on multiple partitions to verify whether this works on all partitions.\\n\\nModel will look like this:\\n\\n![model](chaosTimerStart.png))\\n\\n### Experiment Definition\\n\\nThe experiment definition looks like the following:\\n```json\\n{\\n    \\"version\\": \\"0.1.0\\",\\n    \\"title\\": \\"Zeebe process instance continuation\\",\\n    \\"description\\": \\"Zeebe processing of process instances should be fault-tolerant. Zeebe should be able to handle fail-overs and continue process instances after a new leader starts with processing.\\",\\n    \\"contributions\\": {\\n        \\"reliability\\": \\"high\\",\\n        \\"availability\\": \\"high\\"\\n    },\\n    \\"steady-state-hypothesis\\": {\\n        \\"title\\": \\"Zeebe is alive\\",\\n        \\"probes\\": [\\n            {\\n                \\"name\\": \\"All pods should be ready\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-readiness.sh\\",\\n                    \\"timeout\\": 900\\n                }\\n            }\\n        ]\\n    },\\n    \\"method\\": [\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Deploy process model\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"deploy-specific-model.sh\\", \\n                \\"arguments\\": [ \\"chaosTimerStart.bpmn\\" ]\\n            }\\n        },\\n         {\\n              \\"type\\": \\"action\\",\\n              \\"name\\": \\"Restart partition leader\\",\\n              \\"provider\\": {\\n                   \\"type\\": \\"process\\",\\n                   \\"path\\": \\"shutdown-gracefully-partition.sh\\",\\n                   \\"arguments\\": [ \\"Leader\\", \\"1\\" ]\\n              }\\n         },\\n        {\\n            \\"type\\": \\"probe\\",\\n            \\"name\\": \\"Complete process instance\\",\\n            \\"tolerance\\": 0,\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"complete-instance.sh\\",\\n                \\"arguments\\": [\\"chaos\\"],\\n                \\"timeout\\": 900\\n            }\\n        }\\n    ],\\n    \\"rollbacks\\": []\\n}\\n```\\n\\n#### Timer start events\\n\\nIn order to trigger the timer start event, after 1 minute, after successful deployment. I used the following [feel expression](https://docs.camunda.io/docs/reference/feel/what-is-feel): `=now()+ duration(\\"PT1M\\")`. This is necessary, since only date and cycle are supported for [timer start events](https://docs.camunda.io/docs/reference/bpmn-workflows/timer-events/timer-events/#timer-start-events).\\n\\n#### Continue process instance\\n\\nTo continue and finish the process instance, we will activate the job with the `chaos` job type and complete that job. If there is no job to activate/complete we will loop here until we reach the timeout. With this we can make sure that the timer was scheduled and the process instance was created even after restart. We are not using here an job worker, since the activate-complete commands make it currently easier to mark it as success or fail. With the job worker we would introduce another concurrency layer. \\n\\n```shell\\n#!/bin/bash\\nset -euox pipefail\\n\\nsource utils.sh\\n\\njobType=$1\\nnamespace=$(getNamespace)\\npod=$(getGateway)\\n\\nfunction completeJob() {\\n  # we want to first activate the job with the given job type and then complete it with the given job key\\n  # if we are not able to activate an job with the given type the function will return an error\\n  # and retried from outside\\n  jobs=$(kubectl exec -it \\"$pod\\" -n \\"$namespace\\" -- zbctl --insecure activate jobs \\"$jobType\\")\\n  key=$(echo \\"$jobs\\" | jq -r \'.jobs[0].key\')\\n  kubectl exec -it \\"$pod\\" -n \\"$namespace\\" -- zbctl --insecure complete job \\"$key\\"\\n}\\n\\nretryUntilSuccess completeJob\\n```\\n\\n### Outcome\\n\\nAfter running this experiment we get the following output, which shows us that the experiment **SUCCEEDED**.\\n\\n```shell\\n(chaostk) [zell helm/ cluster: zeebe-cluster ns:zell-chaos]$ chaos run process-continuation/experiment.json \\n[2021-03-09 13:16:34 INFO] Validating the experiment\'s syntax\\n[2021-03-09 13:16:34 INFO] Experiment looks valid\\n[2021-03-09 13:16:34 INFO] Running experiment: Zeebe process instance continuation\\n[2021-03-09 13:16:34 INFO] Steady-state strategy: default\\n[2021-03-09 13:16:34 INFO] Rollbacks strategy: default\\n[2021-03-09 13:16:34 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-03-09 13:16:34 INFO] Probe: All pods should be ready\\n[2021-03-09 13:16:35 INFO] Steady state hypothesis is met!\\n[2021-03-09 13:16:35 INFO] Playing your experiment\'s method now...\\n[2021-03-09 13:16:35 INFO] Action: Deploy process model\\n[2021-03-09 13:16:37 INFO] Action: Restart partition leader\\n[2021-03-09 13:16:46 INFO] Probe: Complete process instance\\n[2021-03-09 13:17:38 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-03-09 13:17:38 INFO] Probe: All pods should be ready\\n[2021-03-09 13:17:38 INFO] Steady state hypothesis is met!\\n[2021-03-09 13:17:38 INFO] Let\'s rollback...\\n[2021-03-09 13:17:38 INFO] No declared rollbacks, let\'s move on.\\n```\\n\\n#### Testbench\\n\\nTbd.\\n\\nWe have currently some problems with running the chaos experiments against camunda cloud, like [testbench#247](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/247) or [testbench#248](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/248). This is the reason why I postponed it.\\n\\n#### Found Bugs\\n\\n##### Chaos Experiments not working correctly\\nI realized that most of the experiments are no longer run correctly, since they referring to `\\"Leader\\"` as the raft role, where the raft role in the topology is `LEADER`. This causes that on some experiments pods are not really restarted.\\n\\nAlmost everywhere we use constructs like:\\n\\n```shell\\n\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Terminate leader of partition 3\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"shutdown-gracefully-partition.sh\\",\\n                \\"arguments\\": [ \\"Leader\\", \\"3\\" ]\\n            }\\n        }\\n```\\n\\nIn the utils script we use a `jq` expression to get the node which is in a certain state for a certain partition.\\n\\nThe `jq` expression looks like this:\\n```shell\\n  index=$(echo \\"$topology\\" | jq \\"[.brokers[]|select(.partitions[]| select(.partitionId == $partition) and .role == \\\\\\"$state\\\\\\")][0].nodeId\\")\\n```\\n`jq` is not able to find the raft `state` which is returned by topology, if we use not capital letters.\\n\\nWhen we run the chaos experiment we see warnings like:\\n\\n```shell\\n(chaostk) [zell helm/ cluster: zeebe-cluster ns:zell-chaos]$ chaos run process-continuation/experiment.json \\n[2021-03-09 13:27:01 INFO] Validating the experiment\'s syntax\\n[2021-03-09 13:27:01 INFO] Experiment looks valid\\n[2021-03-09 13:27:01 INFO] Running experiment: Zeebe process instance continuation\\n[2021-03-09 13:27:01 INFO] Steady-state strategy: default\\n[2021-03-09 13:27:01 INFO] Rollbacks strategy: default\\n[2021-03-09 13:27:01 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-03-09 13:27:01 INFO] Probe: All pods should be ready\\n[2021-03-09 13:27:02 INFO] Steady state hypothesis is met!\\n[2021-03-09 13:27:02 INFO] Playing your experiment\'s method now...\\n[2021-03-09 13:27:02 INFO] Action: Deploy process model\\n[2021-03-09 13:27:03 INFO] Action: Restart partition leader\\n[2021-03-09 13:27:05 WARNING] This process returned a non-zero exit code. This may indicate some error and not what you expected. Please have a look at the logs.\\n\\n```\\n\\nThe experiment still continues (in effect it does nothing, since it was not able to find the right pod to restart).\\nConverting the state into capital letters fixes this issue easily.\\n\\n```shell\\nfunction getIndexOfPodForPartitionInState()\\n{\\n  partition=\\"$1\\"\\n  # expect caps for raft roles\\n  state=${2^^}\\n```\\n\\n##### Redeployment causes retrigger timer\\n\\nDuring running the chaos experiment and testing the scripts I realized that the timer start event is retriggered everytime I redeployed the exact deployment, which was kind of unexpected.\\nI created a bug issue for that [zeebe#6515](https://github.com/camunda-cloud/zeebe/issues/6515)."},{"id":"/2021/02/23/automate-deployments-dist","metadata":{"permalink":"/zeebe-chaos/2021/02/23/automate-deployments-dist","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-02-23-automate-deployments-dist/index.md","source":"@site/blog/2021-02-23-automate-deployments-dist/index.md","title":"Automating Deployment Distribution Chaos Experiment","description":"This time I wanted to automate a chaos experiment via the ChaosToolkit, which I did on the last chaos day. For a recap check out the last chaos day summary.","date":"2021-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"tests","permalink":"/zeebe-chaos/tags/tests"}],"readingTime":6.855,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Automating Deployment Distribution Chaos Experiment","date":"2021-02-23T00:00:00.000Z","categories":["chaos_experiment","broker","network","deployment"],"tags":["tests"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Fault-tolerant processing of process instances","permalink":"/zeebe-chaos/2021/03/09/cont-workflow-instance"},"nextItem":{"title":"Deployment Distribution","permalink":"/zeebe-chaos/2021/01/26/deployments"}},"content":"This time I wanted to automate a chaos experiment via the [ChaosToolkit](https://chaostoolkit.org/), which I did on the last chaos day. For a recap check out the [last chaos day summary](/2021-01-26-deployments/index.md).\\n\\n**TL;DR;**\\n\\nI was able to automate the deployment distribution chaos experiment successfully and deployed it on testbench for a `Production - M` cluster plan.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nFor testing, I have run our normal setup of three nodes, three partitions and replication factor three.\\nLater I wanted to automate the experiment against the production cluster plans.\\n\\n### Expected\\n\\nWe want to verify whether deployments are still distributed after a network partition, for that we will write the following chaos experiment:\\n\\n1. Verify Steady State: All Pods are ready\\n  2. Introduce Chaos:\\n      1. *Action:* Disconnect two leaders (Leader of partition one and another leader)\\n      2. *Action:* Deploy multiple versions of a workflow\\n      3. *Action:* Connect two leaders again\\n      4. *Probe*: I can create workflow instance on all partitions with the last workflow version\\n3. Verify Steady State: All Pods are ready\\n\\n### Experiment Definition\\n\\nThe experiment definition looks like the following:\\n```json\\n{\\n    \\"version\\": \\"0.1.0\\",\\n    \\"title\\": \\"Zeebe deployment distribution\\",\\n    \\"description\\": \\"Zeebe deployment distribution should be fault-tolerant. Zeebe should be able to handle network outages and fail-overs and distribute the deployments after partitions are available again.\\",\\n    \\"contributions\\": {\\n        \\"reliability\\": \\"high\\",\\n        \\"availability\\": \\"high\\"\\n    },\\n    \\"steady-state-hypothesis\\": {\\n        \\"title\\": \\"Zeebe is alive\\",\\n        \\"probes\\": [\\n            {\\n                \\"name\\": \\"All pods should be ready\\",\\n                \\"type\\": \\"probe\\",\\n                \\"tolerance\\": 0,\\n                \\"provider\\": {\\n                    \\"type\\": \\"process\\",\\n                    \\"path\\": \\"verify-readiness.sh\\",\\n                    \\"timeout\\": 900\\n                }\\n            }\\n        ]\\n    },\\n    \\"method\\": [\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Create network partition between leaders\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"disconnect-leaders.sh\\"\\n            }\\n        },\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Deploy different deployment versions.\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"deploy-different-versions.sh\\",\\n                \\"arguments\\": [\\"Follower\\", \\"3\\"]\\n            }\\n        },\\n        {\\n            \\"type\\": \\"action\\",\\n            \\"name\\": \\"Delete network partition\\",\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"connect-leaders.sh\\"\\n            }\\n        },\\n        {\\n            \\"type\\": \\"probe\\",\\n            \\"name\\": \\"Create workflow instance of latest version on partition one\\",\\n            \\"tolerance\\": 0,\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"start-instance-on-partition-with-version.sh\\",\\n                \\"arguments\\": [\\"1\\", \\"10\\"],\\n                \\"timeout\\": 900\\n            }\\n        },\\n        {\\n            \\"type\\": \\"probe\\",\\n            \\"name\\": \\"Create workflow instance of latest version on partition two\\",\\n            \\"tolerance\\": 0,\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"start-instance-on-partition-with-version.sh\\",\\n                \\"arguments\\": [\\"2\\", \\"10\\"],\\n                \\"timeout\\": 900\\n            }\\n        },\\n        {\\n            \\"type\\": \\"probe\\",\\n            \\"name\\": \\"Create workflow instance of latest version on partition three\\",\\n            \\"tolerance\\": 0,\\n            \\"provider\\": {\\n                \\"type\\": \\"process\\",\\n                \\"path\\": \\"start-instance-on-partition-with-version.sh\\",\\n                \\"arguments\\": [\\"3\\", \\"10\\"],\\n                \\"timeout\\": 900\\n            }\\n        }\\n    ],\\n    \\"rollbacks\\": []\\n}\\n```\\n\\n\\n#### Create network partition between leaders\\n\\nWe reuse a script which we introduce in earlier chaos days. It needed to be improved a bit, since we haven\'t handled clusters where one node leads multiple partitions.\\n\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\ngateway=$(getGateway)\\n\\n# determine leader for partition one\\nindex=$(getIndexOfPodForPartitionInState \\"$partition\\" \\"LEADER\\")\\nleader=$(getBroker \\"$index\\")\\nleaderIp=$(kubectl get pod \\"$leader\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n# determine leader for partition three\\nindex=$(getIndexOfPodForPartitionInState \\"3\\" \\"LEADER\\")\\nleaderTwo=$(getBroker \\"$index\\")\\nleaderTwoIp=$(kubectl get pod \\"$leaderTwo\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\nif [ \\"$leaderTwo\\" == \\"$leader\\" ]\\nthen\\n  # determine leader for partition two\\n  index=$(getIndexOfPodForPartitionInState \\"2\\" \\"LEADER\\")\\n  leaderTwo=$(getBroker \\"$index\\")\\n  leaderTwoIp=$(kubectl get pod \\"$leaderTwo\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n  if [ \\"$leaderTwo\\" == \\"$leader\\" ]\\n  then\\n    # We could try to kill the pod and hope that he is not able to become leader again,\\n    # but there is a high chance that it is able to do so after restart. It can make our test fragile,\\n    # especially if we want to connect again, which is the reason why we do nothing in that case.\\n    exit\\n  fi\\nfi\\n\\n# To print the topology in the journal\\nretryUntilSuccess kubectl exec \\"$gateway\\" -n \\"$namespace\\" -- zbctl status --insecure\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$leader\\" \\"$leaderTwoIp\\"\\nretryUntilSuccess disconnect \\"$leaderTwo\\" \\"$leaderIp\\" \\n```\\n\\n\\n#### Delete network partition\\n\\nLooks similar to the disconnect script.\\n\\n##### Deploy different deployment versions\\n\\nThis script is interesting. My first approach was to have one workflow model, where I replace an comment via `sed` before redeploying. Later I realized it is much easier to just have two versions of the same worfklow model in the repository and deploy them in an alternating manner. This reduced the dependecy of an extra tool (`sed`), which might not be available everywhere or work differently on different linux distributions. \\n\\n```shell\\n#!/bin/bash\\n\\nset -exuo pipefail\\n\\nscriptPath=$( cd \\"$(dirname \\"${BASH_SOURCE[0]}\\")\\" ; pwd -P )\\nsource utils.sh\\n\\nnamespace=$(getNamespace)\\npod=$(getGateway)\\n\\nbpmnPath=\\"$scriptPath/../bpmn/\\"\\n\\n# we put both together in one function to retry both, because it might be that pod has been restarted\\n# then the model is not longer on the node, which cause endless retries of deployments\\nfunction deployModel() {\\n  kubectl cp \\"$bpmnPath\\" \\"$pod:/tmp/\\" -n \\"$namespace\\"\\n\\n  for i in {1..5}\\n  do\\n    # the models differ in one line, the share the same name and process id\\n    # if we deploy them after another it will create two different deployment versions\\n    # the deploy command only compares the last applied deployment - so we can do that in a loop to cause\\n    # multiple deployments\\n    kubectl exec \\"$pod\\" -n \\"$namespace\\" -- sh -c \\"zbctl deploy /tmp/bpmn/multi-version/multiVersionModel.bpmn --insecure\\"\\n    kubectl exec \\"$pod\\" -n \\"$namespace\\" -- sh -c \\"zbctl deploy /tmp/bpmn/multi-version/multiVersionModel_v2.bpmn --insecure\\"\\n  done\\n}\\n\\nretryUntilSuccess deployModel\\n```\\n\\nWhen running this script we deploy `10` new versions of the workflow `multiVersion`.\\n\\n#### Create workflow instance of latest version on partition X\\n\\nThe following script allows us to be sure that we can create a workflow instance on a specific partition with the given version of the `multiVersion` workflow.\\nThis means we can verify that on all partitions the last deployment version is deployed/distributed.\\n\\n```shell\\n#!/bin/bash\\nset -xo pipefail\\n\\nif [ -z \\"$1\\" ]\\nthen\\n  echo \\"Please provide an required partition!\\"\\n  exit 1\\nfi\\n\\nif [ -z \\"$2\\" ]\\nthen\\n  echo \\"Please provide an required deployment version!\\"\\n  exit 1\\nfi\\n\\nsource utils.sh\\n\\nnamespace=$(getNamespace)\\npod=$(getGateway)\\n\\nrequiredPartition=$1\\nrequiredDeploymentVersion=$2\\nprocessId=\\"multiVersion\\"\\n\\n# we put all into one function because we need to make sure that even after preemption the\\n# dependency are installed, which is in this case is the deployment\\nfunction startInstancesOnPartition() {\\n\\n  partition=0\\n  until [[ \\"$partition\\" -eq \\"$requiredPartition\\" ]]; do\\n    workflowInstanceKey=$(kubectl exec \\"$pod\\" -n \\"$namespace\\" -- zbctl create instance \\"$processId\\" --version \\"$requiredDeploymentVersion\\" --insecure)\\n    workflowInstanceKey=$(echo \\"$workflowInstanceKey\\" | jq \'.workflowInstanceKey\')\\n    partition=$(( workflowInstanceKey >> 51 ))\\n    echo \\"Started workflow with key $workflowInstanceKey, corresponding partition $partition\\"\\n  done\\n}\\n\\nretryUntilSuccess startInstancesOnPartition\\n```\\n\\n### Outcome\\n\\nAfter running this experiment we get the following output, which shows us that the experiment **SUCCEEDED**.\\n\\n```shell\\n(chaostk) [zell camunda-cloud/ cluster: zeebe-cluster ns:zell-chaos]$ chaos run production-m/deployment-distribution/experiment.json \\n[2021-02-23 14:15:06 INFO] Validating the experiment\'s syntax\\n[2021-02-23 14:15:06 INFO] Experiment looks valid\\n[2021-02-23 14:15:06 INFO] Running experiment: Zeebe deployment distribution\\n[2021-02-23 14:15:06 INFO] Steady-state strategy: default\\n[2021-02-23 14:15:06 INFO] Rollbacks strategy: default\\n[2021-02-23 14:15:06 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-02-23 14:15:06 INFO] Probe: All pods should be ready\\n[2021-02-23 14:15:07 INFO] Probe: Should be able to create workflow instances on partition 3\\n[2021-02-23 14:15:10 INFO] Steady state hypothesis is met!\\n[2021-02-23 14:15:10 INFO] Playing your experiment\'s method now...\\n[2021-02-23 14:15:10 INFO] Action: Create network partition between leaders\\n[2021-02-23 14:15:26 INFO] Action: Deploy thousand different deployment versions.\\n[2021-02-23 14:15:31 INFO] Action: Delete network partition\\n[2021-02-23 14:15:43 INFO] Probe: Create workflow instance of latest version on partition one\\n[2021-02-23 14:15:43 INFO] Probe: Create workflow instance of latest version on partition two\\n[2021-02-23 14:15:51 INFO] Probe: Create workflow instance of latest version on partition three\\n[2021-02-23 14:15:52 INFO] Steady state hypothesis: Zeebe is alive\\n[2021-02-23 14:15:52 INFO] Probe: All pods should be ready\\n[2021-02-23 14:15:52 INFO] Probe: Should be able to create workflow instances on partition 3\\n[2021-02-23 14:15:55 INFO] Steady state hypothesis is met!\\n[2021-02-23 14:15:55 INFO] Let\'s rollback...\\n[2021-02-23 14:15:55 INFO] No declared rollbacks, let\'s move on.\\n[2021-02-23 14:15:55 INFO] Experiment ended with status: completed\\n```\\n\\n#### Testbench\\n\\nI executed the new experiment via zeebe testbench, to verify that this works with the `Production - M` cluster plan and it was successful :muscle:\\n\\n![operate](operate-success.png)\\n\\n```json\\n{\\"results\\":[\\"production-m/deployment-distribution/experiment.json completed successfully\\",\\"production-m/follower-restart/experiment.json completed successfully\\",\\"production-m/follower-terminate/experiment.json completed successfully\\",\\"production-m/leader-restart/experiment.json completed successfully\\",\\"production-m/leader-terminate/experiment.json completed successfully\\",\\"production-m/msg-correlation/experiment.json completed successfully\\",\\"production-m/multiple-leader-restart/experiment.json completed successfully\\",\\"production-m/snapshot-corruption/experiment.json completed successfully\\",\\"production-m/stress-cpu-on-broker/experiment.json completed successfully\\",\\"production-m/worker-restart/experiment.json completed successfully\\"]}\\n```"},{"id":"/2021/01/26/deployments","metadata":{"permalink":"/zeebe-chaos/2021/01/26/deployments","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-01-26-deployments/index.md","source":"@site/blog/2021-01-26-deployments/index.md","title":"Deployment Distribution","description":"On this chaos day we wanted to experiment a bit with deployment\'s and there distribution.","date":"2021-01-26T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"data","permalink":"/zeebe-chaos/tags/data"}],"readingTime":10.855,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Deployment Distribution","date":"2021-01-26T00:00:00.000Z","categories":["chaos_experiment","broker","network"],"tags":["availability","data"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Automating Deployment Distribution Chaos Experiment","permalink":"/zeebe-chaos/2021/02/23/automate-deployments-dist"},"nextItem":{"title":"Network partitions","permalink":"/zeebe-chaos/2021/01/19/network-partition"}},"content":"On this chaos day we wanted to experiment a bit with deployment\'s and there distribution.\\n\\nWe run a chaos experiment with deploying multiple workflows and disconnecting two leaders. We verified whether deployments are distributed later. The chaos experiment was successful and showed a bit how fault tolerant deployment distribution is. :muscle:\\n\\n\x3c!--truncate--\x3e\\n\\n## Deployments\\n\\nIn order to continue here we need to explain how the workflow deployment and distribution actually works, if you know it then you can skip this section :wink:.\\n\\n### Deployment Distribution\\n\\nIf you deploy a workflow and you have multiple partitions, Zeebe needs to make sure that all partitions eventually have the same version of the deployment. This is done with via the deployment distribution.\\n\\n![distribution](distribution.png)\\n\\nOn deploying a workflow via a client, like the java client, we send a deployment command to the gateway. The gateway sends the received deployment to the \\"deployment partition\\", which is partition one. The partition one is in charge of distributing the deployment. When the client receives a response for the deployment command, this means that the deployment is written/created on partition one. It doesn\'t mean that it is distributed to all other partitions. The distribution is done asynchronously. \\n\\nThis can cause issues, if you want to create workflow instances immediately after the deployment. If you try to create a workflow instance on a partition which hasn\'t received the deployment yet, then this creation will fail. The gateway sends commands, like workflow instance creation, in a round-robin fashion and if you have multiple partitions, then the chance that you hit another partition is quite high.\\n\\n#### Reasoning\\n\\nYou may ask why we build it like that. Let me explain this a bit more.\\n\\n##### Why isn\'t the gateway in charge of distributing the deployment?\\n\\nBecause the gateway is stateless. If the gateway restarts it has no state it can replay, so it is not able to retry the deployment distributions. If the gateway failed during deployment distribution some partition might lose the deployment update. In the broker we replay the state, which means we can detect whether we distributed the deployment already to a certain partition if not we can retry it.\\n\\n##### Why the response isn\'t send after the distribution is done. Why it is build in an asynchronous way?\\n\\nThe simple answer would be, because it can take a long time until the deployment is distributed to all partitions. In a distributed system it is likely that a service fail, which means if one partition is not available the distribution is not complete. With the current approach you can already start creating instances at least at partition one and you can retry the requests if you get an rejection.\\n\\nAfter the small excursion of how deployment distribution look like and why, we can start with our experiment to verify that it works as expected.\\n\\n## Chaos Experiment\\n\\nWe have a standard setup of three nodes, three partitions and replication factor three.\\n\\n### Expected\\n\\nWe deploy multiple versions (1000+) of a deployment and assume that at some point all deployments are distributed on all partitions and that we are able to create a workflow instance with the latest version on all partitions. The system should remain stable during distributing the deployments. This can be seen as the steady state.\\n\\nIf we now disconnect a leader of a different partition (different from partition one) with the leader of partition one, then the deployments can\'t be distributed. If we try to create workflow instances on that partition we should receive rejection\'s. After we connect them again the deployments should be distributed and we should be able to create workflow instances on that specific partition.\\n\\n### Actual\\n\\n#### Steady State\\n\\nFollowing Java code was used to verify the steady state. In order to find out on which partition the workflow instances was created I used the `Protocol#decodePartitionId` method, which is available in the zeebe protocol module. This functionality and the property of the key\'s was already quite useful in the past on doing chaos experiments.\\n\\n\\n```java\\nimport io.zeebe.client.ZeebeClient;\\nimport io.zeebe.model.bpmn.Bpmn;\\nimport io.zeebe.protocol.Protocol;\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\n\\npublic class ChaosDayMain {\\n\\n\\n  private static final Logger LOG = LoggerFactory.getLogger(ChaosDayMain.class.getName());\\n\\n  public static void main(String[] args) {\\n    final var zeebeClient = ZeebeClient.newClientBuilder().usePlaintext().gatewayAddress(\\"localhost:26500\\").build();\\n\\n    final var topology = zeebeClient.newTopologyRequest().send().join();\\n    LOG.info(\\"{}\\", topology);\\n\\n    var lastVersion = 0;\\n    for (int i = 0; i < 1_000; i++) {\\n\\n      final var modelInstance = Bpmn.createExecutableProcess(\\"workflow\\").startEvent().endEvent().done();\\n\\n      final var workflow = zeebeClient.newDeployCommand()\\n          .addWorkflowModel(modelInstance, \\"workflow\\").send().join();\\n      lastVersion = workflow.getWorkflows().get(0).getVersion();\\n    }\\n    LOG.info(\\"Last version deployed: {}\\", lastVersion);\\n\\n    final var partitions = new ArrayList<>(List.of(1, 2, 3));\\n\\n    while (!partitions.isEmpty()) {\\n      try {\\n        final var workflowInstanceEvent = zeebeClient.newCreateInstanceCommand()\\n            .bpmnProcessId(\\"workflow\\")\\n            .version(lastVersion).send().join();\\n        final var workflowInstanceKey = workflowInstanceEvent.getWorkflowInstanceKey();\\n        final var partitionId = Protocol.decodePartitionId(workflowInstanceKey);\\n\\n        partitions.remove(Integer.valueOf(partitionId));\\n        LOG.info(\\"Created workflow instance on partition {}, {} partitions left ({}).\\", partitionId, partitions.size(), partitions);\\n      } catch (Exception e) {\\n        // retry\\n        LOG.info(\\"Failed to create workflow instance\\", e);\\n      }\\n\\n    }\\n  }\\n}\\n```\\n\\n**Small Note** the line: `Bpmn.createExecutableProcess(\\"workflow\\").startEvent().endEvent().done()` will always create a new version of a workflow, since internally new id\'s are generated.\\n\\nAfter running the code above we can see following output:\\n\\n```\\n13:45:00.606 [] [main] INFO  ChaosDayMain - TopologyImpl{brokers=[BrokerInfoImpl{nodeId=0, host=\'zell-chaos-zeebe-0.zell-chaos-zeebe.zell-chaos.svc.cluster.local\', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=LEADER, health=HEALTHY}]}, BrokerInfoImpl{nodeId=2, host=\'zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc.cluster.local\', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=LEADER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=LEADER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=FOLLOWER, health=HEALTHY}]}, BrokerInfoImpl{nodeId=1, host=\'zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc.cluster.local\', port=26501, version=0.27.0-SNAPSHOT, partitions=[PartitionInfoImpl{partitionId=1, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=2, role=FOLLOWER, health=HEALTHY}, PartitionInfoImpl{partitionId=3, role=FOLLOWER, health=HEALTHY}]}], clusterSize=3, partitionsCount=3, replicationFactor=3, gatewayVersion=0.27.0-SNAPSHOT}\\n13:46:04.384 [] [main] INFO  ChaosDayMain - Last version deployed: 6914\\n13:46:04.434 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 2 partitions left ([2, 3]).\\n13:46:04.505 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n13:46:04.571 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 3, 0 partitions left ([]).\\n```\\n\\nAs you can see in the version count I run it already multiple times :). With this we are able to verify our steady state, that the deployments are distributed to all partitions and that we are able to create workflow instances with the specific (last) version on all partitions.\\n\\n*Side note, I needed multiple runs because there was a leader change (of partition one) in between and I had to adjust the code etc. Needs to be investigated whether the deployment distribution caused that.*\\n\\n#### Chaos Injection (Method)\\n\\nThe following topology we used to determined who to disconnect:\\n\\n```yaml\\nCluster size: 3\\nPartitions count: 3\\nReplication factor: 3\\nGateway version: 0.27.0-SNAPSHOT\\nBrokers:\\n  Broker 0 - zell-chaos-zeebe-0.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\\n    Version: 0.27.0-SNAPSHOT\\n    Partition 1 : Leader, Healthy\\n    Partition 2 : Leader, Healthy\\n    Partition 3 : Follower, Healthy\\n  Broker 1 - zell-chaos-zeebe-1.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\\n    Version: 0.27.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy\\n    Partition 2 : Follower, Healthy\\n    Partition 3 : Follower, Healthy\\n  Broker 2 - zell-chaos-zeebe-2.zell-chaos-zeebe.zell-chaos.svc.cluster.local:26501\\n    Version: 0.27.0-SNAPSHOT\\n    Partition 1 : Follower, Healthy\\n    Partition 2 : Follower, Healthy\\n    Partition 3 : Leader, Healthy\\n```\\n\\nBased on the work of the last chaos days we are able to disconnect brokers easily.\\n```sh\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\ngateway=$(getGateway)\\n\\n# determine leader for partition one\\nindex=$(getIndexOfPodForPartitionInState \\"$partition\\" \\"LEADER\\")\\nleader=$(getBroker \\"$index\\")\\nleaderIp=$(kubectl get pod \\"$leader\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n# determine leader for partition three\\n\\nindex=$(getIndexOfPodForPartitionInState \\"3\\" \\"FOLLOWER\\")\\nleaderTwo=$(getBroker \\"$index\\")\\nleaderTwoIp=$(kubectl get pod \\"$leaderTwo\\" -n \\"$namespace\\" --template=\\"{{.status.podIP}}\\")\\n\\n# To print the topology in the journal\\nretryUntilSuccess kubectl exec \\"$gateway\\" -n \\"$namespace\\" -- zbctl status --insecure\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$leader\\" \\"$leaderTwoIp\\"\\nretryUntilSuccess disconnect \\"$leaderTwo\\" \\"$leaderIp\\" \\n\\n```\\n\\nWe used partition three here, since the leader of partition one and two are the same node. After disconnecting and running the Java code from above, we get the following output:\\n\\n```\\n14:11:56.655 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\\n14:11:56.713 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n14:11:56.777 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\\nio.zeebe.client.api.command.ClientStatusException: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\t... 1 more\\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\\n\\tat java.lang.Thread.run(Thread.java:834) ~[?:?]\\n14:11:56.846 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\\n14:11:56.907 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n14:11:56.971 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\\nio.zeebe.client.api.command.ClientStatusException: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\t... 1 more\\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Command \'CREATE\' rejected with code \'NOT_FOUND\': Expected to find workflow definition with process ID \'workflow\' and version \'8916\', but none found\\n\\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\\n\\tat java.lang.Thread.run(Thread.java:834) ~[?:?]\\n```\\nWhich is of course expected, since the deployment is not available at the third partition.\\n\\nIn the stackdriver we see also warnings regarding the deployment distribution:\\n```\\n2021-01-26 14:11:49.439 CET\\nFailed to push deployment to node 2 for partition 3\\n2021-01-26 14:11:49.439 CET\\nFailed to push deployment to node 2 for partition 3\\n2021-01-26 14:11:49.439 CET\\nFailed to push deployment to node 2 for partition 3\\n```\\n\\nAfter we connected the leaders again (deleting the ip route), we can see in the application log that the exception changed to deadline exceeded and at some point we are able to create a workflow instance on partition three.\\n\\n```\\n14:16:21.958 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\\n14:16:22.020 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n14:16:32.032 [] [main] INFO  ChaosDayMain - Failed to create workflow instance\\nio.zeebe.client.api.command.ClientStatusException: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\tat ChaosDayMain.main(ChaosDayMain.java:37) [classes/:?]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\\n\\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[zeebe-client-java-0.26.0.jar:0.26.0]\\n\\t... 1 more\\nCaused by: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 9.999911981s. [remote_addr=localhost/127.0.0.1:26500]\\n\\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[grpc-api-1.34.0.jar:1.34.0]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:478) ~[grpc-stub-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:617) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:803) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:782) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[grpc-core-1.34.0.jar:1.34.0]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\\n\\tat java.lang.Thread.run(Thread.java:834) ~[?:?]\\n14:16:32.062 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 1, 1 partitions left ([3]).\\n14:16:32.125 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 2, 1 partitions left ([3]).\\n14:16:37.596 [] [main] INFO  ChaosDayMain - Created workflow instance on partition 3, 0 partitions left ([]).\\n```\\n\\nWe see the deadline exceeded, because the processor is not able to send the response\'s in time. The reason for that is probably because we have so many deployments to process, which have been pushed by the partition one.\\n\\nThe resource consumption around that time look ok. We can see that the memory spikes a bit, but it recovers later. On the CPU graph we can see when we connected the nodes again.\\n\\n![res](res.png))\\n\\n### Result\\n\\nThe chaos experiment was successful. The deployment was distributed even after a network disconnect and we were able to create workflow instance of the latest version on all partitions at the end.\\n\\nWith this experiment we were able to show that the deployment distribution is fault tolerant in way that it can handle unavailability of other partitions. This means eventually all partitions will receive there deployment\'s and we are able to create workflow instances on these partitions. \\n\\n#### Further work\\n\\nFurther possible experiments would be to restart the leader of partition one to see that even after restart we re-distribute the deployments. It is probably also interesting to see how the distribution behaves on more partitions than three.\\n\\nDuring the experiment we have observed some leader changes. It needs to be investigated further, whether this was related to the deployments or something different. It is probably also interesting to see how it behaves with larger deployments, also resource consumption wise."},{"id":"/2021/01/19/network-partition","metadata":{"permalink":"/zeebe-chaos/2021/01/19/network-partition","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-01-19-network-partition/index.md","source":"@site/blog/2021-01-19-network-partition/index.md","title":"Network partitions","description":"As you can see, I migrated the old chaos day summaries to github pages, for better readability.","date":"2021-01-19T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":7.03,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Network partitions","date":"2021-01-19T00:00:00.000Z","categories":["chaos_experiment","broker","network"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Deployment Distribution","permalink":"/zeebe-chaos/2021/01/26/deployments"},"nextItem":{"title":"Disconnect Leader and one Follower","permalink":"/zeebe-chaos/2021/01/07/disconnect-leader-and-follower"}},"content":"As you can see, I migrated the old chaos day summaries to github pages, for better readability. \\nI always wanted to play around with github pages and jekyll so this was a good opportunity. I hope you like it. :smile:\\n\\nOn the last Chaos Day, we experimented with disconnecting a Leader and *one* follower. We expected no bigger disturbance, since we still have quorum and can process records. Today I want to experiment with bigger network partitions.\\n\\n * In the first chaos experiment: I had a cluster of 5 nodes and split that into two groups, the processing continued as expected, since we had still quorum. :muscle:\\n * In the second chaos experiment: I split the cluster again into two groups, but this time we added one follower of the bigger group to the smaller group after snapshot was taken and compaction was done. The smaller group needed to keep up with the newer state, before new processing can be started again, but everything worked fine.\\n\\n\x3c!--truncate--\x3e\\n\\n## First Chaos Experiment\\n\\nSay we have cluster of 5 nodes, one partition with replication factor 3 and we split the cluster in two parts (2 nodes and 3 nodes).\\n\\n### Expected\\n\\nWe expect if we partition two followers away that one part of the cluster can still continue, since it has quorum. Quorum is defined as `quorum=floor(nodes/2) + 1`\\n\\n### Actual\\n\\n![general](general.png)\\n\\nWhen partitioning two followers, this means we would have two groups. First group would be Broker-0 and Broker-1, the second group contains then Broker-2, Broker-3 and Broker-4. I adjusted the disconnect script from the last chaos day a bit. It looks now like this:\\n\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\n# this scripts expects a setup of 5 nodes with replication factor 5 or higher\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\ngateway=$(getGateway)\\n\\nbroker0=$(getBroker \\"0\\")\\nbroker0Ip=$(kubectl get pod \\"$broker0\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\nbroker1=$(getBroker \\"1\\")\\nbroker1Ip=$(kubectl get pod \\"$broker1\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\nbroker2=$(getBroker \\"2\\")\\nbroker2Ip=$(kubectl get pod \\"$broker2\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\nbroker3=$(getBroker \\"3\\")\\nbroker3Ip=$(kubectl get pod \\"$broker3\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\nbroker4=$(getBroker \\"4\\")\\nbroker4Ip=$(kubectl get pod \\"$broker4\\" -n \\"$namespace\\" --template=\\"{ { .status.podIP } }\\")\\n\\n# To print the topology in the journal\\nretryUntilSuccess kubectl exec \\"$gateway\\" -n \\"$namespace\\" -- zbctl status --insecure\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\n# Broker 0 and 1 is one group\\n\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$broker2Ip\\"\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$broker3Ip\\"\\nretryUntilSuccess disconnect \\"$broker0\\" \\"$broker4Ip\\"\\n\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$broker2Ip\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$broker3Ip\\"\\nretryUntilSuccess disconnect \\"$broker1\\" \\"$broker4Ip\\"\\n\\n# Broker 2, 3 and 4 is the other group\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$broker0Ip\\"\\nretryUntilSuccess disconnect \\"$broker2\\" \\"$broker1Ip\\"\\n\\nretryUntilSuccess disconnect \\"$broker3\\" \\"$broker0Ip\\"\\nretryUntilSuccess disconnect \\"$broker3\\" \\"$broker1Ip\\"\\n\\nretryUntilSuccess disconnect \\"$broker4\\" \\"$broker0Ip\\"\\nretryUntilSuccess disconnect \\"$broker4\\" \\"$broker1Ip\\"\\n\\n```\\n\\n\\n![general-network-partition](general-network-partition-0.png)\\n\\nIt works quite well, we can see that another broker took over the leadership and continues with processing. We reach almost the same throughput, interesting is that the activate job requests seem to scale up, which is totally unexpected! We drop now 82% of our requests because we are overloaded with activate job requests.\\n\\n![growing-request-network-partition](growing-requests-network-partition-0.png)\\n\\nIn the atomix section we can see that the both nodes, which are partitioned away, miss a lot of heatbeats and we can see the leader change, which has happened earlier.\\n![atomix-network-partition](atomix-network-partition-0.png)\\n\\nQuite early after the network partition a node preemption happened.\\n\\n![node-died](node-died-0.png)\\n\\nWe see that the processing completely stops, two reasons here: one is that the gateway was restarted and another is that the leader was restarted and we lost quourum, since we already have the network partition in place. After the restart the Broker-4 actually should know again the other nodes, which is why the heartbeat misses stopped.\\n\\n![atomix-after-restart](atomix-after-restart.png)\\n\\nAfter the Broker comes back the processing started again.\\n\\n![new-start-broker-4](new-start-broker-4.png)\\n\\nAs mentioned earlier the grpc requests increased significantly, we now drop 100% of the requests. We have ~3k incoming activate job requests.\\n\\n![grpc-after-restart](grpc-after-restart.png)\\n\\nSome time later we can see that the grpc requests has stabilized again.\\n\\n![grpc-stabilized](grpc-stabilized.png)\\n\\nThis should be investigated further, but we will stop here with this experiment since it worked as expected that we kept processing even if we partition two brokers away.\\n\\n## Second Chaos Experiment\\n\\nAfter the first experiment succeeded, I wanted to experiment how the cluster behaves if we add one follower back to group one and remove it from the second group. As you might remember we have in the first group (Broker-0, Broker-1) and in the second group (Broker-2, Broker-3, Broker-4).\\n\\n### Expected \\n\\nWhen the network partition is created and we continue with processing at some point a snapshot is taken and the log is compacted. The first group will not receive any events, which means it has the old state. If we now add Broker-2 to the first group we would expect that the first group now can take over, since it has quorum, and the second will stop working. Before it can start with further processing the Broker-0 and Broker-1 need to get the latest state of Broker-2. We expect that Broker-2 becomes leader in the first group, since it has the longer (latest) log.\\n\\n### Actual\\n\\nAgain same set up, 5 nodes, one partition and replication factor 3. I\'m using the same script as above. I will wait until a snapshot is taken, we could also trigger it now via an end point.\\n\\nWe can see no difference in processing throughput after setting up the network partition again.\\n\\n![new-partition](new-partition.png)\\n\\nFurthermore, the grpc requests seem to be stable, so it must be something related to the gateway or leader restart.\\n\\n![new-grpc](new-grpc.png)\\n\\nWhen we take a look at the atomix metrics we see that both brokers are missing heartbeats, which is expected. \\n\\n![new-heartbeats](new-heartbeats.png)\\n\\nNode preemption wanted to annoy me again... Broker 2 was restarted, because of node preemption. Since we had no quorum, a new election was started. Broker-2 came back voted for Broker-3, but missed soon also heartbeats, so it started an election again and became leader, because it was able to talk with all nodes again. This was not what we wanted to test, but it is nice to know that it works :laughing:\\n\\n![second-restart](second-restart.png)\\n\\nSo again, I re-deployed the cluster and created a snapshot by hand (via API).\\n\\nFor that I port-forwarded our admin port (9600)\\n```shell\\n$ k port-forward zell-chaos-zeebe-1 9600\\n```\\n\\nOn the leader we send the POST request to take a snapshot.\\n```sh\\n[zell zell-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ curl -X POST http://localhost:9600/actuator/partitions/takeSnapshot\\n{\\"1\\":{\\"role\\":\\"LEADER\\",\\"snapshotId\\":\\"591299-1-1611061561457-718421-717758\\",\\"processedPosition\\":723973,\\"processedPositionInSnapshot\\":718421,\\"streamProcessorPhase\\":\\"PROCESSING\\",\\"exporterPhase\\":\\"EXPORTING\\",\\"exportedPosition\\":722841}}\\n```\\n\\nWe can see in the metrics that a snapshot was taken (probably two, because I accidently executed the command twice).\\n\\n![snapshot.png](snapshot.png)\\n\\nFor the Broker 2 we check whether it already received the snapshot:\\n\\n```shell\\n[zell zell-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ curl -X GET http://localhost:9600/actuator/partitions\\n{\\"1\\":{\\"role\\":\\"FOLLOWER\\",\\"snapshotId\\":\\"595599-1-1611061566584-723972-722841\\",\\"processedPosition\\":null,\\"processedPositionInSnapshot\\":null,\\"streamProcessorPhase\\":null,\\"exporterPhase\\":null,\\"exportedPosition\\":null}}\\n```\\n\\nWe also verify that Broker-0 hasn\'t received any snapshots nor events.\\n\\n```shell\\n[zell zell-chaos/ cluster: zeebe-cluster ns:zell-chaos]$ curl -X GET http://localhost:9600/actuator/partitions\\n{\\"1\\":{\\"role\\":\\"FOLLOWER\\",\\"snapshotId\\":\\"44199-1-1611061147163-76565-53432\\",\\"processedPosition\\":null,\\"processedPositionInSnapshot\\":null,\\"streamProcessorPhase\\":null,\\"exporterPhase\\":null,\\"exportedPosition\\":null}}\\n```\\n\\nAfter that, we start with the disconnection to group two and connect Broker-2 to group one (Broker-0 and Broker-1).\\n\\n```shell\\n retryUntilSuccess disconnect \\"$broker2\\" \\"$broker3Ip\\"\\n retryUntilSuccess disconnect \\"$broker2\\" \\"$broker4Ip\\"\\n \\n retryUntilSuccess connect \\"$broker2\\" \\"$broker0Ip\\"\\n retryUntilSuccess connect \\"$broker2\\" \\"$broker1Ip\\"   \\n```\\n\\nWe can see that we now have no leader at all, because I missed to connect the first group with Broker-2 in the reverse and disconnecting group 2 from Broker-2.\\n\\n![network-partition-happening.png](network-partition-happening.png)\\n\\nAfter doing so:\\n\\n```shell\\n\\nretryUntilSuccess disconnect \\"$broker3\\" \\"$broker2Ip\\"\\nretryUntilSuccess disconnect \\"$broker4\\" \\"$broker2Ip\\"\\n\\nretryUntilSuccess connect \\"$broker0\\" \\"$broker2Ip\\"\\nretryUntilSuccess connect \\"$broker1\\" \\"$broker2Ip\\"    \\n```\\n\\nWe can see in the logs but also in the metrics that snapshots are replicated to Broker-0 and Broker-1.\\n\\n![snapshot-metrics.png](snapshot-metrics.png)\\n\\nI would expect that we also see something in the atomix snapshot panels, but here it looks like only the duration is published.\\n\\n![atomix-snapshot-metrics.png](atomix-snapshot-metrics.png)\\n\\nAfter connecting the Broker\'s we see that Broker-0 and Broker-1 are not missing heartbeats anymore and that a new leader has been chosen, Broker-2 which was the expected leader! \\n\\n![after-connect-all.png](after-connect-all.png)\\n\\nThe processing started and cluster seem to look healthy again.\\n\\n![healed.png](healed.png)\\n\\nExperiment was successful! :+1:\\n\\n## New Issues\\n\\n * Unexpected request count on network partition/node restart\\n * Snapshot metrics are unclear, which show what and Atomix snapshot metrics are not showing values\\n\\n## Participants\\n\\n  * @zelldon"},{"id":"/2021/01/07/disconnect-leader-and-follower","metadata":{"permalink":"/zeebe-chaos/2021/01/07/disconnect-leader-and-follower","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2021-01-07-disconnect-leader-and-follower/index.md","source":"@site/blog/2021-01-07-disconnect-leader-and-follower/index.md","title":"Disconnect Leader and one Follower","description":"Happy new year everyone","date":"2021-01-07T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":7.64,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Disconnect Leader and one Follower","date":"2021-01-07T00:00:00.000Z","categories":["chaos_experiment","broker","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Network partitions","permalink":"/zeebe-chaos/2021/01/19/network-partition"},"nextItem":{"title":"Message Correlation after Failover","permalink":"/zeebe-chaos/2020/11/24/message-correlation-after-failover"}},"content":"Happy new year everyone :tada:\\n\\nThis time I wanted to verify the following hypothesis `Disconnecting Leader and one Follower should not make cluster disruptive` ([#45](https://github.com/camunda/zeebe-chaos/issues/45)).\\nBut in order to do that we need to extract the Leader and Follower node for a partition from the Topology. Luckily in December we got an [external contribution](https://github.com/zeebe-io/zeebe/pull/5943) which allows us to print `zbctl status` as json.\\nThis gives us now more possibilities, since we can extract values much better out of it.\\n\\n**TL;DR** The experiment was successful :+1:\\n\\n\x3c!--truncate--\x3e\\n\\n## Preparation\\n\\nBefore we start with the experiment I wanted to extract the right node id\'s for the follower\'s and leader from the `zbctl status` output via `jq`. If we have that we can use this for other use cases.\\n\\nI stored the `zbctl` json output in a file, to make the lines a bit more readable and that I can focus on the jq stuff. The tested output looks like this:\\n\\n```shell\\n$ cat test.json \\n{\\n  \\"brokers\\": [\\n    {\\n      \\"nodeId\\": 1,\\n      \\"host\\": \\"zeebe-chaos-zeebe-1.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local\\",\\n      \\"port\\": 26501,\\n      \\"partitions\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"role\\": \\"LEADER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 2,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 3,\\n          \\"role\\": \\"LEADER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 4,\\n          \\"role\\": \\"LEADER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        }\\n      ],\\n      \\"version\\": \\"0.27.0-SNAPSHOT\\"\\n    },\\n    {\\n      \\"nodeId\\": 2,\\n      \\"host\\": \\"zeebe-chaos-zeebe-2.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local\\",\\n      \\"port\\": 26501,\\n      \\"partitions\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 2,\\n          \\"role\\": \\"LEADER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 3,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 4,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        }\\n      ],\\n      \\"version\\": \\"0.27.0-SNAPSHOT\\"\\n    },\\n    {\\n      \\"nodeId\\": 0,\\n      \\"host\\": \\"zeebe-chaos-zeebe-0.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local\\",\\n      \\"port\\": 26501,\\n      \\"partitions\\": [\\n        {\\n          \\"partitionId\\": 1,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 2,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 3,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        },\\n        {\\n          \\"partitionId\\": 4,\\n          \\"role\\": \\"FOLLOWER\\",\\n          \\"health\\": \\"HEALTHY\\"\\n        }\\n      ],\\n      \\"version\\": \\"0.27.0-SNAPSHOT\\"\\n    }\\n  ],\\n  \\"clusterSize\\": 3,\\n  \\"partitionsCount\\": 4,\\n  \\"replicationFactor\\": 3,\\n  \\"gatewayVersion\\": \\"0.27.0-SNAPSHOT\\"\\n}\\n```\\n\\nI had a really hard time to find the correct `jq` expression, but here is it:\\n\\n```shell\\n$ cat test.json | jq \\".brokers[]|select(.partitions[]| select(.partitionId == 3) and .role == \\\\\\"LEADER\\\\\\")\\"\\n```\\n\\nYou may ask why there are multiple [selects](https://stedolan.github.io/jq/manual/#select(boolean_expression)). I tried it previous with one and the issue is that it then works like an cartesian-product. It takes broker objects, which take part of the partition 3 and it will take broker objects, which are leader for an partition into the output. This is obviously not that what I want.\\nThe current expression filters brokers for partitions which have the partitionId and are leader for that partition. This [gist comment](https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4#gistcomment-3257810) helped me here.\\n\\nExamples:\\n\\n```shell\\n$ cat test.json | jq \\".brokers[]|select(.partitions[]| select(.partitionId == 3) and .role == \\\\\\"LEADER\\\\\\")|.nodeId\\"\\n1\\n$ cat test.json | jq \\".brokers[]|select(.partitions[]| select(.partitionId == 2) and .role == \\\\\\"LEADER\\\\\\")|.nodeId\\"\\n2\\n```\\n\\nLater I realized that this doesn\'t work for followers, since you can have multiple ones, BUT also this can be solved. [Just put it in an array and get the first entry](\\nhttps://stackoverflow.com/questions/38500363/get-the-first-or-nth-element-in-a-jq-json-parsing).\\n\\n```shell\\njq \\"[.brokers[]|select(.partitions[]| select(.partitionId == $partition) and .role == \\\\\\"$state\\\\\\")][0].nodeId\\n```\\n\\nAs you can see `jq` is quite powerful and I learned a lot about it this day. If you interested you can also check [the manual](https://stedolan.github.io/jq/manual/) which has ton\'s of examples.\\n\\n### Resources\\n\\n* [https://stackoverflow.com/questions/18592173/select-objects-based-on-value-of-variable-in-object-using-jq](https://stackoverflow.com/questions/18592173/select-objects-based-on-value-of-variable-in-object-using-jq)\\n* [https://unix.stackexchange.com/questions/404699/using-multiple-wildcards-in-jq-to-select-objects-in-a-json-file](https://unix.stackexchange.com/questions/404699/using-multiple-wildcards-in-jq-to-select-objects-in-a-json-file)\\n* [https://stedolan.github.io/jq/manual/#Builtinoperatorsandfunctions](https://stedolan.github.io/jq/manual/#Builtinoperatorsandfunctions)\\n* [https://stackoverflow.com/questions/33057420/jq-select-multiple-conditions](https://stackoverflow.com/questions/33057420/jq-select-multiple-conditions)\\n* [https://github.com/stedolan/jq/issues/319](https://github.com/stedolan/jq/issues/319)\\n* [https://unix.stackexchange.com/questions/491669/jq-get-attribute-of-nested-object](https://unix.stackexchange.com/questions/491669/jq-get-attribute-of-nested-object)\\n* [https://stackoverflow.com/questions/27562424/jq-nested-object-extract-top-level-id-and-lift-a-value-from-internal-object](https://stackoverflow.com/questions/27562424/jq-nested-object-extract-top-level-id-and-lift-a-value-from-internal-object)\\n* _false track_ [https://stackoverflow.com/questions/28615174/jq-filter-on-sub-object-value](https://stackoverflow.com/questions/28615174/jq-filter-on-sub-object-value)\\n* final key: [https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4#gistcomment-3257810](https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4#gistcomment-3257810)\\n\\n### Script\\n\\nI was able to replace the old utility:\\n\\n```shell\\nfunction getIndexOfPodForPartitionInState()\\n{\\n  partition=\\"$1\\"\\n  state=\\"$2\\"\\n  pod=$(getGateway)\\n  namespace=$(getNamespace)\\n\\n  # To print the topology in the journal\\n  until topology=\\"$(kubectl exec \\"$pod\\" -n \\"$namespace\\" -- zbctl status --insecure)\\"\\n  do\\n    true;\\n  done\\n\\n\\n  # For cluster size 3 and replication factor 3\\n  # we know the following partition matrix\\n  # partition \\\\ node  0    1     2\\n  #     1             L    F     F\\n  #     2             F    L     F\\n  #     3             F    F     L\\n  #    etc.\\n  # This means broker 1, 2 or 3 participates on partition 3\\n  # BE AWARE the topology above is just an example and the leader can every node participating node.\\n\\n  index=$(($(echo \\"$topology\\" \\\\\\n    | grep \\"Partition $partition\\" \\\\\\n    | grep -n \\"$state\\" -m 1 \\\\\\n    | sed \'s/\\\\([0-9]*\\\\).*/\\\\1/\') - 1))\\n  echo \\"$index\\"\\n}\\n```\\n\\nWith this:\\n\\n```shell\\nfunction getIndexOfPodForPartitionInState()\\n{\\n  partition=\\"$1\\"\\n  state=\\"$2\\"\\n  pod=$(getGateway)\\n  namespace=$(getNamespace)\\n\\n  # To print the topology in the journal\\n  until topology=\\"$(kubectl exec \\"$pod\\" -n \\"$namespace\\" -- zbctl status --insecure -o json)\\"\\n  do\\n    true;\\n  done\\n\\n  index=$(echo \\"$topology\\" | jq \\"[.brokers[]|select(.partitions[]| select(.partitionId == $partition) and .role == \\\\\\"$state\\\\\\")][0].nodeId\\")\\n  echo \\"$index\\"\\n}\\n```\\n\\nThe previous function worked only with homogeneous clusters, which means where the partitions are equally distributed. This caused issues on experiments on Production L clusters, where partitions are heterogeneous distributed, see related issue [zeebe-io/zeebe-cluster-testbench#154](https://github.com/zeebe-io/zeebe-cluster-testbench/issues/154). With this new utility we can create some new experiments also for Production - L clusters.\\n\\nI wrote a new script based on the [older disconnect/connect gateway scripts](https://github.com/camunda/zeebe-chaos/blob/master/chaos-experiments/scripts/disconnect-standalone-gateway.sh), where we disconnect the gateway with the brokers. The new one disconnects an leader for an partition with the follower and vice-versa.\\n\\nDisconnect Leader-Follower:\\n\\n```shell\\n#!/bin/bash\\nset -exuo pipefail\\n\\nsource utils.sh\\n\\npartition=1\\nnamespace=$(getNamespace)\\ngateway=$(getGateway)\\n\\n# determine leader for partition\\nindex=$(getIndexOfPodForPartitionInState \\"$partition\\" \\"LEADER\\")\\nleader=$(getBroker \\"$index\\")\\nleaderIp=$(kubectl get pod \\"$leader\\" -n \\"$namespace\\" --template=\\"{ {.status.podIP} }\\")\\n\\nindex=$(getIndexOfPodForPartitionInState \\"$partition\\" \\"FOLLOWER\\")\\nfollower=$(getBroker \\"$index\\")\\nfollowerIp=$(kubectl get pod \\"$follower\\" -n \\"$namespace\\" --template=\\"{ {.status.podIP} }\\")\\n\\n# To print the topology in the journal\\nretryUntilSuccess kubectl exec \\"$gateway\\" -n \\"$namespace\\" -- zbctl status --insecure\\n\\n# we put all into one function because we need to make sure that even after preemption the \\n# dependency is installed\\nfunction disconnect() {\\n toChangedPod=\\"$1\\"\\n targetIp=\\"$2\\"\\n\\n # update to have access to ip\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt update\\n kubectl exec -n \\"$namespace\\" \\"$toChangedPod\\" -- apt install -y iproute2\\n kubectl exec \\"$toChangedPod\\" -n \\"$namespace\\" -- ip route add unreachable \\"$targetIp\\"\\n\\n}\\n\\nretryUntilSuccess disconnect \\"$leader\\" \\"$followerIp\\"\\nretryUntilSuccess disconnect \\"$follower\\" \\"$leaderIp\\" \\n```\\n\\n## Chaos Experiment\\n\\nWe want to disconnect a leader and a follower from a specific partition.\\n\\n### Hypothesis \\n\\nWe expect that even if the leader and the follower can\'t talk with each other the follower is not able to disrupt the cluster and no new election is started, such that he becomes the leader.\\nOn reconnect we expect that the follower keeps up again and is eventually on the same page with the other follower and leader.\\n\\n### Actual\\n\\nWe deployed a cluster with one partition for simplicity. We run the above posted script to disconnect one leader with a follower and the same follower with the leader.\\n\\n#### Disconnect\\n\\nAfter running the disconnect script we see in general no disruption. The processing is still continuing.\\n\\n![](general.png)\\n\\nWe can see that the followers misses a lot of heartbeats, which is expected.\\n\\n![](heartbeats.png)\\n\\nThis is also visible in the logs:\\n\\n```shell\\n2021-01-07 20:22:28.320 CET\\nzeebe-chaos-zeebe-0\\nRaftServer{raft-partition-partition-1}{role=FOLLOWER} - No heartbeat from null in the last PT2.98S (calculated from last 2980 ms), sending poll requests\\n2021-01-07 20:22:28.321 CET\\nzeebe-chaos-zeebe-0\\nRaftServer{raft-partition-partition-1}{role=FOLLOWER} - Poll request to 1 failed: io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No route to host: zeebe-chaos-zeebe-1.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local/10.0.0.7:26502\\n2021-01-07 20:22:28.625 CET\\nzeebe-chaos-zeebe-1\\nRaftServer{raft-partition-partition-1} - AppendRequest{term=1, leader=1, prevLogIndex=2643199, prevLogTerm=1, entries=0, checksums=0, commitIndex=2755920} to 0 failed: java.util.concurrent.CompletionException: io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No route to host: zeebe-chaos-zeebe-0.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local/10.0.7.13:26502\\n2021-01-07 20:22:28.977 CET\\nzeebe-chaos-zeebe-1\\nRaftServer{raft-partition-partition-1} - AppendRequest{term=1, leader=1, prevLogIndex=2643199, prevLogTerm=1, entries=0, checksums=0, commitIndex=2756276} to 0 failed: java.util.concurrent.CompletionException: io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No route to host: zeebe-chaos-zeebe-0.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local/10.0.7.13:26502\\n2021-01-07 20:22:29.382 CET\\nzeebe-chaos-zeebe-1\\nRaftServer{raft-partition-partition-1} - AppendRequest{term=1, leader=1, prevLogIndex=2643199, prevLogTerm=1, entries=0, checksums=0, commitIndex=2756571} to 0 failed: java.util.concurrent.CompletionException: io.netty.channel.AbstractChannel$AnnotatedConnectException: connect(..) failed: No route to host: zeebe-chaos-zeebe-0.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local/10.0.7.13:26502\\n```\\n\\nThe follower is failing  to send poll requests to Broker-1, which is the leader. I assume we don\'t see that the follower sends the other follower poll requests because our log level is to high. \\nFurthermore we can see that the leader is not able to send append requests. We have a panel where we can see how many entries the follower lags behind.\\n\\n![](slow-follower.png)\\n\\nInteresting that the java heap of the follower is growing.\\n\\n![](resources-follower.png)\\n\\nBut after some time GC steps in and it goes back to normal.\\n\\n![](later-gc.png)\\n\\n#### Connect\\n\\nAfter running the connect script we can see in the log that almost immediately a snapshot is send to the follower.\\n\\n```shell\\nD 2021-01-07T19:26:24.042908Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk 000333.log of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.045690Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk 000334.sst of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.052229Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk 000335.log of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.068270Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk 000336.sst of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.076135Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk CURRENT of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.081880Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk MANIFEST-000003 of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\nD 2021-01-07T19:26:24.089900Z zeebe-chaos-zeebe-0 Consume snapshot snapshotChunk OPTIONS-000090 of snapshot 2643199-1-1610047230637-3215713-3214967  zeebe-chaos-zeebe-0\\n``` \\n\\nThis is also visible in the metrics\\n\\n![](raft-snap.png)\\n\\nWe see a healed raft.\\n\\n![](healed-raft.png)\\n\\nWhat I was wondering is why the metric which shows the lag of the follower is not really recovering.\\n\\n![](metrics-is-not-correct.png)\\n\\nEven after almost 12 hours it is still showing ~4K\\n\\n![](failing-metric.png)\\n\\n## Result\\n\\nAs we can see the experiment was successful, we were able to verify our hypothesis. The new extraction of the leader and follower from the topology gives us new possibilities for new chaos experiments.\\nI think we can also experiment a bit more with disconnecting different nodes, to see how the cluster behaves.\\n\\n## New Issues\\n\\n * Metric: Follower lag doesn\'t recover\\n\\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/11/24/message-correlation-after-failover","metadata":{"permalink":"/zeebe-chaos/2020/11/24/message-correlation-after-failover","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-11-24-message-correlation-after-failover/index.md","source":"@site/blog/2020-11-24-message-correlation-after-failover/index.md","title":"Message Correlation after Failover","description":"Today I wanted to finally implement an experiment which I postponed for long time, see #24.","date":"2020-11-24T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.38,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Message Correlation after Failover","date":"2020-11-24T00:00:00.000Z","categories":["chaos_experiment","broker","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Disconnect Leader and one Follower","permalink":"/zeebe-chaos/2021/01/07/disconnect-leader-and-follower"},"nextItem":{"title":"Many Job Timeouts","permalink":"/zeebe-chaos/2020/11/11/job-timeouts"}},"content":"Today I wanted to finally implement an experiment which I postponed for long time, see [#24](https://github.com/camunda/zeebe-chaos/issues/24).\\nThe problem was that previous we were not able to determine on which partition the message was published, so we were not able to assert that it was published on the correct partition. With this [#4794](https://github.com/zeebe-io/zeebe/issues/4794) it is now possible, which was btw an community contribution. :tada:\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nWe want to publish a message to a specific partition. After publishing the message we want to restart the corresponding leader of that partition, deploy and create a workflow instance to which the message should correlate to. \\n\\n### Hypothesis\\n\\nWe expect that even due to a leader change messages can be correlated to a workflow instance, after a new leader comes up for that partition.\\n\\n### Actual\\n\\n#### Implementation\\n The experiment should ideally work on all cluster plans, since we run the chaos experiments now with all existing cluster plans. For that we want to publish the message on partition one. In Zeebe the messages are distributed over the partitions via the correlation key. Our current cluster plans have 1, 4 or 8 partitions. In order to always reach the same partition we need a correlation key which is modulo the partition count always the same number. Ideally it is just one character, which makes the calculation easier. If we take a look at the ASCII table we see that for example `48 mod 1, 4 or 8` is always `0`. This would correspond then to partition one, since in the partition calculation we add 1. If we use \\"0\\" as correlation key we can be sure this will end up in the production clusters on partition one. For more information about the calculate you can check the [SubscriptionUtil](https://github.com/zeebe-io/zeebe/blob/develop/protocol-impl/src/main/java/io/zeebe/protocol/impl/SubscriptionUtil.java) class.\\n\\nThe process is quite simple, we just have one intermediate message catch event and we will create an new instance and await the result. With that we make sure that the message was correlated correctly.\\n\\n![oneReceiveMsgEvent](oneReceiveMsgEvent.png)\\n\\nOn testing the separate scripts I had at the begining problems with the `awaitResult`. I got always timeouts.\\n\\n```sh\\nError: rpc error: code = DeadlineExceeded desc = Time out between gateway and broker: Request type command-api-4 timed out in 8999 milliseconds\\ncommand terminated with exit code 1\\n+ echo \'Failed to execute: \'\\\\\'\'awaitInstance\'\\\\\'\'. Retry.\'\\n```\\n\\n![operate](operate.png)\\n\\nVia operate nor via zbctl it is easy to find out what is the real issue. I\'m not able to see any details regarding the intermediate message catch event in operate. With help of [zdb](https://github.com/Zelldon/zdb) I was able to track down the issue. The time to live was to small. The published messages have been already deleted before I created the corresponding workflow instancs. Per default the time to live is `5s` with `zbctl`. It is not easy to find out why the message doesn\'t correlate. After setting the `ttl` quite high it works and I can run my experiment successfully.\\n\\n#### Result\\n\\n```sh\\n$ chaos run production-m/msg-correlation/experiment.json \\n[2020-11-24 14:56:28 INFO] Validating the experiment\'s syntax\\n[2020-11-24 14:56:28 INFO] Experiment looks valid\\n[2020-11-24 14:56:28 INFO] Running experiment: Zeebe message correlation experiment\\n[2020-11-24 14:56:28 INFO] Steady-state strategy: default\\n[2020-11-24 14:56:28 INFO] Rollbacks strategy: default\\n[2020-11-24 14:56:28 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-11-24 14:56:28 INFO] Probe: All pods should be ready\\n[2020-11-24 14:56:28 INFO] Steady state hypothesis is met!\\n[2020-11-24 14:56:28 INFO] Playing your experiment\'s method now...\\n[2020-11-24 14:56:28 INFO] Action: Publish message to partition one\\n[2020-11-24 14:56:29 INFO] Action: Terminate leader of partition 1 non-gracefully\\n[2020-11-24 14:56:34 INFO] Probe: Should be able to create a workflow and await the message correlation\\n[2020-11-24 14:56:39 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-11-24 14:56:39 INFO] Probe: All pods should be ready\\n[2020-11-24 14:57:16 INFO] Steady state hypothesis is met!\\n[2020-11-24 14:57:16 INFO] Let\'s rollback...\\n[2020-11-24 14:57:16 INFO] No declared rollbacks, let\'s move on.\\n[2020-11-24 14:57:16 INFO] Experiment ended with status: completed\\n```\\n\\nExperiment added to all cluster plans:\\n * https://github.com/camunda/zeebe-chaos/commit/adeab53915e12b4a76fd4d49bb359684619b117f\\n * https://github.com/camunda/zeebe-chaos/commit/93daf11864fdd851267dae67fdfc31e0ea78b407\\n\\n\\n## New Issues\\n\\n * Operate: Show details of an intermediate catch event [OPE-1165](https://jira.camunda.com/browse/OPE-1165)\\n\\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/11/11/job-timeouts","metadata":{"permalink":"/zeebe-chaos/2020/11/11/job-timeouts","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-11-11-job-timeouts/index.md","source":"@site/blog/2020-11-11-job-timeouts/index.md","title":"Many Job Timeouts","description":"In the last game day (on friday 06.11.2020) I wanted to test whether we can break a partition if many messages time out at the same time. What I did was I send many many messages with a decreasing TTL, which all targeting a specific point in time, such that they will all timeout at the same time. I expected that if this happens that the processor will try to time out all at once and break because the batch is to big. Fortunately this didn\'t happen, the processor was able to handle this.","date":"2020-11-11T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":3.885,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Many Job Timeouts","date":"2020-11-11T00:00:00.000Z","categories":["chaos_experiment","broker"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Message Correlation after Failover","permalink":"/zeebe-chaos/2020/11/24/message-correlation-after-failover"},"nextItem":{"title":"Investigate failing Chaos Tests","permalink":"/zeebe-chaos/2020/11/03/investigate-failing-tests"}},"content":"In the last game day (on friday 06.11.2020) I wanted to test whether we can break a partition if many messages time out at the same time. What I did was I send many many messages with a decreasing TTL, which all targeting a specific point in time, such that they will all timeout at the same time. I expected that if this happens that the processor will try to time out all at once and break because the batch is to big. Fortunately this didn\'t happen, the processor was able to handle this.\\n\\nI wanted to verify the same with job time out\'s.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\nI setup an Production S cluster in camunda cloud. Deployed an normal starter, which starts 20 workflow instances per second. I used similar code to activate jobs with a decreasing timeout, such that they all timeout at the same time. The target time was 3 PM. I started the test ~11 am.\\n\\n**Code:**\\n```csharp\\n            var now = DateTime.Now;\\n            var today3PM = now.Date.AddHours(15);\\n            int count = 0;\\n            var totalMilli = (today3PM - now).TotalMilliseconds;\\n            do\\n            {\\n                try\\n                {\\n\\n                    await client\\n                        .NewActivateJobsCommand()\\n                        .JobType(\\"benchmark-task\\")\\n                        .MaxJobsToActivate(100)\\n                        .Timeout(TimeSpan.FromMilliseconds(totalMilli))\\n                        .WorkerName(\\"lol\\")\\n                        .Send(TimeSpan.FromSeconds(30));\\n                    count++;\\n\\n                    totalMilli = (today3PM - DateTime.Now).TotalMilliseconds;\\n                    if (count % 10 == 0)\\n                    {\\n                        Console.WriteLine(\\"Activated next 1000, count:\\" + count);\\n                        Console.WriteLine(\\"Total \\" + totalMilli + \\" ms until 3 am\\");\\n                    }\\n                }\\n                catch (Exception ex)\\n                {\\n                    Console.WriteLine(\\"Failed to activate job, because of \\" + ex.Message);\\n                }\\n            } while (totalMilli > 0);\\n\\n```\\n\\nI experienced a lot of pod restarts during the experiment, but at 3 pm the processor seems to handle the situation correctly and has no problems with so many events.\\n\\n\\n![timebomb](timeout-bomb.png)\\n![timebomb-general](timeout-bomb-general.png)\\n\\n## Related issues\\n\\n### No worker name\\n\\nFirst I missed the `.WorkerName` in the activation command and this broke somehow the activation.\\nIn the client I got either timeouts or resource exhausted responses, but in the gateway I saw that the worker name is missing.\\n\\n**Gateway output:**\\n\\n```\\nio.zeebe.gateway.cmd.BrokerRejectionException: Command (ACTIVATE) rejected (INVALID_ARGUMENT): Expected to activate job batch with worker to be present, but it was blank\\n\\tat io.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:185) ~[zeebe-gateway-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:137) ~[zeebe-gateway-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorJob.invoke(ActorJob.java:76) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorJob.execute(ActorJob.java:39) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorTask.execute(ActorTask.java:122) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorThread.executeCurrentTask(ActorThread.java:107) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorThread.doWork(ActorThread.java:91) [zeebe-util-0.25.0.jar:0.25.0]\\n\\tat io.zeebe.util.sched.ActorThread.run(ActorThread.java:204) [zeebe-util-0.25.0.jar:0.25.0]\\n```\\n\\n**Client output:**\\n\\n```\\nFailed to activate job, because of Status(StatusCode=\\"DeadlineExceeded\\", Detail=\\"Deadline Exceeded\\", DebugException=\\"Grpc.Core.Internal.CoreErrorDetailException: {\\"created\\":\\"@1605092309.325960242\\",\\"description\\":\\"Error received from peer ipv4:35.205.156.246:443\\",\\"file\\":\\"/var/local/git/grpc/src/core/lib/surface/call.cc\\",\\"file_line\\":1062,\\"grpc_message\\":\\"Deadline Exceeded\\",\\"grpc_status\\":4}\\")\\n```\\n\\nAfter adding the worker name it works, but begins with lot of resource exhausted. I created a new issue for it https://github.com/zeebe-io/zeebe/issues/5812 .\\n\\n## Pod restarts\\n\\n![preempt](preemptions.png)\\n\\nEvery 10 min it seems to be a node dying, which causes resource exhausted then.\\n\\nAfter looking at the [gke events](https://console.cloud.google.com/logs/viewer?interval=PT1H&authuser=1&organizationId=669107107215&project=camunda-cloud-240911&minLogLevel=0&expandAll=false&timestamp=2020-11-11T14:04:53.000000000Z&customFacets=&limitCustomFacetWidth=true&advancedFilter=jsonPayload.kind%3D%22Event%22%0Aresource.labels.cluster_name%3D%22ultrachaos%22%0AjsonPayload.involvedObject.namespace%3D%2299c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe%22%0AjsonPayload.involvedObject.name:%22zeebe%22&scrollTimestamp=2020-11-11T13:49:52.000000000Z&dateRangeEnd=2020-11-11T14:06:47.813Z&dateRangeStart=2020-11-11T13:06:47.813Z) I saw now evidence that this is caused by node preemptions. \\n\\nI checked the pods directly and saw no heap dumps in the data folder. After describing the pod I can see:\\n\\n```\\n[zell zeebe-cluster-testbench/ ns:99c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe]$ k describe pod zeebe-1\\nName:         zeebe-1\\nNamespace:    99c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe\\nPriority:     0\\nNode:         gke-ultrachaos-compute-fea23edf-tqbm/10.132.0.58\\nStart Time:   Wed, 11 Nov 2020 14:49:52 +0100\\nLabels:       app.kubernetes.io/app=zeebe\\n              app.kubernetes.io/component=gateway\\n              controller-revision-hash=zeebe-66b694fbfc\\n              statefulset.kubernetes.io/pod-name=zeebe-1\\nAnnotations:  <none>\\nStatus:       Running\\nIP:           10.56.7.16\\nIPs:\\n  IP:           10.56.7.16\\nControlled By:  StatefulSet/zeebe\\nContainers:\\n  zeebe:\\n    Container ID:   docker://a55fe90d3184bea064aec29d845680241096b0d971d66b05a35495857c5d7427\\n    Image:          camunda/zeebe:0.25.0\\n    Image ID:       docker-pullable://camunda/zeebe@sha256:1286086e786975837dcbf664daa29d41d2666af4daf4abd3192fff1426804dd6\\n    Ports:          9600/TCP, 26500/TCP, 26501/TCP, 26502/TCP, 26503/TCP, 26504/TCP\\n    Host Ports:     0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP\\n    State:          Waiting\\n      Reason:       CrashLoopBackOff\\n    Last State:     Terminated    <=====\\n      Reason:       OOMKilled    <====\\n      Exit Code:    137   \\n      Started:      Wed, 11 Nov 2020 16:06:59 +0100\\n      Finished:     Wed, 11 Nov 2020 16:16:19 +0100\\n    Ready:          False\\n```\\n\\nFurthermore we can see the `JAVA_OPTIONS`, which are:\\n\\n```\\nJAVA_TOOL_OPTIONS:                                -XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError\\n```\\n\\nI was wondering why we not setting any path for the heap dump. @npepinpe mentioned that this is done in the start up script.\\n\\nIt is true this is part of the script:\\n\\n```sh\\n[zell zeebe-cluster-testbench/ ns:99c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe]$ k describe configmaps zeebe-configmap \\nName:         zeebe-configmap\\nNamespace:    99c029a3-b6ae-4e7b-a2aa-6496adebf94c-zeebe\\nLabels:       cloud.camunda.io/channel=Internal_Dev\\n              cloud.camunda.io/clusterPlan=Production_S_v1\\n              cloud.camunda.io/clusterPlanType=Production_S\\n              cloud.camunda.io/generation=Zeebe_0_25_0\\n              cloud.camunda.io/internalSalesPlan=false\\n              cloud.camunda.io/orgName=the_org_with_the_big_cluster\\n              cloud.camunda.io/salesPlan=Paid\\nAnnotations:  <none>\\n\\nData\\n====\\nstartup.sh:\\n----\\n# ...\\n# append datestamped heapdump path\\nexport JAVA_TOOL_OPTIONS=\\"${JAVA_TOOL_OPTIONS} -XX:HeapDumpPath=/usr/local/zeebe/data/java_started_$(date +%s).hprof\\"\\n\\nenv\\nexec /usr/local/zeebe/bin/broker\\n```\\n\\nUnfortunately this is not visible outside of this context, which is why I thought it is not set.\\n\\n```sh\\nroot@zeebe-0:/usr/local/zeebe# java -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal -version\\nPicked up JAVA_TOOL_OPTIONS: -XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError\\n\\n#...\\n\\n    bool HeapDumpBeforeFullGC                     = false                                  {manageable} {default}\\n     bool HeapDumpOnOutOfMemoryError               = true                                   {manageable} {environment}\\n    ccstr HeapDumpPath                             =                                        {manageable} {default}\\n    uintx HeapFirstMaximumCompactionCount          = 3                                         {product} {default}\\n\\n#...\\n```\\n\\n```sh\\nroot@zeebe-0:/usr/local/zeebe# env | grep JAVA\\nJAVA_HOME=/usr/local/openjdk-11\\nJAVA_TOOL_OPTIONS=-XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError\\nJAVA_VERSION=11.0.8\\nroot@zeebe-0:/usr/local/zeebe# \\n\\n```\\n\\nI think we should give the production s cluster plans a bit more memory, currently has 2 gig and java can use only 1 gig. It is currently quite easy to overload the brokers and kill them with a small load.\\n\\n## New Issues\\n\\n * [#5812](https://github.com/zeebe-io/zeebe/issues/5812) \\n \\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/11/03/investigate-failing-tests","metadata":{"permalink":"/zeebe-chaos/2020/11/03/investigate-failing-tests","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-11-03-investigate-failing-tests/index.md","source":"@site/blog/2020-11-03-investigate-failing-tests/index.md","title":"Investigate failing Chaos Tests","description":"Today as part of the Chaos Day I wanted to investigate why our current Chaos Tests are failing and why our targeting cluster has been broken by them,","date":"2020-11-03T00:00:00.000Z","tags":[{"inline":true,"label":"tests","permalink":"/zeebe-chaos/tags/tests"}],"readingTime":4.57,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Investigate failing Chaos Tests","date":"2020-11-03T00:00:00.000Z","categories":["investigation"],"tags":["tests"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Many Job Timeouts","permalink":"/zeebe-chaos/2020/11/11/job-timeouts"},"nextItem":{"title":"Non-graceful Shutdown Broker","permalink":"/zeebe-chaos/2020/10/20/non-graceful-shutdown"}},"content":"Today as part of the Chaos Day I wanted to investigate why our current Chaos Tests are failing and why our targeting cluster has been broken by them,\\nsee the related issue [#5688](https://github.com/zeebe-io/zeebe/issues/5688).\\n\\n**TL;DR**\\n\\nWe found three new bugs regarding the reprocessing detection and deployment distribution, but still were not able to reproduce the real issue.\\n\\n\x3c!--truncate--\x3e\\n\\n## Investigation\\n\\nI started already yesterday with the investigation and found out that two brokers (`Broker-0` and `Broker-2`) are failing to restart on partition three, but `Broker-0` is able to start the processing on partition three. See this [comment](https://github.com/zeebe-io/zeebe/issues/5688#issuecomment-720401612) for more information. Note that partition three is receiving deployment from partition one via deployment distribution.\\n\\nTogether with @saig0 I looked at the code and we found out that reprocessing can be a bit problematic in some situations regarding the `WorkflowPersistenceCache`, see related [comment](https://github.com/zeebe-io/zeebe/issues/5688#issuecomment-721021464).\\n\\nThe cache is used in the `DeploymentCreateProcessor` to verify that the deployment is new and to add it to the state. If this deployment with the same key already exists (in-memory) then the processor rejects the `CREATE` command.\\nNow we can have situations which might be problematic. Say we have two of the `CREATE` commands on normal processing, this can happen when the first partition re-distributes the deployment. When the first one is processed we create a follow up event (`CREATED`), on the second `CREATE` we will write a rejection, since it is already in-memory. If a leader change happens, then it is crucial where the snapshot was taken. If the snapshot position is **AFTER** the first `CREATE` this means that we will handle the second `CREATE` as the first one, which means we will generate on reprocessing a `CREATE` but on the log is a rejection written. This is because the in-memory state doesn\'t reflect the real state. Opened an issue for this [#5753](https://github.com/zeebe-io/zeebe/issues/5753).\\n\\nThe bug which we found was not the real issue we currently have with the broken cluster, since we have two nodes which have the **same** snapshot, but on one Broker it fails and another it doesn\'t. To really understand what the issue is we need the related log, so we need to reproduce this issue.\\n\\n### Reproducing Chaos\\n\\nI created a benchmark with the Zeebe version `0.24.4`. Check [this](https://github.com/zeebe-io/zeebe/tree/develop/benchmarks/setup) for how to setup a benchmark. I realized that creating a benchmark for an earlier version is currently not working because we set the `useMMap` flag in the `zeebe-values.yaml` file. After removing that it works without problems.\\n\\nTo run all experiments in a loop I used in the `chaos-experiments/kubernetes` folder\\n```\\n while [ $? -eq 0 ]; do for ex in */experiment.json; do chaos run $ex; done; done\\n\\n```\\nDuring running the experiments I found a bug in our chaos experiments, where it seems that some experiments are not executed correctly, see [#43](https://github.com/camunda/zeebe-chaos/issues/43).\\n\\n\\nIt took a while, but at some point the experiments start to fail. Interesting is that if you look at the pods all seem to be ready, but in the metrics we can see that one partition is unhealthy (Partition one this time).\\nChecking the logs I found this:\\n\\n```\\nio.zeebe.engine.processor.InconsistentReprocessingException: Reprocessing issue detected! Restore the data from a backup and follow the recommended upgrade procedure. [cause: \\"The key of the record on the log stream doesn\'t match to the record from reprocessing.\\", log-stream-record: {\\"partitionId\\":1,\\"value\\":{\\"errorMessage\\":\\"\\",\\"type\\":\\"benchmark-task\\",\\"errorCode\\":\\"\\",\\"variables\\":{},\\"worker\\":\\"benchmark-worker\\",\\"deadline\\":1604403149010,\\"bpmnProcessId\\":\\"benchmark\\",\\"workflowKey\\":2251799813685250,\\"customHeaders\\":{},\\"retries\\":3,\\"elementId\\":\\"task\\",\\"elementInstanceKey\\":2251799813688892,\\"workflowDefinitionVersion\\":1,\\"workflowInstanceKey\\":2251799813688864},\\"sourceRecordPosition\\":8054,\\"timestamp\\":1604403162815,\\"position\\":9274,\\"valueType\\":\\"JOB\\",\\"intent\\":\\"TIME_OUT\\",\\"recordType\\":\\"COMMAND\\",\\"rejectionReason\\":\\"\\",\\"rejectionType\\":\\"NULL_VAL\\",\\"key\\":2251799813688902}, reprocessing-record: {key=2251799813688825, sourceRecordPosition=8054, intent=DeploymentIntent:DISTRIBUTED, recordType=EVENT}]\\n\\tat io.zeebe.engine.processor.ReProcessingStateMachine.lambda$verifyRecordMatchesToReprocessing$12(ReProcessingStateMachine.java:400) ~[zeebe-workflow-engine-0.24.4.jar:0.24.4]\\n\\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\\n\\tat io.zeebe.engine.processor.ReProcessingStateMachine.verifyRecordMatchesToReprocessing(ReProcessingStateMachine.java:394) ~[zeebe-workflow-engine-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.engine.processor.ReProcessingStateMachine.reprocessEvent(ReProcessingStateMachine.java:258) ~[zeebe-workflow-engine-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.engine.processor.ReProcessingStateMachine.reprocessNextEvent(ReProcessingStateMachine.java:226) ~[zeebe-workflow-engine-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorJob.invoke(ActorJob.java:73) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorJob.execute(ActorJob.java:39) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorTask.execute(ActorTask.java:118) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorThread.executeCurrentTask(ActorThread.java:107) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorThread.doWork(ActorThread.java:91) [zeebe-util-0.24.4.jar:0.24.4]\\n\\tat io.zeebe.util.sched.ActorThread.run(ActorThread.java:204) [zeebe-util-0.24.4.jar:0.24.4]\\n```\\n\\nI think this is caused by [#3124](https://github.com/zeebe-io/zeebe/issues/3124), because if the distribution succeeds we want to write a `DISTRUBITED` event on the log, with the stream writer from the context. This can happen concurrently with our reprocessing.\\nMy assumption was that this is caused by a race condition, which might get fixed when we restart the pod. I restarted the `Broker-2`, which was leader for partition 3 and `Broker-1` took over and started the partition successfully.\\n\\n```\\n2020-11-03 14:54:32.916 CET Broker-1-StreamProcessor-1 Processor finished reprocessing at event position 18711\\n```\\n\\nI started the chaos experiments again. They are running now for a while.\\n\\nTogether with @npepinpe I discussed the open issues. We found another problematic bug with the in-memory state of the workflow cache. If on reprocessing an error/exception happens, then we retry the processing of this record, endless.\\nBefore we retry we normally rollback the current transaction to discard all changes in the state. This doesn\'t apply for the in-memory state of the workflow cache, so this can lead to our specific scenario. We checked the log but found no\\nindication that on reprocessing another exception happen, which caused an retry. In anyway we need to fix the cache to avoid the bugs we have described. We concluded that we need the log of the failing cluster to really understand what was happening. We will try to fix the bugs above soon as possible and in parallel run the experiments endless until they fail again.\\n\\n### Notes\\nTo investigate the disks I prepared the follwing commands, which I can use to download the state of the brokers to my local machine.\\n\\n```sh\\nkubectl exec zell-chaos-0244-zeebe-2 -- tar -cf data.tar.gz data/ # compress the data dir\\nkubectl cp zell-chaos-0244-zeebe-2:/usr/local/zeebe/data.tar.gz broker-2/data.tar.gz # download the tarball\\ncd broker-2/\\ntar -xvf broker-2-data.tar.gz\\n\\n```\\n\\n\\n## New Issues\\n\\n * Gateway experiments are not executed [#43](https://github.com/camunda/zeebe-chaos/issues/43)\\n * Deployment Reprocessing inconsistencies [#5753](https://github.com/zeebe-io/zeebe/issues/5753)\\n \\n## Participants\\n\\n  * @saig0\\n  * @npepinpe\\n  * @zelldon"},{"id":"/2020/10/20/non-graceful-shutdown","metadata":{"permalink":"/zeebe-chaos/2020/10/20/non-graceful-shutdown","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-10-20-non-graceful-shutdown/index.md","source":"@site/blog/2020-10-20-non-graceful-shutdown/index.md","title":"Non-graceful Shutdown Broker","description":"Today I had not much time for the chaos day, because of writing Gameday Summary, Incident review, taking part of incidents etc. So enough chaos for one day :)","date":"2020-10-20T00:00:00.000Z","tags":[],"readingTime":1.83,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Non-graceful Shutdown Broker","date":"2020-10-20T00:00:00.000Z","categories":["chaos_experiment","broker"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Investigate failing Chaos Tests","permalink":"/zeebe-chaos/2020/11/03/investigate-failing-tests"},"nextItem":{"title":"Gateway memory consumption","permalink":"/zeebe-chaos/2020/10/27/standalone-gw-memory"}},"content":"Today I had not much time for the chaos day, because of writing Gameday Summary, Incident review, taking part of incidents etc. So enough chaos for one day :)\\n\\nBut I wanted to merge the PR from Peter and test how our brokers behave if they are not gracefully shutdown. \\nI did that on Wednesday (21-10-2020).\\n\\n\x3c!--truncate--\x3e\\n\\n## PR Merge\\n\\nI tried again the new chaos experiment with a Production M cluster, before merging. It worked quite smooth.\\nPR is merged [#41](https://github.com/camunda/zeebe-chaos/pull/41) :tada:\\n\\n## Non-graceful shutdown\\n\\nCurrently in our experiments we do a normal `kubectl delete pod`, which does an graceful shutdown. The application has time to stop it\'s services etc. It would be interesting how Zeebe handles non-graceful shutdowns. In order to achieve that we can use the option `--grace-period=0`. For more information you can read for example [this](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#force-deletion)\\n\\nI added additional experiments to our normal follower and leader restarts experiments, such that we have both graceful and non-graceful restarts.\\nBoth seem to work without any issues. I was also able to fix some bash script error with the help of [shellcheck](https://github.com/koalaman/shellcheck). Related issue https://github.com/camunda/zeebe-chaos/issues/42.\\n\\n\\nExample output:\\n\\n```\\n(chaostk) [zell kubernetes/ ns:f45d4dee-f73a-4733-9cd4-a4aa8b022376-zeebe]$ chaos run leader-terminate/experiment.json \\n[2020-10-21 15:57:23 INFO] Validating the experiment\'s syntax\\n[2020-10-21 15:57:23 INFO] Experiment looks valid\\n[2020-10-21 15:57:23 INFO] Running experiment: Zeebe Leader restart non-graceful experiment\\n[2020-10-21 15:57:23 INFO] Steady-state strategy: default\\n[2020-10-21 15:57:23 INFO] Rollbacks strategy: default\\n[2020-10-21 15:57:23 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-10-21 15:57:23 INFO] Probe: All pods should be ready\\n[2020-10-21 15:57:23 INFO] Probe: Should be able to create workflow instances on partition 3\\n[2020-10-21 15:57:27 INFO] Steady state hypothesis is met!\\n[2020-10-21 15:57:27 INFO] Playing your experiment\'s method now...\\n[2020-10-21 15:57:27 INFO] Action: Terminate leader of partition 3 non-gracefully\\n[2020-10-21 15:57:33 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-10-21 15:57:33 INFO] Probe: All pods should be ready\\n[2020-10-21 15:58:28 INFO] Probe: Should be able to create workflow instances on partition 3\\n[2020-10-21 15:58:32 INFO] Steady state hypothesis is met!\\n[2020-10-21 15:58:32 INFO] Let\'s rollback...\\n[2020-10-21 15:58:32 INFO] No declared rollbacks, let\'s move on.\\n[2020-10-21 15:58:32 INFO] Experiment ended with status: completed\\n```\\n\\nRelated commits:\\n\\n * [Restart leader non-gracefully](https://github.com/camunda/zeebe-chaos/commit/e6260cb8612a983c8ed74fd2a37a249987ad3d3d)\\n * [Restart follower non-gracefully](https://github.com/camunda/zeebe-chaos/commit/63c481c0c7dd7026f03be4e51d61a918613b0140)\\n\\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/10/27/standalone-gw-memory","metadata":{"permalink":"/zeebe-chaos/2020/10/27/standalone-gw-memory","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-10-27-standalone-gw-memory/index.md","source":"@site/blog/2020-10-27-standalone-gw-memory/index.md","title":"Gateway memory consumption","description":"In the last weeks I check multiple benchmarks and clusters in incidents. Often I had the feeling that the memory consumption from the gateway is not ideal","date":"2020-10-20T00:00:00.000Z","tags":[],"readingTime":3.775,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Gateway memory consumption","date":"2020-10-20T00:00:00.000Z","categories":["chaos_experiment","gateway","resources"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Non-graceful Shutdown Broker","permalink":"/zeebe-chaos/2020/10/20/non-graceful-shutdown"},"nextItem":{"title":"Multiple Leader Changes","permalink":"/zeebe-chaos/2020/10/13/multiple-leader-changes"}},"content":"In the last weeks I check multiple benchmarks and clusters in incidents. Often I had the feeling that the memory consumption from the gateway is not ideal\\nor that there is a memory leak. I wanted to experiment regarding this memory consumptions. Since we saw in investigating https://github.com/zeebe-io/zeebe/issues/5641 a high memory spike\\nwhen the gateway was not able to talk to other nodes I suspected that here might be some bugs hiding\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos experiment\\n\\nWe will run the Standalone gateway without the brokers and put load on it. \\n\\n### Expected\\n\\nAll requests are rejected by the gateway and the memory doesn\'t grow infinitly on a steady load. The memory consumption should be stable at some point.\\n\\n### Actual\\n\\nFirst I run the standalone gateway without any load. It seems that benchmarks without brokers are not shown in our dashboard namespaces. I fixed that for my experiment in my local grafana session. The issue was that we search for a `atomix_role` metric to get the related namespaces. This metrics is only published on broker side.\\n\\n![](memory-gw-no-broker-no-load.png)\\n\\nWe can see that even if there is no load the memory is already growing.\\n\\nPutting more load on it showed that it doesn\'t drastically increase the memory consumption, but still it was growing.\\n\\n![](memory-gw-no-broker-high-load.png)\\n\\nI think the issue here is that we currently have no limits set for the gateway, which means it will use as many as it can. There is also no pressure for the GC to run or reclaim memory.\\nWe probably want to limit it at somepoint. I created an issue for it [#5699](https://github.com/zeebe-io/zeebe/issues/5699) In order to find out whether we have a memory leak I used a profiler.\\n\\nI restarted the experiment with new settings:\\n\\n```\\n# JavaOpts:\\n# DEFAULTS\\nJavaOpts: >-\\n  # other options\\n  -Djava.rmi.server.hostname=127.0.0.1\\n  -Dcom.sun.management.jmxremote.port=9010\\n  -Dcom.sun.management.jmxremote.rmi.port=9010\\n  -Dcom.sun.management.jmxremote.authenticate=false\\n  -Dcom.sun.management.jmxremote.ssl=false\\n  -Dcom.sun.management.jmxremote.local.only=false\\n```\\n> This will open a remote, unauthenticated, plaintext JMX connection - do not use this configuration in production!\\nSee https://github.com/zeebe-io/zeebe/blob/develop/benchmarks/docs/debug/README.md#remote_jmx\\n\\nAfter I added a port forwarding I was able to open an JMX connection with Java Mission Control.\\n\\n### Conclusion\\n\\nI profiled the gateway with and without load but haven\'t found any memory leak so far.\\n\\n![](result.png)\\n\\nAlso with VisualVM and triggering multiple GC\'s I was not able to spot any thing problematic.\\n\\n![](visualvm.png)\\n\\nIn order to avoid that it uses too much memory and the memory continously grows we should set a limit for the Gateway ([#5699](https://github.com/zeebe-io/zeebe/issues/5699)).\\n\\n### Other Observations\\n\\n#### SerialGC usage\\n\\nJava Mission Control reported as an error that on a multi-core machine the serial garabage collector was used.\\nIf we check the JVM properties we can see that as well.\\n\\n![](gc-settings.png)\\n\\nThis is weird because we don\'t set any GC in our benchmarks, so I would suspect the G1 is used with Java 11. Unfortunately this depends on the available resources which are \\"detected\\" by the JVM.\\nRelated to that https://stackoverflow.com/questions/52474162/why-is-serialgc-chosen-over-g1gc\\nI think we should investigate that further, because we can see in Java mission control that we have GC pauses up to 8 seconds! I created a new issue for it [#5700](https://github.com/zeebe-io/zeebe/issues/5700).\\n\\n#### Unexpected responses\\n\\nWhen we start the `starters` they will first try to deploy a workflow model and loop in this state until they succeed. \\nIn the metrics we can see that the responses to the deployment commands are `NOT_FOUND` instead of `PARTITION_NOT_AVAILABLE`, which I would expect.\\n\\n![](unexepcted-result.png)\\n\\nWe can also see that in the log of the starter:\\n```\\n11:01:18.035 [main] WARN  io.zeebe.Starter - Failed to deploy workflow, retrying\\nio.zeebe.client.api.command.ClientStatusException: Expected to execute command, but this command refers to an element that doesn\'t exist.\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:93) ~[app.jar:0.24.2]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:50) ~[app.jar:0.24.2]\\n\\tat io.zeebe.Starter.deployWorkflow(Starter.java:128) [app.jar:0.24.2]\\n\\tat io.zeebe.Starter.run(Starter.java:55) [app.jar:0.24.2]\\n\\tat io.zeebe.App.createApp(App.java:50) [app.jar:0.24.2]\\n\\tat io.zeebe.Starter.main(Starter.java:142) [app.jar:0.24.2]\\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: NOT_FOUND: Expected to execute command, but this command refers to an element that doesn\'t exist.\\n\\tat java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]\\n\\tat java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]\\n\\tat io.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:48) ~[app.jar:0.24.2]\\n\\t... 4 more\\nCaused by: io.grpc.StatusRuntimeException: NOT_FOUND: Expected to execute command, but this command refers to an element that doesn\'t exist.\\n\\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[app.jar:0.24.2]\\n\\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:460) ~[app.jar:0.24.2]\\n\\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[app.jar:0.24.2]\\n\\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[app.jar:0.24.2]\\n\\tat me.dinowernli.grpc.prometheus.MonitoringClientCallListener.onClose(MonitoringClientCallListener.java:50) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:426) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl.access$500(ClientCallImpl.java:66) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:689) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$900(ClientCallImpl.java:577) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:751) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:740) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[app.jar:0.24.2]\\n\\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[app.jar :0.24.2]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\\n\\tat java.lang.Thread.run(Unknown Source) ~[?:?]\\n```\\n\\nThis doesn\'t make any sense. I created a new issue for it [#5702](https://github.com/zeebe-io/zeebe/issues/5702)\\n\\n## New Issues\\n\\n * Limit Gateway https://github.com/zeebe-io/zeebe/issues/5699\\n * SerialGC usage https://github.com/zeebe-io/zeebe/issues/5700\\n * Wrong error response on deployment command https://github.com/zeebe-io/zeebe/issues/5702\\n \\n## Participants\\n\\n  * @zelldon"},{"id":"/2020/10/13/multiple-leader-changes","metadata":{"permalink":"/zeebe-chaos/2020/10/13/multiple-leader-changes","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-10-13-multiple-leader-changes/index.md","source":"@site/blog/2020-10-13-multiple-leader-changes/index.md","title":"Multiple Leader Changes","description":"Today I wanted to add new chaostoolkit experiment, which we can automate.","date":"2020-10-13T00:00:00.000Z","tags":[],"readingTime":3.39,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Multiple Leader Changes","date":"2020-10-13T00:00:00.000Z","categories":["chaos_experiment","broker"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Gateway memory consumption","permalink":"/zeebe-chaos/2020/10/27/standalone-gw-memory"},"nextItem":{"title":"Play around with ToxiProxy","permalink":"/zeebe-chaos/2020/10/06/toxi-proxy"}},"content":"Today I wanted to add new chaostoolkit experiment, which we can automate.\\nWe already have experiments like restarting followers and leaders for a partition, but in the past what also caused issues was multiple restarts/leader changes\\nin a short period of time. This is the reason why I created [#39](https://github.com/camunda/zeebe-chaos/issues/39). \\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment: Multiple Leader Elections\\n\\nIn order to reduce the blast radius I setup an new Zeebe cluster with one partition (clusterplan: production-s). This makes it possible that we exactly restart the leader for that one partition.\\nLater we can also try it out with multiple partitions. In our automated environment it is anyway executed with multiple partitions.\\n\\n### Steady State\\n\\nAll Brokers are ready and we are able to create new workflow instances on the partition one.\\n\\n### Hypothesis\\n\\nEven if we cause multiple leader changes due to broker restarts we should still be able to start new workflow instances on the corresponding partition.\\n\\n### Method \\n\\nWe requesting the Topology, determine the leader for partition one restart that corresponding node and wait until it is up again. We repeat that multiple times (three times).\\n\\n### Result\\n\\nThe corresponding experiment was added via this [commit](https://github.com/camunda/zeebe-chaos/commit/11c3a96fc87991f649fb1559363ba335b2bf42a1).\\nWe were able to prove that our hypothesis is true. we are able to handle multiple leader changes even in a short period of time.\\n\\n#### Metrics\\n\\nIn the metrics we can see the behavior during the experiment and also we can see that it becomes healthy at the end.\\n\\n![general.png](general.png)\\n\\n![atomix.png](atomix.png)\\n\\nI also run this with a cluster plan M cluster with the same results:\\n\\n![multiple.png](multiple.png)\\n\\n#### Chaostoolkit\\n\\n```\\n(chaostk) [zell kubernetes/ ns:4ac065c1-a67e-4f47-8782-38a10d67515d-zeebe]$ chaos run multiple-leader-restart/experiment.json \\n[2020-10-13 14:01:30 INFO] Validating the experiment\'s syntax\\n[2020-10-13 14:01:30 INFO] Experiment looks valid\\n[2020-10-13 14:01:30 INFO] Running experiment: Zeebe Leader restart multiple times experiment\\n[2020-10-13 14:01:30 INFO] Steady-state strategy: default\\n[2020-10-13 14:01:30 INFO] Rollbacks strategy: default\\n[2020-10-13 14:01:30 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-10-13 14:01:30 INFO] Probe: All pods should be ready\\n[2020-10-13 14:01:30 INFO] Probe: Should be able to create workflow instances on partition one\\n[2020-10-13 14:01:32 INFO] Steady state hypothesis is met!\\n[2020-10-13 14:01:32 INFO] Playing your experiment\'s method now...\\n[2020-10-13 14:01:32 INFO] Action: Terminate leader of partition one\\n[2020-10-13 14:01:42 INFO] Pausing after activity for 5s...\\n[2020-10-13 14:01:47 INFO] Probe: All pods should be ready\\n[2020-10-13 14:02:32 INFO] Action: Terminate leader of partition one\\n[2020-10-13 14:02:41 INFO] Pausing after activity for 5s...\\n[2020-10-13 14:02:46 INFO] Probe: All pods should be ready\\n[2020-10-13 14:03:23 INFO] Action: Terminate leader of partition one\\n[2020-10-13 14:03:33 INFO] Pausing after activity for 5s...\\n[2020-10-13 14:03:38 INFO] Steady state hypothesis: Zeebe is alive\\n[2020-10-13 14:03:38 INFO] Probe: All pods should be ready\\n[2020-10-13 14:04:12 INFO] Probe: Should be able to create workflow instances on partition one\\n[2020-10-13 14:04:16 INFO] Steady state hypothesis is met!\\n[2020-10-13 14:04:16 INFO] Let\'s rollback...\\n[2020-10-13 14:04:16 INFO] No declared rollbacks, let\'s move on.\\n[2020-10-13 14:04:16 INFO] Experiment ended with status: completed\\n```\\n\\n## Chaos Experiment: High Load\\n\\nAs mentioned last week @pihme has reported voluntarily that he wants to implement another chaos experiment.\\nHe worked on #7, where we expect that even we put high load on the Zeebe cluster we will cause no leader changes. This was in the past an failure case, where high load disrupted the cluster.\\n\\n\\n### Steady State\\n\\nAll Brokers are ready and we can create workflow instances on all partitions. We store the begining topology for later.\\n\\n### Hypothesis\\n\\nWe expect that even on high load we are not able to disrupt the cluster and that this will not cause any leader changes.\\n\\n### Method\\n\\nPut high load on the cluster for several minutes, via creating workflow instances\\n\\n### Result\\n\\n@pihme create a new PR to add the experiment [#41](https://github.com/camunda/zeebe-chaos/pull/41) \\n\\n\\n#### Metrics\\n\\nWe see that we already put some load on the cluster but it is not enough to exhaust the request limits and reach back pressure.\\n\\n![highload](highload.png)\\n\\nWe neeed to find a good way how put high load on the Zeebe cluster. We will continue on this.\\n\\n## Participants\\n\\n  * @pihme\\n  * @zelldon"},{"id":"/2020/10/06/toxi-proxy","metadata":{"permalink":"/zeebe-chaos/2020/10/06/toxi-proxy","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-10-06-toxi-proxy/index.md","source":"@site/blog/2020-10-06-toxi-proxy/index.md","title":"Play around with ToxiProxy","description":"First chaos day since my parental leave.","date":"2020-10-06T00:00:00.000Z","tags":[{"inline":true,"label":"tools","permalink":"/zeebe-chaos/tags/tools"}],"readingTime":3.275,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Play around with ToxiProxy","date":"2020-10-06T00:00:00.000Z","categories":["chaos_experiment","toxiProxy"],"tags":["tools"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Multiple Leader Changes","permalink":"/zeebe-chaos/2020/10/13/multiple-leader-changes"},"nextItem":{"title":"Experiment with Camunda Cloud","permalink":"/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud"}},"content":"First chaos day since my parental leave :tada:.\\n\\nToday I wanted to play a bit with [ToxiProxy](https://github.com/Shopify/toxiproxy). Toxiproxy is a framework for simulating network conditions and ideal to do some chaos on the network.\\n\\n\x3c!--truncate--\x3e\\n\\n## Run ToxiProxy\\n\\nDownload from the [release page](https://github.com/Shopify/toxiproxy/releases) the latest version (server and cli).\\n\\nStart a broker via docker.\\n\\n```sh\\ndocker pull camunda/zeebe:SNAPSHOT\\ndocker run -p 26500:26500 camunda/zeebe:SNAPSHOT\\n```\\n\\nStart the toxi proxy server.\\n\\n```sh\\n./toxiproxy-server-linux-amd64 start\\n```\\n\\nCreate a proxy for zeebe\\n```sh\\n./toxiproxy-cli-linux-amd64 create zeebe-proxy -l localhost:26379 -u localhost:26500\\nCreated new proxy zeebe-proxy\\n```\\n\\nYou should see something in the toxy proxy server log:\\n\\n```sh\\nINFO[0031] Started proxy                                 name=zeebe-proxy proxy=127.0.0.1:26379 upstream=localhost:26500\\n```\\n\\nTry zbctl to request the topology.\\n\\n```sh\\n./zbctl --address localhost:26379 status --insecure\\n\\nCluster size: 1\\nPartitions count: 1\\nReplication factor: 1\\nGateway version: 0.25.0-SNAPSHOT\\nBrokers:\\n  Broker 0 - 172.17.0.2:26501\\n    Version: 0.25.0-SNAPSHOT\\n    Partition 1 : Leader\\n```\\n\\nIn the toxy proxy server log it should be shown as:\\n\\n```sh\\nINFO[0149] Accepted client                               client=127.0.0.1:41510 name=zeebe-proxy proxy=127.0.0.1:26379 upstream=localhost:26500\\nWARN[0149] Source terminated                             bytes=245 err=read tcp 127.0.0.1:56178->127.0.0.1:26500: use of closed network connection name=zeebe-proxy\\n```\\n\\nAdd latency to requests\\n\\n```sh\\n$ ./toxiproxy-cli-linux-amd64 toxic add -t latency -a latency=5000 zeebe-proxy\\nAdded downstream latency toxic \'latency_downstream\' on proxy \'zeebe-proxy\'\\n```\\n\\nRunning zbctl again:\\n\\n```sh\\n ./zbctl --address localhost:26379 status --insecure\\nError: rpc error: code = DeadlineExceeded desc = context deadline exceeded\\n```\\n\\nUpdating existing toxy:\\n\\n```sh\\n./toxiproxy-cli-linux-amd64 toxic update -n latency_downstream -t latency -a latency=500 zeebe-proxy\\n```\\n\\nRunning zbctl again:\\n\\n```sh\\ntime ./zbctl --address localhost:26379 status --insecure\\nCluster size: 1\\nPartitions count: 1\\nReplication factor: 1\\nGateway version: 0.25.0-SNAPSHOT\\nBrokers:\\n  Broker 0 - 172.17.0.2:26501\\n    Version: 0.25.0-SNAPSHOT\\n    Partition 1 : Leader\\n\\nreal\\t0m1.045s\\nuser\\t0m0.012s\\nsys\\t0m0.021s\\n\\n```\\n\\nInspect existing toxics:\\n\\n```sh\\n$ ./toxiproxy-cli-linux-amd64 inspect zeebe-proxy\\nName: zeebe-proxy\\tListen: 127.0.0.1:26379\\tUpstream: localhost:26500\\n======================================================================\\nUpstream toxics:\\nProxy has no Upstream toxics enabled.\\n\\nDownstream toxics:\\nlatency_downstream:\\ttype=latency\\tstream=downstream\\ttoxicity=1.00\\tattributes=[\\tjitter=0\\tlatency=500\\t]\\n\\n```\\n\\nWith toxicity we can change whether it should be applied on all requests or only on some. It is possible to add the latency instead of downstream to upstream. There also other things we can inject, like slicing and delaying packages, dropping packages and limiting the bandwith.\\n\\n\\nPossible new experiments:\\n\\n * introduce latency between one follower and leader - if only one follower experience delays we expect that no election is started\\n * introduce latency betweem gw and broker - see whether command timeout\\n * slice packages - drop packages, but not every packages - expect that command is send correctly since requests are retried\\n\\n\\n### Slice packages\\n\\nSlices packages after 128 bytes:\\n```sh\\n./toxiproxy-cli-linux-amd64 toxic add zeebe-proxy -t slicer -a average_size=128\\n```\\n\\nPublish message seem to work:\\n```sh\\n$ time ./zbctl --address localhost:26379 publish message failing --insecure --correlationKey key --variables \\"{}\\"\\n{\\n  \\"key\\": 2251799813685253\\n}\\n```\\n\\nAfter limiting it to 32 bytes:\\n\\n```sh\\n$ ./toxiproxy-cli-linux-amd64 toxic update -n slicer_downstream -a average_size=32 zeebe-proxy\\nUpdated toxic \'slicer_downstream\' on proxy \'zeebe-proxy\'\\n```\\n\\nThe publish message seem to not work as expected.\\n\\n```sh\\n$ time ./zbctl --address localhost:26379 publish message failing --insecure --correlationKey key --variables \\"{}\\"\\nnull\\n\\nreal\\t0m0.039s\\nuser\\t0m0.007s\\nsys\\t0m0.023s\\n```\\n\\nActually I would expect here an error instead of just returning null.\\n\\n## Chaos Experiment \\n\\n### No Leader change on high load\\n\\n Peter volunteered for automating a new chaos experiment, where we put high load on a broker and expect that we have no leader change. This was previous an issue, since the leaders were not able to send heartbeats in time. Related issue #7.\\n\\n### Time reset\\nI wanted to work on the clock reset [#3](https://github.com/camunda/zeebe-chaos/issues/3).\\nThis seems to be not easily possible in kubernetes or at least with our current images, since we need for that root privilges.\\n\\n```sh\\nroot@zell-time-reset-zeebe-0:/usr/local/zeebe# date -s $(date +%Y-%m-%dT%H:%M:%S)\\ndate: cannot set date: Operation not permitted\\nTue Oct  6 11:51:19 UTC 2020\\n```\\n\\nIt seems that chaos mesh supports something like that for kubernetes maybe worth to look at\\nhttps://pingcap.com/blog/simulating-clock-skew-in-k8s-without-affecting-other-containers-on-node\\n\\n\\n## Participants\\n\\n  * @pihme\\n  * @zelldon"},{"id":"/2020/08/20/experiment-with-camunda-cloud","metadata":{"permalink":"/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-08-20-experiment-with-camunda-cloud/index.md","source":"@site/blog/2020-08-20-experiment-with-camunda-cloud/index.md","title":"Experiment with Camunda Cloud","description":"In order to make our chaos experiments more realistic we have setup a new gke cluster, which is similar to the Camunda Cloud gke cluster.","date":"2020-08-20T00:00:00.000Z","tags":[],"readingTime":3.875,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Experiment with Camunda Cloud","date":"2020-08-20T00:00:00.000Z","categories":["chaos_experiment","cloud"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Play around with ToxiProxy","permalink":"/zeebe-chaos/2020/10/06/toxi-proxy"},"nextItem":{"title":"Experiment with Low Load","permalink":"/zeebe-chaos/2020/08/06/low-load"}},"content":"In order to make our chaos experiments more realistic we have setup a new gke cluster, which is similar to the Camunda Cloud gke cluster.\\nIt allows us to test and experiment with Zeebe clusters which have the same configuration as Zeebe clusters in the Camunda cloud.\\n\\nAs part of the chaos day I run the same benchmark we normally run in our gke with our configuration against the Camunda Cloud Zeebe clusters.\\n\\n\x3c!--truncate--\x3e\\n\\n## Configurations of Zeebe Clusters\\n\\nIn the following table I want to highlight the different configurations of the different Zeebe Clusters (cluster types).\\n\\n| Name | Our Default | Prod S | Prod M | Prod L |\\n|------|-------------|--------|--------|--------|\\n|Partitions|  3      |   1    |   4    |  8     |\\n|Nodes|       3       |   3    |   3    |   6    |\\n|Replication| 3     |   3     |   3   |    3   |\\n|SnapshotPeriod| 15 | 5 | 5 | 5 |\\n|CPU_THREADS| 4 | 1 | 4 | 4 |\\n|IO_THREADS| 4 | 2 | 4 | 4 |\\n|CPU LIMIT| 5 | 1 | 4 | 4 |\\n|CPU REQUEST| 5 | 200m | 200m | 200m |\\n|RAM LIMIT| 12Gi | 2Gi | 8Gi | 8Gi |\\n|RAM REQUEST| 12Gi | 250Mi | 250Mi | 250Mi|\\n|Gateway|Standalone|Embedded|Embedded|Embedded|\\n\\n## Benchmarks\\n\\n| Name | Our Default | Prod S | Prod M | Prod L |\\n|------|-------------|--------|--------|--------|\\n|General|![base](base.png)|![prods](prod-s-general.png)|![prods](prod-m-general.png)|![prods](prod-l-general.png)|\\n|Resources|![base](base-res.png)|![prods](prod-s-res.png)|![prods](prod-m-res.png)|![prods](prod-l-res.png)|\\n|Disk usage||![prods](prod-s-disk.png)|![prods](prod-m-disk.png)|![prods](prod-l-disk.png)|\\n|Latency|![base](base-latency.png)|![prods](prod-s-latency.png)|![prods](prod-m-latency.png)|![prods](prod-l-latency.png)|\\n|Working|![base](base.png)|![prods](prod-s-working.png)|![prods](prod-m-working.png)|![prods](prod-l-working.png)|\\n\\nIn general we can see that the clusters haven\'t survived long. This is also visible in our Camunda Cloud status page.\\n![status](status.png)\\n\\nI think it is kind of related with the preemtable nodes, high load, long restarts and that pods are restarted after 15 minutes, when there are not getting ready.\\nOne of the reasons why restarting takes so long is fixed now with [#5189](https://github.com/zeebe-io/zeebe/pull/5189) so I hope that this gets better. But currently it is an issue, since you start replicating a snapshot and reprocess on start up. If this takes longer then 15 min the pod will be restarted because of this configuration: `Liveness:   http-get http://:9600/ready delay=900s timeout=1s period=15s #success=1 #failure=3` after restarting the pod you haven\'t gained any value you just need to start again the complete procedure. In k8 we can see a high restart count of the pods.\\n\\nInteresting is if we take a look at the working part of Prod S then we clearly see how often actually a pod is preemted or leader change happens.\\n\\n![prod-s](prod-s-working.png)\\n\\nIt is a known issue that currently the nodes are preemted quite often in Camunda Cloud and they working on a solution to it.\\n\\n### Throughput\\n\\nIf we take a look at the Working part we can see that we scale based on the partition count (or prod cluster sizes) in Camunda Cloud. For Prod S we reach in avg ~24 workflow instance creation/completions per second. For Prod M we reach in avg ~46 workflow instance creation/completions per second. For Prod L we reach in avg ~99 workflow instance creations and completions. To be fair I run the benchmark on these cluster sizes only with three workers, which have 8 threads an activation count of 120 and they completing an job after 150 ms delay, and an starter which starts 100 workflow instances per second. Normally we use in our benchmarks 12 workers and start 300 workflow instances per second. I tried that with the Prod L cluster, but this failed quite fast after increasing the load. Here we probably need to investigate further. If we take a look at our cluster setup then we reach in avg ~147 workflow instance creations/completions per second.\\n\\n### Latency\\n\\nIf take a look at the latency we can see that in Prod M cluster the latency seems to be a bit problematic, where in Prod S and L it seems similar. In our default cluster we get the best latency. Might be worth to take a look as well.\\n\\n### Other Observations\\n\\nDuring the benchmark observations I saw that some metrics are missing.\\n\\nFor example the Gateway metrics are not shown:\\n\\n![gw](missing-gw-metrics.png)\\n\\nFurthermore I saw that all container related and pvc related metrics are missing. I was not able to check the IO metrics nor the CPU metrics and other.\\n\\n![io](missing-io.png)\\n\\nIf we want to run more tests and chaos experiments we need to fix these missing metrics before. Opened a new issue for it [#242](https://github.com/camunda-cloud/monitoring/issues/242)\\n\\n## Running automated Chaos experiments\\n\\nIn order to run automated chaos experiments in our new gke. I had to create a new serviceaccount and rolebindings, such that our Jenkins can access the new Kubernetes cluster and our experiments can delete and create new resources.\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/08/06/low-load","metadata":{"permalink":"/zeebe-chaos/2020/08/06/low-load","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-08-06-low-load/index.md","source":"@site/blog/2020-08-06-low-load/index.md","title":"Experiment with Low Load","description":"* Run a benchmark with low load","date":"2020-08-06T00:00:00.000Z","tags":[],"readingTime":2.91,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Experiment with Low Load","date":"2020-08-06T00:00:00.000Z","categories":["chaos_experiment","load broker"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment with Camunda Cloud","permalink":"/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud"},"nextItem":{"title":"Experiment without Exporters","permalink":"/zeebe-chaos/2020/07/30/experiment-without-exporters"}},"content":"* Run a benchmark with low load\\n * Investigate last benchmark failures\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n We currently seem to have issues with RocksDB, which sometimes generates a lot of SST files during the Broker lifetime. This causes to fail the snapshot replication at some point.\\n This is especially problematic after pod\'s get restarted, since a follower normally need either to be catched up with the log or the leader will send a snapshot to the follower.\\n If the snapshot contains a lot of files this get\'s problematic. In order to understand this better we would like to find out how we can reproduce it. We expected this happens only on low load,\\n the assumption is that RocksDB will not trigger the compaction so often, because we are not reaching a certain threshold. See the related issue [#4887](https://github.com/zeebe-io/zeebe/issues/4887) .\\n\\n### Expected\\n\\n When creating workflow instances and completing them directly afterwards we normally expect that there is nothing left and nothing should accumulate together. If we do it on low load, which means 1 workflow instance creation and completion per second then we expect the same. Furthermore we expect that the used resources are lower then on higher load.\\n\\n### Actual\\n\\n We have setup our default benchmark with three partition, three nodes and replication factor three. We starting one workflow instance at a time. We running one worker, which completes the related workflow.\\n\\nIn the general overview we can see that we start and complete one workflow per second.\\n![general](general-one-workflow.png)\\n\\nThe resource consumption looks ok.\\n![resource](resources-one-workflow.png)\\n\\nBut the RocksDB used size and snapshot files seem to be increasing.\\n![rocks1](rocks1.png)\\n![rocks2](rocks2.png)\\n![snapshot](https://user-images.githubusercontent.com/2758593/89533130-384fcc80-d7f3-11ea-83b8-69bbe75f5211.png)\\n \\n\\nI will let the benchmark run a bit longer and I think we need to investigate this issue further.\\n\\n## Investigation Last Benchmark\\n\\nEvery week I create a benchmark on the chaos day and let it run until the next chaos day. Since I had not much time for trying out other experiments today and I saw that the old benchmark has problems I decided to investigated why the processing went down. I collected here my observations and use this more as a summary, since until now it is not clear what is the issue.\\n\\nIn the general overview we can see that the processing is in avg under 100 workflow instances per second, normally we would expect something around 130. We can also see that the processing in general is not that stable.\\n![general](general.png)\\n\\nIf we take a look at the partitions separatly we can see that the partition two died quite early.\\n\\n**Partition 1 last 7 days**\\n![partition-1-general-7-days](https://user-images.githubusercontent.com/2758593/89543087-cb433380-d800-11ea-9d03-27d60a6cc49d.png)\\n**Partition 2 last 7 days**\\n![partition-2-general-7-days](https://user-images.githubusercontent.com/2758593/89543091-cb433380-d800-11ea-80ab-922c483cd039.png)\\n**Partition 3 last 7 days**\\n![partition-3-general-7-days](https://user-images.githubusercontent.com/2758593/89543093-cbdbca00-d800-11ea-9e86-d14b165e02eb.png)\\n\\nWe can see that the processing seem to be stopped for the partition two and never comes back. This seem to happen on the 31-07-2020 ~ 2 pm.\\nWith the resource panel we can also see that at this time a node preemption happen, since all pod seem to be rescheduled. This can be seen based on the different colors of the graphs.\\n\\n![resources](https://user-images.githubusercontent.com/2758593/89543095-cbdbca00-d800-11ea-962e-aee3fa95a826.png)\\n\\nIn the log we can see that all brokers are getting closed.\\n\\nInteresting is that we see the day after, an continously growing disk usage, which never gets reduced.\\n\\n![disk-usage](https://user-images.githubusercontent.com/2758593/89543083-c9797000-d800-11ea-82f3-a1b505811171.png)\\n\\nCurrently I don\'t understand what was the issue and why it never comes back. I created an issue to further investigate that [#5127](https://github.com/zeebe-io/zeebe/issues/5127) . I will keep the old benchmark running and will setup a separe one in order to reproduce this.\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/07/30/experiment-without-exporters","metadata":{"permalink":"/zeebe-chaos/2020/07/30/experiment-without-exporters","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-07-30-experiment-without-exporters/index.md","source":"@site/blog/2020-07-30-experiment-without-exporters/index.md","title":"Experiment without Exporters","description":"* Run a chaos experiment without exporters","date":"2020-07-30T00:00:00.000Z","tags":[],"readingTime":5.755,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Experiment without Exporters","date":"2020-07-30T00:00:00.000Z","categories":["chaos_experiment","exporters"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment with Low Load","permalink":"/zeebe-chaos/2020/08/06/low-load"},"nextItem":{"title":"Big Multi Instance","permalink":"/zeebe-chaos/2020/07/16/big-multi-instance"}},"content":"* Run a chaos experiment without exporters\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n We wanted to run a chaos experiment, which covers [#20](https://github.com/camunda/zeebe-chaos/issues/20).\\n Furthermore, it was recently asked in the forum whether it makes a difference performance wise to run a broker without exporters, see [here](https://forum.zeebe.io/t/zeebe-low-performance/1356/17) \\n\\n### Expected\\n\\n Running Broker without exporter should work without any problems. We should be able to delete data on every snapshot interval.\\n Performance wise there should be no difference. \\n\\n### Actual\\n\\n We have setup three different benchmarks:\\n\\n * default with elastic and metrics exporter enabled\\n * only with metrics exporter\\n * without any exporter\\n\\n These benchmarks run overnight without bigger issues. This means all of three where able to take snapshots and compact the log. This satisfy our hypothesis of https://github.com/camunda/zeebe-chaos/issues/20 .\\n\\n| Default | Without exporters |\\n|---|---|\\n| ![default-pvc](default-pvc.png) | ![without-exporter-pvc](without-exporter-pvc.png) |\\n\\nThe resource consumption seem to be kind of similar, but we still see that the memory usage increases overtime. This seems to be related to [#4812](https://github.com/zeebe-io/zeebe/issues/4812)\\n\\n| Default | Metric Exporter | Without exporters |\\n|---|---|---|\\n| ![default](default-resources.png) | ![metric](metric-exporter-resources.png) | ![without](without-exporter-resources.png) |\\n\\n Unexpected was that we see a difference in throughput. The benchmark without exporters seem to have a better throughput overall. It is able to complete ~ 30 workflow instances more per second, then the other benchmarks.\\n\\n| Default | Metric Exporter | Without exporters |\\n|---|---|---|\\n| ![default](default-general.png) | ![metric](metric-exporter-general.png) | ![without](without-exporter-general.png) |\\n\\n  We compared also other benchmarks which we have currently running, e.g. a snapshot from 24-07-2020 or from the 0.24.1 release. \\n\\n| Snapshot 24-07 | Release 0.24.1 |\\n|---|---|\\n| ![snapshot](snapshot-24-7-general.png) | ![release-241](release-0241-general.png) |\\n\\n  All benchmarks with exporters seem to have a throughput around ~140 workflow instance completions per second, but the benchmarks without exporters reaches ~170 workflow instance completions per second.\\n When we check the metrics we can see that sometimes brokers are leader for all partition and sometimes it is good distributed, but even this makes not that huge difference as the fact to having no exporter.\\n This is unexpected and we need to investigate further, created new issue for this [#5085](https://github.com/zeebe-io/zeebe/issues/5085)\\n \\n The latency seems to be not affected by this.\\n\\n#### General Observations\\n\\n##### RocksDB\\n\\nAfter taking a look at the metrics of the different benchmarks we can see that at one benchmark we have a higher live data size, which is unexepected.\\n\\n\\n  ![without-exporter-rocksdb](without-exporter-rocksdb.png)\\n  ![default-rocksdb](default-rocksdb.png)\\n  ![metric-exporter-rocksdb](default-rocksdb.png)\\n\\nCreated an issue for it [#5081](https://github.com/zeebe-io/zeebe/issues/5081)\\n\\n##### Atomix Bootstrap\\n\\nOn taking a look at the logs during starting the benchmarks we can see that the logging of atomix is not really useful.\\n\\nIt prints several statements which seem to be just noisy.\\n```\\nI 2020-07-30T10:17:54.198527Z TCP server listening for connections on 0.0.0.0:26502 \\nI 2020-07-30T10:17:54.199468Z Started \\nI 2020-07-30T10:17:54.223547Z UDP server listening for connections on 0.0.0.0:26502 \\nI 2020-07-30T10:17:54.224941Z Joined \\nI 2020-07-30T10:17:54.228644Z Started \\nI 2020-07-30T10:17:54.229384Z Started \\nI 2020-07-30T10:17:54.229856Z Started \\nI 2020-07-30T10:17:54.231121Z Started \\n```\\n\\nCreated an issue for it [#5080](https://github.com/zeebe-io/zeebe/issues/5080)\\n\\n##### Merged log statement\\n\\nAnother issue we can see during startup is that from time the log statements are merged together in stackdriver.\\n\\nLooks like this:\\n\\n```\\nI 2020-07-30T09:16:47.053339Z Bootstrap Broker-1 partitions succeeded. Started 3 steps in 166 ms. \\nI 2020-07-30T09:16:47.053352763Z {\\"severity\\":\\"DEBUG\\",\\"logging.googleapis.com/sourceLocation\\":{\\"function\\":\\"startStepByStep\\",\\"file\\":\\"StartProcess.java\\",\\"line\\":63},\\"message\\":\\"Bootstrap Broker-1 partitions [3/3]: partition 1 started in 7 ms\\",\\"serviceContext\\":{\\"service\\":\\"zeebe-broker\\",\\"version\\":\\"zeebe-chaos-metric-exporter\\"},\\"context\\":{\\"threadId\\":1,\\"broker-id\\":\\"Broker-1\\",\\"threadPriority\\":5,\\"loggerName\\":\\"io.zeebe.broker.system\\",\\"threadName\\":\\"main\\"},\\"timestampSeconds\\":1596100607,\\"timestampNanos\\":52869000,\\"logger\\":\\"io.zeebe.broker.system\\",\\"thread\\":\\"main\\"}{\\"severity\\":\\"DEBUG\\",\\"logging.googleapis.com/sourceLocation\\":{\\"function\\":\\"calculateHealth\\",\\"file\\":\\"CriticalComponentsHealthMonitor.java\\",\\"line\\":91},\\"message\\":\\"The components are healthy. The current health status of components: {ZeebePartition-1=HEALTHY}\\",\\"serviceContext\\":{\\"service\\":\\"zeebe-broker\\",\\"version\\":\\"zeebe-chaos-metric-exporter\\"},\\"context\\":{\\"threadId\\":263,\\"threadPriority\\":5,\\"loggerName\\":\\"io.zeebe.broker.system\\",\\"threadName\\":\\"Broker-1-zb-actors-3\\",\\"actor-name\\":\\"Broker-1-ZeebePartition-1\\"},\\"timestampSeconds\\":1596100607,\\"timestampNanos\\":52860000,\\"logger\\":\\"io.zeebe.broker.system\\",\\"thread\\":\\"Broker-1-zb-actors-3\\"}\\n \\nI 2020-07-30T09:16:47.053379451Z \\n\\n```\\n\\nCreated an issue for it [#5079](https://github.com/zeebe-io/zeebe/issues/5079)\\n\\n##### Wrong Exporter Configuration\\nWhen Exporter is configured falsely it breaks the start up, which means the exporter can\'t be loaded and we see an exception.\\n\\n```\\n{\\n insertId: \\"1mp0o7821rhxwebj4\\"  \\n jsonPayload: {\\n  context: {\u2026}   \\n  exception: \\"java.lang.IllegalStateException: Failed to load exporter with configuration: ExporterCfg{, jarPath=\'null\', className=\'\', args={index={ignoreVariablesAbove=32767}, url=http://elasticsearch-master:9200}}\\n\\tat io.zeebe.broker.system.partitions.ZeebePartition.<init>(ZeebePartition.java:145) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.lambda$partitionsStep$22(Broker.java:344) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.lambda$startStepByStep$2(StartProcess.java:60) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.takeDuration(StartProcess.java:88) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.startStepByStep(StartProcess.java:58) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.takeDuration(StartProcess.java:88) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.start(StartProcess.java:43) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.partitionsStep(Broker.java:352) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.lambda$initStart$10(Broker.java:184) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.lambda$startStepByStep$2(StartProcess.java:60) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.takeDuration(StartProcess.java:88) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.startStepByStep(StartProcess.java:58) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.takeDuration(StartProcess.java:88) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.bootstrap.StartProcess.start(StartProcess.java:43) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.internalStart(Broker.java:135) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.util.LogUtil.doWithMDC(LogUtil.java:21) [zeebe-util-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.Broker.start(Broker.java:115) [zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:65) [zeebe-distribution-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:795) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:779) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:322) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.3.1.RELEASE.jar:2.3.1.RELEASE]\\n\\tat io.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:52) [zeebe-distribution-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\nCaused by: io.zeebe.broker.exporter.repo.ExporterLoadException: Cannot load exporter [elasticsearch]: cannot load specified class\\n\\tat io.zeebe.broker.exporter.repo.ExporterRepository.load(ExporterRepository.java:81) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.system.partitions.ZeebePartition.<init>(ZeebePartition.java:143) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\t... 23 more\\nCaused by: java.lang.ClassNotFoundException: \\n\\tat jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source) ~[?:?]\\n\\tat jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source) ~[?:?]\\n\\tat java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]\\n\\tat io.zeebe.broker.exporter.repo.ExporterRepository.load(ExporterRepository.java:78) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\tat io.zeebe.broker.system.partitions.ZeebePartition.<init>(ZeebePartition.java:143) ~[zeebe-broker-0.25.0-SNAPSHOT.jar:0.25.0-SNAPSHOT]\\n\\t... 23 more\\n\\"   \\n  logger: \\"io.zeebe.broker.system\\"   \\n  message: \\"Bootstrap Broker-1 [11/12]: zeebe partitions failed with unexpected exception.\\"   \\n  serviceContext: {\u2026}   \\n  thread: \\"main\\"   \\n }\\n labels: {\u2026}  \\n logName: \\"projects/zeebe-io/logs/stdout\\"  \\n receiveTimestamp: \\"2020-07-30T04:58:07.561999605Z\\"  \\n resource: {\u2026}  \\n severity: \\"INFO\\"  \\n sourceLocation: {\u2026}  \\n timestamp: \\"2020-07-30T04:58:04.221614Z\\"  \\n}\\n```\\n\\n\\nThe Broker is shutdown afterwards.\\n\\n```\\nI 2020-07-30T05:02:58.275180Z Bootstrap Broker-1 partitions [1/3]: partition 3 \\nI 2020-07-30T05:02:58.288302Z Bootstrap Broker-1 partitions [1/3]: partition 3 failed with unexpected exception. \\nI 2020-07-30T05:02:58.289941Z Closing Broker-1 partitions succeeded. Closed 0 steps in 0 ms. \\nI 2020-07-30T05:02:58.290305Z Bootstrap Broker-1 [11/12]: zeebe partitions failed with unexpected exception. \\nI 2020-07-30T05:02:58.291341Z Closing Broker-1 [1/10]: leader management request handler \\nD 2020-07-30T05:02:58.293136Z Closing Broker-1 [1/10]: leader management request handler closed in 2 ms \\nI 2020-07-30T05:02:58.293512Z Closing Broker-1 [2/10]: disk space monitor \\nD 2020-07-30T05:02:58.294140Z Closing Broker-1 [2/10]: disk space monitor closed in 1 ms \\nI 2020-07-30T05:02:58.294509Z Closing Broker-1 [3/10]: monitoring services \\nD 2020-07-30T05:02:58.295059Z Closing Broker-1 [3/10]: monitoring services closed in 1 ms \\nI 2020-07-30T05:02:58.295371Z Closing Broker-1 [4/10]: topology manager \\nD 2020-07-30T05:02:58.295964Z Closing Broker-1 [4/10]: topology manager closed in 0 ms \\nI 2020-07-30T05:02:58.296264Z Closing Broker-1 [5/10]: cluster services \\nD 2020-07-30T05:02:58.296612Z Closing Broker-1 [5/10]: cluster services closed in 0 ms \\nI 2020-07-30T05:02:58.296928Z Closing Broker-1 [6/10]: subscription api \\nD 2020-07-30T05:02:58.297450Z Closing Broker-1 [6/10]: subscription api closed in 0 ms \\nI 2020-07-30T05:02:58.297763Z Closing Broker-1 [7/10]: command api handler \\nD 2020-07-30T05:02:58.298951Z Closing Broker-1 [7/10]: command api handler closed in 0 ms \\nI 2020-07-30T05:02:58.299366Z Closing Broker-1 [8/10]: command api transport \\nI 2020-07-30T05:03:00.320886Z Stopped \\nD 2020-07-30T05:03:00.321691Z Closing Broker-1 [8/10]: command api transport closed in 2022 ms \\nI 2020-07-30T05:03:00.322220Z Closing Broker-1 [9/10]: membership and replication protocol \\nI 2020-07-30T05:03:00.323776Z RaftServer{raft-partition-partition-3} - Transitioning to INACTIVE \\nI 2020-07-30T05:03:00.324179Z RaftServer{raft-partition-partition-2} - Transitioning to INACTIVE \\nI 2020-07-30T05:03:00.324205Z RaftServer{raft-partition-partition-1} - Transitioning to INACTIVE \\nI 2020-07-30T05:03:00.343039Z Stopped \\nI 2020-07-30T05:03:00.344506Z Stopped \\nI 2020-07-30T05:03:00.345422Z Stopped \\nI 2020-07-30T05:03:00.346547Z Stopped \\nI 2020-07-30T05:03:00.347267Z 1 - Member deactivated: Member{id=1, address=zeebe-chaos-zeebe-1.zeebe-chaos-zeebe.zeebe-chaos.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAQABAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlJAAAAemVlYmUtY2hhb3MtemVlYmUtMS56ZWViZS1jaGFvcy16ZWViZS56ZWViZS1jaGFvcy5zdmMuY2x1c3Rlci5sb2NhbDoyNjUwMQUAAAwAAA8AAAAwLjI1LjAtU05BUFNIT1Q=}} \\nI 2020-07-30T05:03:00.347747Z Stopped \\nI 2020-07-30T05:03:00.348326Z Left \\nI 2020-07-30T05:03:00.349083Z Stopped \\nI 2020-07-30T05:03:04.364059Z Stopped \\nI 2020-07-30T05:03:04.364994Z Stopped \\nD 2020-07-30T05:03:04.365955Z Closing Broker-1 [9/10]: membership and replication protocol closed in 4043 ms \\nI 2020-07-30T05:03:04.366433Z Closing Broker-1 [10/10]: actor scheduler \\nD 2020-07-30T05:03:04.366879Z Closing blocking task runner \\nD 2020-07-30T05:03:04.367217Z Closing actor thread ground \'Broker-1-zb-fs-workers\' \\nD 2020-07-30T05:03:04.368641Z Closing actor thread ground \'Broker-1-zb-fs-workers\': closed successfully \\nD 2020-07-30T05:03:04.369018Z Closing actor thread ground \'Broker-1-zb-actors\' \\nD 2020-07-30T05:03:04.369955Z Closing blocking task runner: closed successfully \\nD 2020-07-30T05:03:04.370112Z Closing actor thread ground \'Broker-1-zb-actors\': closed successfully \\nD 2020-07-30T05:03:04.371034Z Closing Broker-1 [10/10]: actor scheduler closed in 4 ms \\nI 2020-07-30T05:03:04.371414Z Closing Broker-1 succeeded. Closed 10 steps in 6080 ms. \\nE 2020-07-30T05:03:04.371769Z Failed to start broker 1! \\n```\\nThis is works without problems. I think this is good to know.\\n\\n\\n### Code\\n\\nTo deploy a benchmark without exporters we had to do a `helm template` and remove all exporter related env variables.\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/07/16/big-multi-instance","metadata":{"permalink":"/zeebe-chaos/2020/07/16/big-multi-instance","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-07-16-big-multi-instance/index.md","source":"@site/blog/2020-07-16-big-multi-instance/index.md","title":"Big Multi Instance","description":"* investigate and fix automated chaos experiments - works again with 88c404f and cd8d685","date":"2020-07-16T00:00:00.000Z","tags":[],"readingTime":2.82,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Big Multi Instance","date":"2020-07-16T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment without Exporters","permalink":"/zeebe-chaos/2020/07/30/experiment-without-exporters"},"nextItem":{"title":"Experiment with Timers and Huge Variables","permalink":"/zeebe-chaos/2020/07/09/timer-and-huge-variables"}},"content":"* investigate and fix automated chaos experiments - works again with [88c404f](https://github.com/camunda/zeebe-chaos/commit/88c404f97514d4a7a511ce9751085acdd1720cd9) and [cd8d685](https://github.com/camunda/zeebe-chaos/commit/cd8d685b83eaa1ac9050ad3d16868389e1c0c36d)\\n * Closed some issues in the backlog\\n * Run a chaos experiment with bigger multi instance to reach `maxMessageSize`\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n We wanted to run a chaos experiment, which covers [#33](https://github.com/camunda/zeebe-chaos/issues/33).\\n\\n### Expected\\n\\n If we reach `maxMessageSize` in a workflow instance, for example via variables we expect that an incident is created or at least an error event and we can see that in operate. Furthermore we expect that the partition processing is not stopped, which means we can still create new instances.\\n\\n### Actual\\n\\n The experiment uses parallel multiInstance service tasks, to create a lot of tasks which should be completed with big variables.\\n\\n ![multiInstance](multiInstance.png)\\n\\n On collecting the output the `maxMessageSize` should be reached and we expected either an incident or exception for this instance. This should not affect other workflow instance creations.\\n\\n ![overview](overview.png)\\n\\n In operate we can see that we have two running workflow instances, one was started after the first failed. Later we created multiple instance\'s in a loop, without issues. This means we are not breaking partition processing, otherwise we would see timeouts.\\n \\n The problem we have is that we are not able to see in Operate that the workflow instance is actually broken. We have no indication for that.\\n\\n ![broken-multi](broken-multi.png)\\n\\n The instances seem still to be in starting the multi instance, but actually they are already blacklisted. If we check the logs we can find the following:\\n\\n```\\nI 2020-07-16T13:05:20.630315Z Error event was committed, we continue with processing. \\nE 2020-07-16T13:05:26.699139Z Expected to write one or more follow up events for event \'LoggedEvent [type=0, version=0, streamId=2, position=567, key=4503599627370769, timestamp=1594904720629, sourceEventPosition=565]\' without errors, but exception was thrown. \\nE 2020-07-16T13:05:26.735966Z Expected to process event \'TypedEventImpl{metadata=RecordMetadata{recordType=EVENT, intentValue=255, intent=ELEMENT_ACTIVATED, requestStreamId=-2147483648, requestId=-1, protocolVersion=1, valueType=WORKFLOW_INSTANCE, rejectionType=NULL_VAL, rejectionReason=}, value={\\"bpmnProcessId\\":\\"chaos\\",\\"version\\":2,\\"workflowKey\\":2251799813685359,\\"workflowInstanceKey\\":4503599627370761,\\"elementId\\":\\"chaosTask\\",\\"flowScopeKey\\":4503599627370769,\\"bpmnElementType\\":\\"SERVICE_TASK\\",\\"parentWorkflowInstanceKey\\":-1,\\"parentElementInstanceKey\\":-1}}\' without errors, but exception occurred with message \'Expected to claim segment of size 8439432, but can\'t claim more than 4194304 bytes.\' . \\nW 2020-07-16T13:05:26.737532Z Blacklist workflow instance 4503599627370761, due to previous errors. \\nI 2020-07-16T13:05:26.738421Z Error record was written at 568, we will continue with processing if event was committed. Current commit position is 567. \\nI 2020-07-16T13:05:26.793523Z Error event was committed, we continue with processing.\\n\\n```\\n\\nI created a feature request for operate [OPE-1037](https://jira.camunda.com/browse/OPE-1037)\\n\\nIn general chaos experiment succeeded, since we not breaking processing and we are still able to process other instances, but we only see that the instance is blacklisted in the logs not in operate, which is a problem from my POV. Furthermore a bit unexpected was, that we already failed before, we are not able to activate jobs, since the size of the multi instance was already to big.\\n\\n### Code\\n\\n```csharp\\n    // create zeebe client\\n    var client = ZeebeClient.Builder()\\n        .UseLoggerFactory(new NLogLoggerFactory())\\n        .UseGatewayAddress(ZeebeUrl)\\n        .UseTransportEncryption()\\n        .UseAccessTokenSupplier(\\n            CamundaCloudTokenProvider.Builder()\\n                .UseAuthServer(AuthServer)\\n                .UseClientId(ClientId)\\n                .UseClientSecret(ClientSecret)\\n                .UseAudience(Audience)\\n                .Build())\\n        .Build();\\n\\n    var topology = await client.TopologyRequest().Send();\\n\\n    Logger.Info(topology.ToString);\\n    Console.WriteLine(topology);\\n    // deploy\\n    var deployResponse = await client.NewDeployCommand()\\n        .AddResourceFile(DemoProcessPath)\\n        .Send();\\n\\n    // create workflow instance\\n    var intArray = Enumerable.Range(0, 10_000).ToArray();\\n    var jsonObject = new {list = intArray};\\n    var jsonString = JsonConvert.SerializeObject(jsonObject);\\n\\n    await client.NewCreateWorkflowInstanceCommand()\\n                .BpmnProcessId(\\"chaos\\").LatestVersion()\\n                .Variables(jsonString)\\n                .Send();\\n\\n    // open job worker\\n    using (var signal = new EventWaitHandle(false, EventResetMode.AutoReset))\\n    {\\n        client.NewWorker()\\n              .JobType(JobType)\\n              .Handler(HandleJob)\\n              .MaxJobsActive(120)\\n              .Name(WorkerName)\\n              .AutoCompletion()\\n              .PollInterval(TimeSpan.FromMilliseconds(100))\\n              .Timeout(TimeSpan.FromSeconds(10))\\n              .PollingTimeout(TimeSpan.FromSeconds(30))\\n              .Open();\\n\\n        // blocks main thread, so that worker can run\\n        signal.WaitOne();\\n    }\\n\\nprivate static void HandleJob(IJobClient jobClient, IJob job)\\n{\\n    Logger.Debug(\\"Handle job {job}\\", job.Key);\\n\\n    var bigPayload = File.ReadAllText(PayloadPath);\\n    jobClient.NewCompleteJobCommand(job).Variables(bigPayload).Send();\\n}\\n  \\n```\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/07/09/timer-and-huge-variables","metadata":{"permalink":"/zeebe-chaos/2020/07/09/timer-and-huge-variables","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-07-09-timer-and-huge-variables/index.md","source":"@site/blog/2020-07-09-timer-and-huge-variables/index.md","title":"Experiment with Timers and Huge Variables","description":"* Failure documentation about RAFT","date":"2020-07-09T00:00:00.000Z","tags":[],"readingTime":3.43,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Experiment with Timers and Huge Variables","date":"2020-07-09T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Big Multi Instance","permalink":"/zeebe-chaos/2020/07/16/big-multi-instance"},"nextItem":{"title":"Extract K8 resources from namespace","permalink":"/zeebe-chaos/2020/07/02/extract-k8-resources"}},"content":"* Failure documentation about RAFT\\n * Added chaos day summaries to repo\\n * Run Chaos experiment with a lot of timers\\n * Run Chaos experiment with huge variables\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n### A Lot of Timers \\n\\nBased on the Hypothesis written here:  [#31](https://github.com/camunda/zeebe-chaos/issues/31) we run an experiment with a stable load of 10 simple workflow instances per second (only start and end event) and 10 workflow instances with \\nmultiple timers. We wanted to explore what happens when we have a lot of timers running and especially what happens when the are triggered at once. We created the following workflow model, where timers are exponentially created.\\n\\n![timerProcess](timerProcess.png)\\n\\nThe experiments is based on the hypotheses we wrote here [#31](https://github.com/camunda/zeebe-chaos/issues/31).\\n\\n#### Expectations \\n\\n * at some point we can\'t create new instances from out side, since backpressure will block that\\n * the metrics for processing records (events/commands) should be stable since there will be always new events/records\\n * the cluster itself should be stable\\n\\n\\n#### Observations\\n\\nDuring running the experiments we saw that indeed we were not able to create new instances at some point.\\nThe cluster kept stable and there was no leader change at all. Unexpected was the behavior of the processing record metrics, since it fluctuates a lot.\\n\\nFurthermore we reached really high processing records throughput, which we normally not see.\\n\\n![overall](overall.png)\\n\\nThe log appender backpressure seem to work, at some point it deferred around 1.3 million records.\\n\\nOn resource consumption side it seems that memory is growing, but we create also more timer might be the issue.\\n\\n![mem](mem.png)\\n\\nInteresting was that we saw a huge difference between processing and exporting.\\n\\n![exportvsprocess](exportvsprocess.png) \\n\\nThe issue we currently have is that we stop processing to trigger/evaluate due timers and write them to the log.\\nAfter we did that we will process a bunch of events again and then trigger/evaluate again. I think this should be decoupled to streamline the processing throughput. I created a new issue for that [#4931](https://github.com/zeebe-io/zeebe/issues/4931)\\n\\n### Huge Variables\\n\\nIn order to understand better what is the impact of huge variables I did a small experiment with a payload which was ~ 5 MB.\\n\\nI expected that this will not work, since this is larger then the maximum message size. I would expect an appropriate error message, but unfortunately I just got a cancel on the client side.\\n\\n\\nStarter Exception:\\n```\\n2020-07-09T12:27:20.945889183Z 12:27:20.945 [Thread-2] WARN  io.zeebe.ResponseChecker - Request failed\\n I \\n2020-07-09T12:27:20.945909759Z java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: CANCELLED: HTTP/2 error code: CANCEL\\n I \\n2020-07-09T12:27:20.945924206Z Received Rst Stream\\n I \\n2020-07-09T12:27:20.945929123Z \\tat java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]\\n I \\n2020-07-09T12:27:20.945934351Z \\tat java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]\\n I \\n2020-07-09T12:27:20.945939157Z \\tat io.zeebe.ResponseChecker.run(ResponseChecker.java:41) [app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.945972595Z Caused by: io.grpc.StatusRuntimeException: CANCELLED: HTTP/2 error code: CANCEL\\n I \\n2020-07-09T12:27:20.945978663Z Received Rst Stream\\n I \\n2020-07-09T12:27:20.945983657Z \\tat io.grpc.Status.asRuntimeException(Status.java:533) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.945988515Z \\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:449) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.945993803Z \\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.945998591Z \\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946003749Z \\tat me.dinowernli.grpc.prometheus.MonitoringClientCallListener.onClose(MonitoringClientCallListener.java:50) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946007316Z \\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:426) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946010626Z \\tat io.grpc.internal.ClientCallImpl.access$500(ClientCallImpl.java:66) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946013822Z \\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:689) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946017026Z \\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$900(ClientCallImpl.java:577) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946020414Z \\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:751) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946025498Z \\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:740) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946030508Z \\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946035265Z \\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~[app.jar:0.23.3]\\n I \\n2020-07-09T12:27:20.946040Z \\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\\n I \\n2020-07-09T12:27:20.946045833Z \\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\\n I \\n2020-07-09T12:27:20.946050880Z \\tat java.lang.Thread.run(Unknown Source) ~[?:?]\\n I \\n```\\n\\n\\nInteresting is what happens on the Gateway:\\n\\n```\\n2020-07-09 14:27:33.929 CEST\\nStream Error\\n \\nExpand all | Collapse all{\\n insertId: \\"yoxfczvg9usifqd7d\\"  \\n jsonPayload: {\\n  context: {\u2026}   \\n  exception: \\"io.netty.handler.codec.http2.Http2Exception$StreamException: Received DATA frame for an unknown stream 10277\\n\\tat io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:147) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.shouldIgnoreHeadersOrDataFrame(DefaultHttp2ConnectionDecoder.java:596) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:239) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:422) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:174) ~[netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:378) [netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:438) [netty-codec-http2-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) [netty-codec-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) [netty-codec-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) [netty-codec-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:792) [netty-transport-native-epoll-4.1.50.Final-linux-x86_64.jar:4.1.50.Final]\\n\\tat io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$1.run(AbstractEpollChannel.java:387) [netty-transport-native-epoll-4.1.50.Final-linux-x86_64.jar:4.1.50.Final]\\n\\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [netty-transport-native-epoll-4.1.50.Final-linux-x86_64.jar:4.1.50.Final]\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.50.Final.jar:4.1.50.Final]\\n\\tat java.lang.Thread.run(Unknown Source) [?:?]\\n\\"   \\n  logger: \\"io.grpc.netty.NettyServerHandler\\"   \\n  message: \\"Stream Error\\"   \\n  serviceContext: {\u2026}   \\n  thread: \\"grpc-default-worker-ELG-1-1\\"   \\n }\\n\\n```\\n\\nCreated an issue for this: [#4928](https://github.com/zeebe-io/zeebe/issues/4928)\\n\\nI will probably continue with this next week.\\n\\n## Participants\\n\\n * @zelldon"},{"id":"/2020/07/02/extract-k8-resources","metadata":{"permalink":"/zeebe-chaos/2020/07/02/extract-k8-resources","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-07-02-extract-k8-resources/index.md","source":"@site/blog/2020-07-02-extract-k8-resources/index.md","title":"Extract K8 resources from namespace","description":"* Research: Read about DiRT (Disaster Recovery Tests) @ google - gave me same new ideas for more game days","date":"2020-07-02T00:00:00.000Z","tags":[],"readingTime":1.02,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Extract K8 resources from namespace","date":"2020-07-02T00:00:00.000Z","categories":["kubernetes"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment with Timers and Huge Variables","permalink":"/zeebe-chaos/2020/07/09/timer-and-huge-variables"},"nextItem":{"title":"Gateway Network Partition","permalink":"/zeebe-chaos/2020/06/25/gateway-network-partition"}},"content":"* Research: Read about DiRT (Disaster Recovery Tests) @ google - gave me same new ideas for more game days\\n * Failure documentation about Log Appender\\n\\nUnfortunately I had no time today for new chaos experiment, but I spent with @pihme some time to investigate how we can run the cluster plans in our gke.\\nWe did a bit of progress. I\'m finally able to create cluster plans in the ultratest and can extract all resource definitions via command line.\\n\\n```shell\\nkubectl get pvc,configmap,service,deployment,statefulset,cronjob,storageclasses -o yaml --export | sed -e \'/resourceVersion: \\"[0-9]\\\\+\\"/d\' -e \'/uid: [a-z0-9-]\\\\+/d\' -e \'/selfLink: [a-z0-9A-Z/]\\\\+/d\' -e \'/status:/d\' -e \'/phase:/d\' -e \'/creationTimestamp:/d\' > s-cluster.yaml\\n```\\n\\nWe now need to find a way to successfully deploy it in our cluster - it haven\'t been successful yet. We thought about using kustomize to adjust some values they use.\\nMuch easier would be to just deploy the operator they use in our gke cloud and use the CRD to deploy the cluster plans. I think we need to investigate a bit more here what is the best approach. In the end I would like to run our chaos experiments against clusters which correspond to the real deployed ones.\\n\\n\x3c!--truncate--\x3e\\n\\n## Participants\\n\\n * @pihme\\n * @zelldon"},{"id":"/2020/06/25/gateway-network-partition","metadata":{"permalink":"/zeebe-chaos/2020/06/25/gateway-network-partition","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-06-25-gateway-network-partition/index.md","source":"@site/blog/2020-06-25-gateway-network-partition/index.md","title":"Gateway Network Partition","description":"* Documented failure cases for AsyncSnasphortDirector. Gave me some ideas where it might make sense to reinstall partition. Discussed a bit with @Deepthi","date":"2020-06-25T00:00:00.000Z","tags":[],"readingTime":2.34,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Gateway Network Partition","date":"2020-06-25T00:00:00.000Z","categories":["chaos_experiment","gateway"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Extract K8 resources from namespace","permalink":"/zeebe-chaos/2020/07/02/extract-k8-resources"},"nextItem":{"title":"Correlate Message after failover","permalink":"/zeebe-chaos/2020/06/18/correlate-message-after-failover"}},"content":"* Documented failure cases for AsyncSnasphortDirector. Gave me some ideas where it might make sense to reinstall partition. Discussed a bit with @Deepthi\\n * Still our automated chaos experiments are not running. I need some time for that, but I had no time for that today.\\n * Run a chaos experiment together with @pihme, where we do a network partition with the gateway.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos experiment:\\n\\nActually we already have an network partition experiment with the standalone Gateway, where we completely isolate the gateway and take a look whether it comes back after the network partition. Today we wanted to explore how it behaves when only one node and the gateway has a network partition, so Broker 0 and Gateway can\'t talk to each other.\\n\\n### Expected during the experiment:\\n\\nthe topology stays the same, since gateway can ping indirectly (is discussable whether this is ideal or not)\\nwhen Broker 0 is leader for a partition then the processing for that partition stops but other partitions should not be affected\\nWe can somehow determine in the metrics that they can\'t connect to each other\\nAfter connecting again the affected partition should recover\\n\\n### Observations:\\n\\nAs expected we see no difference in the Topology. All commands which are send to that partition time out. Other partitions haven\'t been affected :+1: With the metrics we have we seen that: there is no progress in the partition, the partition is still healthy (which makes sense) and we see a lot of timeouts happening.\\n\\nUnfortunately we need multiple metrics to correlate somehow that it might be due to connectivity issues. I think we can improve here. For example it is not directly visible that one partition stopped processing. For that @Peter Ihme had a good idea and we will add a new panel, which directly shows the current record processing stats. I think this is also useful for exporting to directly see whether we have currently exporting problems. Check the attached image.\\n\\nWhat else is missing on the metrics side from my point of view:\\n\\n * a panel which shows me that all requests to a specific partition currently time out.\\n * metrics for the transport between gateway and broker to better analyze problems like that. Would be nice to have [#4487](https://github.com/zeebe-io/zeebe/issues/4487) \\n  * Liveness and Health stats of the Gateway in the metrics. I think this is currently not supported?\\n\\nAfter reconnecting the nodes we saw that the related partition started to process again. Interesting was that it seems that there piled some traffic up and after reconnecting we saw a burst against partition one (partition 2 was disconnected), but this caused no issues.\\n\\nI think was good and interesting experiment again and gave us a bit more insights what else we need.\\n\\n\\n![feedback](feedback.png)\\n\\n![reduce2](reduce2.png)\\n\\n## Participants\\n\\n * @pihme\\n * @zelldon"},{"id":"/2020/06/18/correlate-message-after-failover","metadata":{"permalink":"/zeebe-chaos/2020/06/18/correlate-message-after-failover","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-06-18-correlate-message-after-failover/index.md","source":"@site/blog/2020-06-18-correlate-message-after-failover/index.md","title":"Correlate Message after failover","description":"* Documented failure cases for engine and stream processor. I think almost all possible failure cases I can think of we already handle, except problems on reading, which I think can\'t be handled.","date":"2020-06-18T00:00:00.000Z","tags":[],"readingTime":0.91,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Correlate Message after failover","date":"2020-06-18T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Gateway Network Partition","permalink":"/zeebe-chaos/2020/06/25/gateway-network-partition"},"nextItem":{"title":"High CPU load on Standalone Gateway","permalink":"/zeebe-chaos/2020/06/11/high-cpu-gateway"}},"content":"* Documented failure cases for engine and stream processor. I think almost all possible failure cases I can think of we already handle, except problems on reading, which I think can\'t be handled.\\n* Checked what the current issue is with the automated chaos experiments. It seems it is a infra problem. You can check the discussion in #infra. It might be affected due to [Infra-1292](https://jira.camunda.com/browse/INFRA-1292)\\n* Run a chaos experiment, where we correlate a message after fail over.\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos Experiment\\n\\n* Start our normal setup and deploy a workflow with an intermediate message catch event.\\n* Publish a message and kill a random broker.\\n* Create a workflow instance and await the result.\\n\\nI did this experiment several times and it works without any problems, as far as I can tell. First I was wondering that the message was only correlated to one instance, but this seems to be expected [message-correlation.html#message-cardinality](https://docs.zeebe.io/reference/message-correlation/message-correlation.html#message-cardinality) So learned something new today about our messages :grin:.\\n\\nI prepared already an automatable chaos experiment for that. Have to fine tune it a bit.\\n\\nNo pictures today."},{"id":"/2020/06/11/high-cpu-gateway","metadata":{"permalink":"/zeebe-chaos/2020/06/11/high-cpu-gateway","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-06-11-high-cpu-gateway/index.md","source":"@site/blog/2020-06-11-high-cpu-gateway/index.md","title":"High CPU load on Standalone Gateway","description":"* Updated failure cases documentation for exporter based on review","date":"2020-06-11T00:00:00.000Z","tags":[],"readingTime":1.985,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"High CPU load on Standalone Gateway","date":"2020-06-11T00:00:00.000Z","categories":["chaos_experiment","gateway"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Correlate Message after failover","permalink":"/zeebe-chaos/2020/06/18/correlate-message-after-failover"},"nextItem":{"title":"First Chaos Day!","permalink":"/zeebe-chaos/2020/06/04/first-chaos-day"}},"content":"* Updated failure cases documentation for exporter based on review\\n * Documented failure cases for ZeebeDB\\n * Wrote an chaostoolkit experiment based on the last manual Chaos experiment\\n * Run a chaos experiment with @Deepthi, where we put high CPU load on the standalone gateway https://github.com/camunda/zeebe-chaos/issues/28\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos experiment:\\n\\nWe did today an chaos experiment where we used our standard setup with a baseline load of 100 workflow instance and 6 workers, which can activate 120 jobs max.\\nOn our steady state we saw that we are able to start and complete 100 workflow instances in a second. One instance took 1 - 2.5 seconds.\\n\\nWe expected when we introduce stress on the standalone gateway CPU that the latency of the processing goes up and the throughput goes down, but there should be no cluster wide failures happening. We expected that after removing the stress the system should come back to normal and the baseline should be reached again.\\n\\n![/assets/2020-06-11/gw-stress-proc](gw-stress-proc.png)\\n![/assets/2020-06-11/gw-cpu](gw-cpu.png)\\n![/assets/2020-06-11/gw-stress-proc-latency](gw-stress-proc-latency.png)\\n\\nThe results look promising. We have seen no outage.\\nWe tested it twice and saw that the throughput goes down and latency up on stress, but comes back to normal after removing it.\\n\\n### What was unexpected or what we found out:\\n\\nUnexpected was that our Broker back pressure goes up, which means it drops requests during the stress time. This was not expected, since the latency between writing to dispatcher and processing the event should not change. We probably need to investigate this more. Current assumption is that the gateway sends requests in batches and this causes in higher spikes on the back pressure. We need more metrics on the transport module to verify that. There is already an open issue for that https://github.com/zeebe-io/zeebe/issues/4487 We might need to tackle this, before we can find out more here.\\n\\nWe found out that the standalone gateway is not resource limited, which caused that we used at some point 12 CPU cores. It seems there is also an open issue for that on the helm charts https://github.com/zeebe-io/zeebe-cluster-helm/issues/74\\n\\nWe want to improve on our current chaos toolkit test. We want to introduce failures and verify the steady state during the failure is happening, on rollback we should remove the failure again. We currently just verify that we can recover, but not the behavior during a failure, which might be also interesting.\\n\\n## Participants\\n\\n * @deepthidevaki\\n * @zelldon"},{"id":"/2020/06/04/first-chaos-day","metadata":{"permalink":"/zeebe-chaos/2020/06/04/first-chaos-day","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2020-06-04-first-chaos-day/index.md","source":"@site/blog/2020-06-04-first-chaos-day/index.md","title":"First Chaos Day!","description":"First Chaos day","date":"2020-06-04T00:00:00.000Z","tags":[],"readingTime":1.095,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"First Chaos Day!","date":"2020-06-04T00:00:00.000Z","categories":["chaos_experiment","broker"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"High CPU load on Standalone Gateway","permalink":"/zeebe-chaos/2020/06/11/high-cpu-gateway"}},"content":"First Chaos day :tada:\\n\\n * Documented failure cases for exporter (already some exist, it seemed) gave me a new idea for ZEP\\n * Introduced Peter to our Chaos Repository, discussed a bit about the hypothesis backlog, reopened the Chaos Trello board where we will organize ourselves\\n * Run a chaos experiment, where we put high CPU load on the Leader [#6](https://github.com/camunda/zeebe-chaos/issues/6)\\n\\n\x3c!--truncate--\x3e\\n\\n## Chaos experiment:\\n\\n * We wanted to decrease the blast radius with only one partition, but we found an bug where this seemed not to be possible [#4664](https://github.com/zeebe-io/zeebe/issues/4664)\\n * We run the experiment with 2 partitions and put really high CPU load on the Leader (internally in the pod), we expected that this will not impact the complete cluster. That at most we have a leader change because the leader is not able to send heartbeats in time. After removing the cpu load we should be back on our throughput base line, where we start and complete around 70 - 80 workflow instances per second.\\n * The results where quite promising we had no leader change at all. The leader was able to send heartbeats in time and the backpressure did a good job and drop more requests. After reducing the cpu load we went back to our steady state. \\n\\n![img](result-chaos.png)\\n\\n## Participants\\n\\n * @pihme\\n * @zelldon"}]}}')}}]);