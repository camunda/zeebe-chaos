"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[7775],{27748:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var a=n(49705),i=n(74848),s=n(28453);const o={layout:"posts",title:"Big Multi Instance",date:new Date("2020-07-16T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],authors:"zell"},r="Chaos Day Summary",c={authorsImageUrls:[void 0]},l=[{value:"Chaos Experiment",id:"chaos-experiment",level:2},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"Code",id:"code",level:3},{value:"Participants",id:"participants",level:2}];function d(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["investigate and fix automated chaos experiments - works again with ",(0,i.jsx)(t.a,{href:"https://github.com/zeebe-io/zeebe-chaos/commit/88c404f97514d4a7a511ce9751085acdd1720cd9",children:"88c404f"})," and ",(0,i.jsx)(t.a,{href:"https://github.com/zeebe-io/zeebe-chaos/commit/cd8d685b83eaa1ac9050ad3d16868389e1c0c36d",children:"cd8d685"})]}),"\n",(0,i.jsx)(t.li,{children:"Closed some issues in the backlog"}),"\n",(0,i.jsxs)(t.li,{children:["Run a chaos experiment with bigger multi instance to reach ",(0,i.jsx)(t.code,{children:"maxMessageSize"})]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"chaos-experiment",children:"Chaos Experiment"}),"\n",(0,i.jsxs)(t.p,{children:["We wanted to run a chaos experiment, which covers ",(0,i.jsx)(t.a,{href:"https://github.com/zeebe-io/zeebe-chaos/issues/33",children:"#33"}),"."]}),"\n",(0,i.jsx)(t.h3,{id:"expected",children:"Expected"}),"\n",(0,i.jsxs)(t.p,{children:["If we reach ",(0,i.jsx)(t.code,{children:"maxMessageSize"})," in a workflow instance, for example via variables we expect that an incident is created or at least an error event and we can see that in operate. Furthermore we expect that the partition processing is not stopped, which means we can still create new instances."]}),"\n",(0,i.jsx)(t.h3,{id:"actual",children:"Actual"}),"\n",(0,i.jsx)(t.p,{children:"The experiment uses parallel multiInstance service tasks, to create a lot of tasks which should be completed with big variables."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"multiInstance",src:n(20730).A+"",width:"820",height:"553"})}),"\n",(0,i.jsxs)(t.p,{children:["On collecting the output the ",(0,i.jsx)(t.code,{children:"maxMessageSize"})," should be reached and we expected either an incident or exception for this instance. This should not affect other workflow instance creations."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"overview",src:n(13635).A+"",width:"1888",height:"447"})}),"\n",(0,i.jsx)(t.p,{children:"In operate we can see that we have two running workflow instances, one was started after the first failed. Later we created multiple instance's in a loop, without issues. This means we are not breaking partition processing, otherwise we would see timeouts."}),"\n",(0,i.jsx)(t.p,{children:"The problem we have is that we are not able to see in Operate that the workflow instance is actually broken. We have no indication for that."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"broken-multi",src:n(37177).A+"",width:"1732",height:"731"})}),"\n",(0,i.jsx)(t.p,{children:"The instances seem still to be in starting the multi instance, but actually they are already blacklisted. If we check the logs we can find the following:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'I 2020-07-16T13:05:20.630315Z Error event was committed, we continue with processing. \nE 2020-07-16T13:05:26.699139Z Expected to write one or more follow up events for event \'LoggedEvent [type=0, version=0, streamId=2, position=567, key=4503599627370769, timestamp=1594904720629, sourceEventPosition=565]\' without errors, but exception was thrown. \nE 2020-07-16T13:05:26.735966Z Expected to process event \'TypedEventImpl{metadata=RecordMetadata{recordType=EVENT, intentValue=255, intent=ELEMENT_ACTIVATED, requestStreamId=-2147483648, requestId=-1, protocolVersion=1, valueType=WORKFLOW_INSTANCE, rejectionType=NULL_VAL, rejectionReason=}, value={"bpmnProcessId":"chaos","version":2,"workflowKey":2251799813685359,"workflowInstanceKey":4503599627370761,"elementId":"chaosTask","flowScopeKey":4503599627370769,"bpmnElementType":"SERVICE_TASK","parentWorkflowInstanceKey":-1,"parentElementInstanceKey":-1}}\' without errors, but exception occurred with message \'Expected to claim segment of size 8439432, but can\'t claim more than 4194304 bytes.\' . \nW 2020-07-16T13:05:26.737532Z Blacklist workflow instance 4503599627370761, due to previous errors. \nI 2020-07-16T13:05:26.738421Z Error record was written at 568, we will continue with processing if event was committed. Current commit position is 567. \nI 2020-07-16T13:05:26.793523Z Error event was committed, we continue with processing.\n\n'})}),"\n",(0,i.jsxs)(t.p,{children:["I created a feature request for operate ",(0,i.jsx)(t.a,{href:"https://jira.camunda.com/browse/OPE-1037",children:"OPE-1037"})]}),"\n",(0,i.jsx)(t.p,{children:"In general chaos experiment succeeded, since we not breaking processing and we are still able to process other instances, but we only see that the instance is blacklisted in the logs not in operate, which is a problem from my POV. Furthermore a bit unexpected was, that we already failed before, we are not able to activate jobs, since the size of the multi instance was already to big."}),"\n",(0,i.jsx)(t.h3,{id:"code",children:"Code"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-csharp",children:'    // create zeebe client\n    var client = ZeebeClient.Builder()\n        .UseLoggerFactory(new NLogLoggerFactory())\n        .UseGatewayAddress(ZeebeUrl)\n        .UseTransportEncryption()\n        .UseAccessTokenSupplier(\n            CamundaCloudTokenProvider.Builder()\n                .UseAuthServer(AuthServer)\n                .UseClientId(ClientId)\n                .UseClientSecret(ClientSecret)\n                .UseAudience(Audience)\n                .Build())\n        .Build();\n\n    var topology = await client.TopologyRequest().Send();\n\n    Logger.Info(topology.ToString);\n    Console.WriteLine(topology);\n    // deploy\n    var deployResponse = await client.NewDeployCommand()\n        .AddResourceFile(DemoProcessPath)\n        .Send();\n\n    // create workflow instance\n    var intArray = Enumerable.Range(0, 10_000).ToArray();\n    var jsonObject = new {list = intArray};\n    var jsonString = JsonConvert.SerializeObject(jsonObject);\n\n    await client.NewCreateWorkflowInstanceCommand()\n                .BpmnProcessId("chaos").LatestVersion()\n                .Variables(jsonString)\n                .Send();\n\n    // open job worker\n    using (var signal = new EventWaitHandle(false, EventResetMode.AutoReset))\n    {\n        client.NewWorker()\n              .JobType(JobType)\n              .Handler(HandleJob)\n              .MaxJobsActive(120)\n              .Name(WorkerName)\n              .AutoCompletion()\n              .PollInterval(TimeSpan.FromMilliseconds(100))\n              .Timeout(TimeSpan.FromSeconds(10))\n              .PollingTimeout(TimeSpan.FromSeconds(30))\n              .Open();\n\n        // blocks main thread, so that worker can run\n        signal.WaitOne();\n    }\n\nprivate static void HandleJob(IJobClient jobClient, IJob job)\n{\n    Logger.Debug("Handle job {job}", job.Key);\n\n    var bigPayload = File.ReadAllText(PayloadPath);\n    jobClient.NewCompleteJobCommand(job).Variables(bigPayload).Send();\n}\n  \n'})}),"\n",(0,i.jsx)(t.h2,{id:"participants",children:"Participants"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"@zelldon"}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},37177:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/broken-multi-0df164bbc4633660c5926c2d9caa9b84.png"},20730:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/multiInstance-34834f33ef17ba5beebfe4801420587d.png"},13635:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/overview-7c0f42db61db06446bc93d8a94b04fcf.png"},28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>r});var a=n(96540);const i={},s=a.createContext(i);function o(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:t},e.children)}},49705:e=>{e.exports=JSON.parse('{"permalink":"/zeebe-chaos/2020/07/16/big-multi-instance","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2020-07-16-big-multi-instance/index.md","source":"@site/blog/2020-07-16-big-multi-instance/index.md","title":"Big Multi Instance","description":"* investigate and fix automated chaos experiments - works again with 88c404f and cd8d685","date":"2020-07-16T00:00:00.000Z","tags":[],"readingTime":2.82,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Big Multi Instance","date":"2020-07-16T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Experiment without Exporters","permalink":"/zeebe-chaos/2020/07/30/experiment-without-exporters"},"nextItem":{"title":"Experiment with Timers and Huge Variables","permalink":"/zeebe-chaos/2020/07/09/timer-and-huge-variables"}}')}}]);