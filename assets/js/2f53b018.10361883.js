"use strict";(globalThis.webpackChunkzell_chaos=globalThis.webpackChunkzell_chaos||[]).push([[7778],{28453(e,n,t){t.d(n,{R:()=>o,x:()=>i});var a=t(96540);const r={},s=a.createContext(r);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:n},e.children)}},37615(e){e.exports=JSON.parse('{"permalink":"/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-06-12-How-does-Zeebe-behave-with-NFS/index.md","source":"@site/blog/2025-06-12-How-does-Zeebe-behave-with-NFS/index.md","title":"How does Zeebe behave with NFS","description":"This week, we (Lena, Nicolas, Roman, and I) held a workshop where we looked into how Zeebe behaves with network file storage (NFS).","date":"2025-06-12T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":14.41,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Principal Software Engineer @ Camunda","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"How does Zeebe behave with NFS","date":"2025-06-12T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"Performance of REST API","permalink":"/zeebe-chaos/2025/06/30/Performance-of-REST-API"},"nextItem":{"title":"Lower memory consumption of Camunda deployment","permalink":"/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment"}}')},54999(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var a=t(37615),r=t(74848),s=t(28453);const o={layout:"posts",title:"How does Zeebe behave with NFS",date:new Date("2025-06-12T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},i="Chaos Day Summary",c={authorsImageUrls:[void 0]},l=[{value:"Setup",id:"setup",level:2},{value:"Run NFS Docker Container",id:"run-nfs-docker-container",level:3},{value:"Mount the NFS to local file storage",id:"mount-the-nfs-to-local-file-storage",level:3},{value:"Run the Zeebe Container",id:"run-the-zeebe-container",level:3},{value:"Running load",id:"running-load",level:3},{value:"Chaos Experiment - Use iptables with containerized NFS Server",id:"chaos-experiment---use-iptables-with-containerized-nfs-server",level:2},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"Chaos Experiment - Use iptables with an external NFS Server",id:"chaos-experiment---use-iptables-with-an-external-nfs-server",level:2},{value:"Setup external NFS",id:"setup-external-nfs",level:3},{value:"Mount external NFS",id:"mount-external-nfs",level:3},{value:"Run Zeebe Container",id:"run-zeebe-container",level:3},{value:"Expected",id:"expected-1",level:3},{value:"Actual",id:"actual-1",level:3},{value:"Chaos Experiment 3 - Random dropping packages",id:"chaos-experiment-3---random-dropping-packages",level:2},{value:"Expected",id:"expected-2",level:3},{value:"Actual",id:"actual-2",level:3},{value:"Chaos Experiment 4 - Drop connection on reading",id:"chaos-experiment-4---drop-connection-on-reading",level:2},{value:"Expected",id:"expected-3",level:3},{value:"Actual",id:"actual-3",level:3},{value:"Results",id:"results",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["This week, we (",(0,r.jsx)(n.a,{href:"https://github.com/lenaschoenburg",children:"Lena"}),", ",(0,r.jsx)(n.a,{href:"https://github.com/npepinpe",children:"Nicolas"}),", ",(0,r.jsx)(n.a,{href:"https://github.com/romansmirnov",children:"Roman"}),", and ",(0,r.jsx)(n.a,{href:"https://github.com/ChrisKujawa",children:"I"}),") held a workshop where we looked into how Zeebe behaves with network file storage (NFS)."]}),"\n",(0,r.jsx)(n.p,{children:"We ran several experiments with NFS and Zeebe, and messing around with connectivity."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"TL;DR;"})," We were able to show that NFS can handle certain connectivity issues, just causing Zeebe to process slower. IF we completely lose the connection to the NFS server, several issues can arise, like IOExceptions on flush (where RAFT goes into inactive mode) or SIGBUS errors on reading (like replay), causing the JVM to crash."]}),"\n",(0,r.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"Note:"}),"\n",(0,r.jsx)(n.p,{children:"You can skip this section if you're not interested in how we set up the NFS server"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"For our experiments, we want to have a quick feedback loop and small blast radius, meaning avoiding using K8, or any other cloud services. The idea was to set up a NFS server via docker, and mess with the network, to cause NFS errors."}),"\n",(0,r.jsx)(n.h3,{id:"run-nfs-docker-container",children:"Run NFS Docker Container"}),"\n",(0,r.jsxs)(n.p,{children:["After a smaller research we were able ",(0,r.jsx)(n.a,{href:"https://github.com/normal-computing/docker-nfs-server",children:"to find a project"}),", that provides us a NFS server docker image."]}),"\n",(0,r.jsx)(n.p,{children:"This can be run via:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"sudo podman run \\\n   # Needs privileged access for setting up the exports rule, etc\\\n  --privileged\n  # Mounting a local directory as volume into the container\n  -v /home/cqjawa/nfs-workshop/nfs:/mnt/data:rw  \\\n   # expose the NFS por\n  -p 2049:2049t \\\n   # Allowing the local host IP to access the NFS server \n  -e NFS_SERVER_ALLOWED_CLIENTS=10.88.0.0/12 \\\n   # Enable DEBUG LOGS\n  -e NFS_SERVER_DEBUG=1 \\ \n   ghcr.io/normal-computing/nfs-server:latest\n"})}),"\n",(0,r.jsx)(n.h3,{id:"mount-the-nfs-to-local-file-storage",children:"Mount the NFS to local file storage"}),"\n",(0,r.jsx)(n.p,{children:"To use the NFS server and make it available to our Zeebe container, we first have to mount it via the NFS client."}),"\n",(0,r.jsx)(n.p,{children:"This can be done via:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"sudo mount -v -t nfs4 \\\n  -o proto=tcp,port=2049,soft,timeo=10 \\\n  localhost:/ \\\n  ~/nfs-workshop/nfs-client-mount/\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"-v"})," verbose"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"-t"})," file system type: tells the client to use NFS4"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"-o"})," Options for the mount: ",(0,r.jsx)(n.code,{children:"proto=tcp,port=2049,soft,timeo=10"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Protocol options, like transport via ",(0,r.jsx)(n.code,{children:"tcp"}),", port to be used, ",(0,r.jsx)(n.a,{href:"https://kb.netapp.com/on-prem/ontap/da/NAS/NAS-KBs/What_are_the_differences_between_hard_mount_and_soft_mount",children:"soft mount"})," to make sure to retry on unavailability and not block, timeout after 10s"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"run-the-zeebe-container",children:"Run the Zeebe Container"}),"\n",(0,r.jsx)(n.p,{children:"After we mounted the NFS to our local filesystem, we can start our Zeebe container."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:" podman run -d \\\n   -v /home/cqjawa/nfs-workshop/nfs-client-mount/:/usr/local/zeebe/data \\\n   -p 26500:26500 \\\n   -p 9600:9600 \\\n   gcr.io/zeebe-io/zeebe:8.7.5-root\n"})}),"\n",(0,r.jsx)(n.p,{children:"This is mounting our NFS mounted directory into the container as the data directory for the Zeebe container."}),"\n",(0,r.jsx)(n.h3,{id:"running-load",children:"Running load"}),"\n",(0,r.jsxs)(n.p,{children:["For simplicity, we used ",(0,r.jsx)(n.code,{children:"zbctl"})," to start some load. As a first step, we had to deploy a process model."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:" zbctl --insecure deploy one_task.bpmn \n"})}),"\n",(0,r.jsxs)(n.p,{children:["This was using the ",(0,r.jsx)(n.a,{href:"https://github.com/camunda/zeebe-chaos/blob/main/go-chaos/internal/bpmn/one_task.bpmn",children:"one_task.bpmn"})," from ",(0,r.jsx)(n.code,{children:"go-chaos/"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Creating instances in a loop:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"while [[ true ]];\ndo \n    zbctl --insecure \\\n    create instance 2251799813685250;\n    sleep 5;\ndone\n"})}),"\n",(0,r.jsx)(n.p,{children:"Running worker:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:' zbctl --insecure \\\n   create worker "benchmark-task"  \\\n   --handler "echo {\\"result\\":\\"Pong\\"}"\n'})}),"\n",(0,r.jsx)(n.h2,{id:"chaos-experiment---use-iptables-with-containerized-nfs-server",children:"Chaos Experiment - Use iptables with containerized NFS Server"}),"\n",(0,r.jsxs)(n.p,{children:["We wanted to disrupt the NFS connections with ",(0,r.jsx)(n.code,{children:"iptables"})," and cause some errors."]}),"\n",(0,r.jsx)(n.h3,{id:"expected",children:"Expected"}),"\n",(0,r.jsxs)(n.p,{children:["We can drop packages with ",(0,r.jsx)(n.code,{children:"iptables"}),", and we can observe errors in the Zeebe container logs."]}),"\n",(0,r.jsx)(n.h3,{id:"actual",children:"Actual"}),"\n",(0,r.jsx)(n.p,{children:"Setting up the following iptables rule should allow us to disrupt the NFS connection, but it didn't worked."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"sudo iptables -A OUTPUT -p tcp --dport 2049 --sport 2049 -d localhost -j DROP\n"})}),"\n",(0,r.jsx)(n.p,{children:"At the end we were setting up a lots of different rules, but nothing seem to work."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"Every 1.0s: sudo iptables -L -v                                                                                                             cq-p14s: Thu Jun 12 16:01:28 2025\n\nChain INPUT (policy ACCEPT 6090K packets, 11G bytes)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 DROP       tcp  --  any    any     anywhere             cq-p14s              tcp dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             10.0.88.5            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             10.0.88.1            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             10.88.0.5            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             cq-p14s              tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             anywhere             tcp spt:nfs dpt:nfs\n\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain OUTPUT (policy ACCEPT 6182K packets, 22G bytes)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             0.0.0.0              tcp dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             cq-p14s              tcp dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             localhost            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             11.0.88.5            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             10.0.88.5            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             0.0.0.0              tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             10.0.88.1            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             10.88.0.5            tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             cq-p14s              tcp spt:nfs dpt:nfs\n    0     0 DROP       tcp  --  any    any     anywhere             anywhere             tcp spt:nfs dpt:nfs\n\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We even suspended the NFS server, via ",(0,r.jsx)(n.code,{children:"docker pause"}),". We were able to observe that data was still synced between directories."]}),"\n",(0,r.jsx)(n.p,{children:"This was some indication for us, that the kernel might do some magic behind the scenes, and the NFS server didn't worked as we expected it to."}),"\n",(0,r.jsx)(n.h2,{id:"chaos-experiment---use-iptables-with-an-external-nfs-server",children:"Chaos Experiment - Use iptables with an external NFS Server"}),"\n",(0,r.jsx)(n.p,{children:"As we were not able to disrupt the network, we thought it might make sense to externalize the NFS server (to a different host)."}),"\n",(0,r.jsx)(n.h3,{id:"setup-external-nfs",children:"Setup external NFS"}),"\n",(0,r.jsxs)(n.p,{children:["We followed ",(0,r.jsx)(n.a,{href:"https://idroot.us/install-nfs-server-fedora-41",children:"this guide"}),", to set up a NFS server running on a different machine."]}),"\n",(0,r.jsx)(n.h3,{id:"mount-external-nfs",children:"Mount external NFS"}),"\n",(0,r.jsx)(n.p,{children:"The mounting was quite similar to before, now using a different host"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"sudo  mount -v -t nfs4 -o proto=tcp,port=2049,soft,timeo=10 192.168.24.110:/ ~/nfs-workshop/nfs-client-mount/\n"})}),"\n",(0,r.jsx)(n.h3,{id:"run-zeebe-container",children:"Run Zeebe Container"}),"\n",(0,r.jsx)(n.p,{children:"The same for running the Zeebe container."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"podman run -d -v /home/cqjawa/nfs-workshop/nfs-client-mount/srv/nfs/:/usr/local/zeebe/data -p 26500:26500 -p 9600:9600 gcr.io/zeebe-io/zeebe:8.7.5-root\n"})}),"\n",(0,r.jsx)(n.h3,{id:"expected-1",children:"Expected"}),"\n",(0,r.jsx)(n.p,{children:"We were expecting some errors during processing and writing when the connection was completely dropped."}),"\n",(0,r.jsx)(n.h3,{id:"actual-1",children:"Actual"}),"\n",(0,r.jsxs)(n.p,{children:["Similar to previous ",(0,r.jsx)(n.code,{children:"iptables"})," we dropped all outgoing packages for the port ",(0,r.jsx)(n.code,{children:"2049"})," with the new destination."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"sudo iptables -A OUTPUT -p tcp --dport 2049 -d 192.168.24.110 -j DROP\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"Every 1.0s: sudo iptables -L -v                                                                                                            cq-p14s: Thu Jun 12 16:13:44 2025\n\nChain INPUT (policy ACCEPT 6211K packets, 11G bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain OUTPUT (policy ACCEPT 6297K packets, 23G bytes)\n pkts bytes target     prot opt in     out     source               destination\n   35 2064K DROP       tcp  --  any    any     anywhere             192.168.24.110\t tcp dpt:nfs\n\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Now we were actually able to observe some errors. The clients were receiving ",(0,r.jsx)(n.code,{children:"DEADLINE EXCEEDED"})," exceptions (starter and worker)."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"2025/06/12 16:14:03 Failed to activate jobs for worker 'zbctl': rpc error: code = DeadlineExceeded desc = context deadline exceeded\nError: rpc error: code = DeadlineExceeded desc = context deadline exceeded\nError: rpc error: code = DeadlineExceeded desc = context deadline exceeded\nError: rpc error: code = DeadlineExceeded desc = stream terminated by RST_STREAM with error code: CANCEL\nError: rpc error: code = DeadlineExceeded desc = context deadline exceeded\n"})}),"\n",(0,r.jsx)(n.p,{children:"After some time running with the disconnected NFS server, Zeebe actually failed to flush"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"[2025-06-12 09:02:00.819] [raft-server-0-1] [{actor-name=raft-server-1, actor-scheduler=Broker-0, partitionId=1, raft-role=LEADER}] ERROR\n        io.atomix.raft.impl.RaftContext - An uncaught exception occurred, transition to inactive role\njava.io.UncheckedIOException: java.io.IOException: Input/output error (msync with parameter MS_SYNC failed)\n        at java.base/java.nio.MappedMemoryUtils.force(Unknown Source) ~[?:?]\n        at java.base/java.nio.Buffer$2.force(Unknown Source) ~[?:?]\n        at java.base/jdk.internal.misc.ScopedMemoryAccess.forceInternal(Unknown Source) ~[?:?]\n        at java.base/jdk.internal.misc.ScopedMemoryAccess.force(Unknown Source) ~[?:?]\n        at java.base/java.nio.MappedByteBuffer.force(Unknown Source) ~[?:?]\n        at java.base/java.nio.MappedByteBuffer.force(Unknown Source) ~[?:?]\n        at io.camunda.zeebe.journal.file.Segment.flush(Segment.java:125) ~[zeebe-journal-8.7.5.jar:8.7.5]\n        at io.camunda.zeebe.journal.file.SegmentsFlusher.flush(SegmentsFlusher.java:58) ~[zeebe-journal-8.7.5.jar:8.7.5]\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.flush(SegmentedJournalWriter.java:125) ~[zeebe-journal-8.7.5.jar:8.7.5]\n        at io.camunda.zeebe.journal.file.SegmentedJournal.flush(SegmentedJournal.java:173) ~[zeebe-journal-8.7.5.jar:8.7.5]\n        at io.atomix.raft.storage.log.RaftLogFlusher$DirectFlusher.flush(RaftLogFlusher.java:73) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\n        at io.atomix.raft.storage.log.RaftLog.flush(RaftLog.java:196) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\n        at io.atomix.raft.impl.RaftContext.setCommitIndex(RaftContext.java:538) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\n        at io.atomix.raft.roles.LeaderAppender.appendEntries(LeaderAppender.java:560) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\n        at io.atomix.raft.roles.LeaderRole.replicate(LeaderRole.java:740) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\n        at io.atomix.raft.roles.LeaderRole.safeAppendEntry(LeaderRole.java:735) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\n        at io.atomix.raft.roles.LeaderRole.lambda$appendEntry$15(LeaderRole.java:701) ~[zeebe-atomix-cluster-8.7.5.jar:8.7.5]\n        at io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:178) ~[zeebe-atomix-utils-8.7.5.jar:8.7.5]\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]\n        at java.base/java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]\n        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[?:?]\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\n        at java.base/java.lang.Thread.run(Unknown Source) [?:?]\nCaused by: java.io.IOException: Input/output error (msync with parameter MS_SYNC failed)\n        at java.base/java.nio.MappedMemoryUtils.force0(Native Method) ~[?:?]\n        ... 24 more\n"})}),"\n",(0,r.jsx)(n.p,{children:"This caused the RAFT Leader role to become inactive, and uninstalling all related services."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"INFO io.atomix.raft.impl.RaftContext - Transitioning to INACTIVE\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Furthermore, interesting is that the ",(0,r.jsx)(n.code,{children:"DiskSpaceMonitor"})," was detecting OOD and pausing the stream processor."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"[2025-06-12 09:02:00.795] [zb-actors-0] [{actor-name=DiskSpaceUsageMonitorActor, actor-scheduler=Broker-0}] WARN \n        io.camunda.zeebe.broker.system - Out of disk space. Current available 0 bytes. Minimum needed 2147483648 bytes.\n[2025-06-12 09:02:00.796] [zb-actors-0] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] WARN \n        io.camunda.zeebe.broker.system - Disk space usage is above threshold. Pausing stream processor.\n"})}),"\n",(0,r.jsx)(n.p,{children:"At the end, the system was not running anymore. This means availability was impacted, but not durability, as we do not write anything wrong (or do not continue with dirty data)"}),"\n",(0,r.jsx)(n.h2,{id:"chaos-experiment-3---random-dropping-packages",children:"Chaos Experiment 3 - Random dropping packages"}),"\n",(0,r.jsxs)(n.p,{children:["It is possible with ",(0,r.jsx)(n.code,{children:"iptables"})," to randomly drop packages, allow to validate how the system behaves on certain package loss."]}),"\n",(0,r.jsx)(n.h3,{id:"expected-2",children:"Expected"}),"\n",(0,r.jsx)(n.p,{children:"We expected that here the system might also fail, potentially, with some exceptions."}),"\n",(0,r.jsx)(n.h3,{id:"actual-2",children:"Actual"}),"\n",(0,r.jsxs)(n.p,{children:["Running the following command sets up an ",(0,r.jsx)(n.code,{children:"iptables"})," rule that drops random packets with ",(0,r.jsx)(n.code,{children:"80%"})," probability for destination port ",(0,r.jsx)(n.code,{children:"2049"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"sudo iptables -A OUTPUT -p tcp --dport 2049 -d 192.168.24.110 -m statistic --mode random --probability 0.80 -j DROP\n"})}),"\n",(0,r.jsx)(n.p,{children:"As NFS is TCP based it seem to be that NFS can handle certain data/package loss, and is repeating the packages."}),"\n",(0,r.jsx)(n.p,{children:"The general processing was much slower, this was observed by the rate of how many instances were created and jobs completed."}),"\n",(0,r.jsx)(n.p,{children:"Other than that the system continued to run healthy."}),"\n",(0,r.jsx)(n.h2,{id:"chaos-experiment-4---drop-connection-on-reading",children:"Chaos Experiment 4 - Drop connection on reading"}),"\n",(0,r.jsx)(n.p,{children:"We wanted to cause some SIGBUS errors, as we knew this can happen with mmapped files, like it is used in Zeebe. This might be reproduced on reading of memory mapped data."}),"\n",(0,r.jsx)(n.p,{children:"For this we planned to create a lot of data on our Zeebe system and restarting it, causing Zeebe to fail on replay when the connection is blocked."}),"\n",(0,r.jsx)(n.h3,{id:"expected-3",children:"Expected"}),"\n",(0,r.jsx)(n.p,{children:"We expected that during read, we would cause a SIGBUS, causing the system to crash"}),"\n",(0,r.jsx)(n.h3,{id:"actual-3",children:"Actual"}),"\n",(0,r.jsx)(n.p,{children:"To make sure we are creating continuous segments, and not compacting (causing longer replay) we increased the snapshot period and reduced the log segment size."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"podman run -d \\\n  -v /home/cqjawa/nfs-workshop/nfs-client-mount/srv/nfs/:/usr/local/zeebe/data \\\n  -p 26500:26500 -p 9600:9600 \\\n  -e ZEEBE_BROKER_THREADS_CPUTHREADCOUNT=2 \\\n  -e ZEEBE_BROKER_THREADS_IOTHREADCOUNT=2 \\\n  -e ZEEBE_BROKER_DATA_LOGSEGMENTSIZE=16MB \\\n  -e ZEEBE_BROKER_DATA_SNAPSHOTPERIOD=8h \\\n  gcr.io/zeebe-io/zeebe:8.7.5-root\n"})}),"\n",(0,r.jsxs)(n.p,{children:["First we set up an ",(0,r.jsx)(n.code,{children:"iptable"})," rule to make sure that the reading was slower from NFS (by random dropping ~80% of packages)."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"sudo iptables -A OUTPUT -p tcp --dport 2049 -d 192.168.24.110 -m statistic --mode random --probability 0.80 -j DROP\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"[2025-06-12 09:25:00.543] [zb-actors-1] [{actor-name=StreamProcessor-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.processor - Processor starts replay of events. [snapshot-position: 611, replay-mode: PROCESSING]\n"})}),"\n",(0,r.jsx)(n.p,{children:"When we saw that the StreamProcessor was starting with replay we started to drop packages again completely."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"sudo iptables -A OUTPUT -p tcp --dport 2049 -d 192.168.24.110 -j DROP\n"})}),"\n",(0,r.jsx)(n.p,{children:"After a certain period of time, we ran into a SIGBUS Error"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:'[2025-06-12 09:25:00.543] [zb-actors-1] [{actor-name=StreamProcessor-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.processor - Processor starts replay of events. [snapshot-position: 611, replay-mode: PROCESSING]\n[2025-06-12 09:25:00.545] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning CommandApiService\n[2025-06-12 09:25:00.547] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning SnapshotDirector\n[2025-06-12 09:25:00.549] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning ExporterDirector\n[2025-06-12 09:25:00.555] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning BackupApiRequestHandler\n[2025-06-12 09:25:00.557] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 - transitioning Admin API\n[2025-06-12 09:25:00.558] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.broker.system - Transition to LEADER on term 4 completed\n[2025-06-12 09:25:00.561] [zb-actors-1] [{actor-name=ZeebePartition-1, actor-scheduler=Broker-0, partitionId=1}] INFO \n\tio.camunda.zeebe.broker.system - ZeebePartition-1 recovered, marking it as healthy\n[2025-06-12 09:25:00.562] [zb-actors-1] [{actor-name=HealthCheckService, actor-scheduler=Broker-0}] INFO \n\tio.camunda.zeebe.broker.system - Partition-1 recovered, marking it as healthy\n#\n# A fatal error has been detected by the Java Runtime Environment:\n#\n#  SIGBUS (0x7) at pc=0x00007f89ec4601a5, pid=2, tid=49\n#\n# JRE version: OpenJDK Runtime Environment Temurin-21.0.7+6 (21.0.7+6) (build 21.0.7+6-LTS)\n# Java VM: OpenJDK 64-Bit Server VM Temurin-21.0.7+6 (21.0.7+6-LTS, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\n# Problematic frame:\n# v  ~StubRoutines::updateBytesCRC32C 0x00007f89ec4601a5\n#\n# Core dump will be written. Default location: Core dumps may be processed with "/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %d" (or dumping to /usr/local/zeebe/core.2)\n#\n# An error report file with more information is saved as:\n# /usr/local/zeebe/hs_err_pid2.log\n[275.689s][warning][os] Loading hsdis library failed\n#\n# If you would like to submit a bug report, please visit:\n#   https://github.com/adoptium/adoptium-support/issues\n'})}),"\n",(0,r.jsx)(n.p,{children:"This caused the JVM to crash and stop the Docker container, as expected."}),"\n",(0,r.jsx)(n.h2,{id:"results",children:"Results"}),"\n",(0,r.jsx)(n.p,{children:"With the workshop on experimenting with NFS, we got several learnings on how Zeebe and NFS behave on connectivity issues, summarized as follows:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["We could confirm that network errors lead to unrecoverable SIGBUS errors, which cause the broker to crash.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This is due primarily to our usage of mmap both in RocksDB and Zeebe."}),"\n",(0,r.jsx)(n.li,{children:"There is an easy workaround with RocksDB where you can simply turn off mmap, but no such workaround exists in Zeebe at the moment."}),"\n",(0,r.jsx)(n.li,{children:"This only impacts availability as the application crashes, but since Zeebe is designed to be crash resilient, so no inconsistencies or data corruption."}),"\n",(0,r.jsx)(n.li,{children:"We don\u2019t have a clear idea of the frequency of these errors - it\u2019s essentially environment-based (i.e., how bad the network connectivity is)."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"With only partial connectivity (simulated by dropping packets, e.g. 70% of packets), we mostly observed performance issues, as things got slower; however, messages were retried, so no errors occurred."}),"\n",(0,r.jsxs)(n.li,{children:["Network errors when using normal file I/O resulted in IOException as expected.","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"This caused the Raft partition to go inactive, for example, when the leader fails to flush on commit (a known issue which is already planned to be fixed for graceful error handling)."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"When the NFS server was unavailable, the disk space monitor detected that there was no more disk space available, and writes stopped."}),"\n",(0,r.jsx)(n.li,{children:"Did not test that it recovers when the server is back, but we expect it would."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Minor"}),", but we should open an issue for it:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"When the leader goes inactive, we report an internal error that there is no message handler for command-api-1, but really we should be returning an UNAVAILABLE as a proper error, and not logging this as error level (we have other means to detect this)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"What does this mean?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"We can say Zeebe can work with NFS, but it is not yet supported."}),"\n",(0,r.jsx)(n.li,{children:"We need to improve certain error handling, like flushing errors, to better support it."}),"\n",(0,r.jsx)(n.li,{children:"When operating Zeebe on bare-metal and having an unreliable environment SIGBUS might be more likely and crashin JVM be more problematic then using an K8 deployment, where pods automatically getting rescheduled"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);