"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[3309],{88033:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>n,toc:()=>c});var n=s(81758),i=s(74848),a=s(28453);const r={layout:"posts",title:"Lower memory consumption of Camunda deployment",date:new Date("2025-06-05T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"zell"},o="Chaos Day Summary",l={authorsImageUrls:[void 0]},c=[{value:"Checking weekly benchmarks",id:"checking-weekly-benchmarks",level:2},{value:"Memory consumption",id:"memory-consumption",level:3},{value:"CPU Consumption",id:"cpu-consumption",level:3},{value:"1. Experiment: Reduce memory limits generally",id:"1-experiment-reduce-memory-limits-generally",level:2},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"2. Experiment: Reduce memory limits and RocksDB memory",id:"2-experiment-reduce-memory-limits-and-rocksdb-memory",level:2},{value:"Expected",id:"expected-1",level:3},{value:"Actual",id:"actual-1",level:3},{value:"3. Experiment: Half it",id:"3-experiment-half-it",level:2},{value:"Expected",id:"expected-2",level:3},{value:"Actual",id:"actual-2",level:3},{value:"4. Experiment: Half it (again)",id:"4-experiment-half-it-again",level:2},{value:"Expected",id:"expected-3",level:3},{value:"Actual",id:"actual-3",level:3},{value:"5. Experiment: Half RocksDb once more",id:"5-experiment-half-rocksdb-once-more",level:2},{value:"Expected",id:"expected-4",level:3},{value:"Actual",id:"actual-4",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Found Bugs",id:"found-bugs",level:2}];function d(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.p,{children:"I'm back to finally do some load testing again."}),"\n",(0,i.jsx)(t.p,{children:"In the past months, we have changed our architecture. This was to deploy instead all of our components as a separate deployment,\nwe now have one single statefulset. This statefulset is running our single Camunda standalone application,\ncombining all components together."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"simpler deployment",src:s(25811).A+"",width:"1550",height:"744"})}),"\n",(0,i.jsxs)(t.p,{children:["More details on this change we will share on a separate blog post. For simplicity, in our load tests (benchmark helm charts), we\ncombined all the resources we had split over multiple deployments together, see related PR ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/zeebe-benchmark-helm/pull/213",children:"#213"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"We are currently running our test with the following resources by default:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-yaml",children:"    Limits:\n      cpu:     2\n      memory:  12Gi\n    Requests:\n      cpu:      2\n      memory:   6Gi\n"})}),"\n",(0,i.jsx)(t.p,{children:"In today's Chaos day, I want to look into our resource consumption and whether we can reduce our used requests and limits."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"TL;DR;"})," We have focused on experimenting with different memory resources, and were able to show that we can reduce the used memory by 75%, and our previous provisioned resources by more than 80% for our load tests."]}),"\n",(0,i.jsx)(t.h2,{id:"checking-weekly-benchmarks",children:"Checking weekly benchmarks"}),"\n",(0,i.jsx)(t.p,{children:"Before I started to experiment and reduce it. I validated whether we actually have room for improvement. For that, I check our\nweekly load tests. These are tests we start every week, that are running for four weeks straight. These can be used as a good reference point (base)."}),"\n",(0,i.jsx)(t.p,{children:"I picked the mixed load test, which is running our realistic benchmark using a  more complex process model, covering more elements, etc."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"base general",src:s(45395).A+"",width:"2536",height:"814"})}),"\n",(0,i.jsx)(t.p,{children:"When we look at the general metrics, we can see it reaches, on average, ~100 task completions per second. As we use pre-emptive nodes, it might happen that workers, starters, or even the Camunda application are restarted in between."}),"\n",(0,i.jsx)(t.h3,{id:"memory-consumption",children:"Memory consumption"}),"\n",(0,i.jsx)(t.p,{children:"Looking at the memory consumption, we can see that we still have quite some headroom."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"base-memory",src:s(92498).A+"",width:"2526",height:"679"})}),"\n",(0,i.jsx)(t.p,{children:"Our deployments use between three and four gigabytes of memory, which is divided by the JVM heap, JVM metaspace, native memory usage like RocksDB, off-heap usage etc."}),"\n",(0,i.jsx)(t.p,{children:"For example, we can see that the JVM uses less than one gigabyte for the heap, but can use up to ~3.5 gigabytes for its heap. This is related to the default JVM settings, which are ~25% of the available memory on the machine."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"base-jvm-mem",src:s(8442).A+"",width:"2525",height:"814"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://rocksdb.org/",children:"RocksDB"}),", the embedded key-value store that Zeebe uses to store its state, is per default configured to use 512 MB per partition. We can observe via exposed metrics this as well."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"base-rocksdb",src:s(53015).A+"",width:"1291",height:"445"})}),"\n",(0,i.jsxs)(t.p,{children:["We can set this memory limit for RocksDB via an ",(0,i.jsx)(t.a,{href:"https://github.com/camunda/camunda/blob/main/zeebe/broker/src/main/java/io/camunda/zeebe/broker/system/configuration/RocksdbCfg.java#L23",children:"experimental configuration"}),". For example, via environment variable: ",(0,i.jsx)(t.code,{children:"ZEEBE_BROKER_EXPERIMENTAL_ROCKSDB_MEMORYLIMIT"})," or property: ",(0,i.jsx)(t.code,{children:"zeebe.broker.experimental.rocksdb.memoryLimit"}),"."]}),"\n",(0,i.jsx)(t.h3,{id:"cpu-consumption",children:"CPU Consumption"}),"\n",(0,i.jsx)(t.p,{children:"After having checked the memory consumption, we can look at the CPU consumption. As mentioned earlier, we are running a rather more complex orchestration use case that involves more work on processing, exporting, etc."}),"\n",(0,i.jsx)(t.p,{children:"Here we can already see that we scratch on our limits, we can observe some throttling for some of our pods."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"base-cpu",src:s(57949).A+"",width:"2536",height:"836"})}),"\n",(0,i.jsx)(t.p,{children:"For today, I will focus on the memory consumption to improve it. We might want to look into the CPU consumption on another day."}),"\n",(0,i.jsx)(t.h2,{id:"1-experiment-reduce-memory-limits-generally",children:"1. Experiment: Reduce memory limits generally"}),"\n",(0,i.jsx)(t.p,{children:"As a first experiment, I tried to reduce the general memory to something which I thought made sense based on the observation I made earlier. This means setting requests and limits to four gigabytes."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-yaml",children:"    Limits:\n      cpu:     2\n      memory:  4Gi\n    Requests:\n      cpu:      2\n      memory:   4Gi\n"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"This is a 66% decrease from the previous limit and a 33% decrease from the previous used requests!"})}),"\n",(0,i.jsx)(t.p,{children:"Be aware that I set both to the same value on purpose. This is to make sizing, scheduling, and memory management more predictable. Furthermore, to reduce the chance of getting OOMs/killed/evicted."}),"\n",(0,i.jsxs)(t.p,{children:["The memory request is used for Kubernetes pod scheduling. This means the limit is not a guaranteed size, but more like a guard to prevent the container to use more. If a container uses more than its requests ",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run",children:"there is a chance to be evicted"}),", if the node becomes memory pressure. If it exceeds its limits, it will be killed eventually by the kernel."]}),"\n",(0,i.jsx)(t.h3,{id:"expected",children:"Expected"}),"\n",(0,i.jsx)(t.p,{children:"As we observed in our weekly load test, we are able to use less than 1 gigabyte of Java Heap, which is ~25% of four gigabytes, and we normally use three partitions, a 512 MB (~1,5 Gi). I expect that four gigabytes of memory should perform well."}),"\n",(0,i.jsx)(t.h3,{id:"actual",children:"Actual"}),"\n",(0,i.jsx)(t.p,{children:"The general performance is comparable to our base, we do not spot any frequent restarts."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp1-general",src:s(95458).A+"",width:"2537",height:"812"})}),"\n",(0,i.jsx)(t.p,{children:"Looking at the memory, we see that we are able to run with the reduced memory as well."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp1-memory",src:s(11545).A+"",width:"2542",height:"686"})}),"\n",(0,i.jsx)(t.p,{children:"The JVM memory usage even shows us that we are able to use less memory; previously, we used ~1 gig as heap, now it is around 256 MB."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp1-jvm-mem",src:s(68655).A+"",width:"2529",height:"943"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"This gives us room for further improvement. Let's continue with Experiment 2"})}),"\n",(0,i.jsx)(t.h2,{id:"2-experiment-reduce-memory-limits-and-rocksdb-memory",children:"2. Experiment: Reduce memory limits and RocksDB memory"}),"\n",(0,i.jsx)(t.p,{children:"With the results from Experiment 1, I was confident that we could run with less memory. I was wondering what if we reduced the memory limit of RocksDB."}),"\n",(0,i.jsx)(t.p,{children:"As mentioned earlier can be done via a property or an environment variable. For our next experiment, I set our limit\nto 128 MB. This is a 75% reduction of previous used memory for RocksDB per partition."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-shell",children:"zeebe.broker.experimental.rocksdb.memoryLimit: 128MB\n"})}),"\n",(0,i.jsx)(t.p,{children:"We are running similar configurations in our SaaS environment, so I knew this is working, but I don't know how this behaves on a more complex use case and benchmark."}),"\n",(0,i.jsx)(t.h3,{id:"expected-1",children:"Expected"}),"\n",(0,i.jsx)(t.p,{children:"My expectation would be that the general memory consumption is reduced, not affecting the JVM. Our load test should run stable still."}),"\n",(0,i.jsx)(t.h3,{id:"actual-1",children:"Actual"}),"\n",(0,i.jsx)(t.p,{children:"Indeed, the general performance looks similar, with some smaller outliers but still performing good."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp2-general",src:s(95458).A+"",width:"2537",height:"812"})}),"\n",(0,i.jsx)(t.p,{children:"We reduced the memory consumption for the process by half! It is now around 1.5 gigabytes, while it was in the previous experiment around three gigabytes, and at the start, close to four."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp2-mem",src:s(37090).A+"",width:"2533",height:"678"})}),"\n",(0,i.jsx)(t.p,{children:"In our RocksDB related metrics, we are able to observe the actual size of our RocksDB instance as well, which is indeed 128 MB."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp2-rocks",src:s(41263).A+"",width:"1252",height:"483"})}),"\n",(0,i.jsx)(t.h2,{id:"3-experiment-half-it",children:"3. Experiment: Half it"}),"\n",(0,i.jsx)(t.p,{children:"As we were still running fine, and wanted to reach a point where it doesn't run well anymore. I simply thought about reducing our resources by half again."}),"\n",(0,i.jsx)(t.p,{children:"Changing our deployment resources to two gigabytes:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-yaml",children:"    Limits:\n      cpu:     2\n      memory:  2Gi\n    Requests:\n      cpu:      2\n      memory:   2Gi\n\n"})}),"\n",(0,i.jsx)(t.p,{children:"Configuring RocksDB memory limit to 64MB"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"zeebe.broker.experimental.rocksdb.memoryLimit: 64MB\n"})}),"\n",(0,i.jsx)(t.h3,{id:"expected-2",children:"Expected"}),"\n",(0,i.jsx)(t.p,{children:"Similar to above, I was still expecting that it works, as we saw that the JVM usage was rather low and still performing good."}),"\n",(0,i.jsx)(t.h3,{id:"actual-2",children:"Actual"}),"\n",(0,i.jsx)(t.p,{children:"The performance of the test still looks acceptable. We see some restarts, but they seem not to be related to memory pressure."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp3-general",src:s(92128).A+"",width:"2540",height:"821"})}),"\n",(0,i.jsx)(t.p,{children:"Again, we were able to reduce the memory, but not with such big steps as before. For this load test, we have on average a ~1.2 G memory usage per pod."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp3-mem",src:s(67051).A+"",width:"2529",height:"680"})}),"\n",(0,i.jsx)(t.p,{children:"When we look at the JVM metrics, we can see that we are getting closer to our maximum, commited, and used heap values. Still, the used heap was reduced and is now around ~128 MB in many cases."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp3-jvm",src:s(91305).A+"",width:"2523",height:"815"})}),"\n",(0,i.jsx)(t.p,{children:"The RocksDB instance now uses 64MB as expected."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp3-rocks",src:s(50426).A+"",width:"1266",height:"487"})}),"\n",(0,i.jsx)(t.h2,{id:"4-experiment-half-it-again",children:"4. Experiment: Half it (again)"}),"\n",(0,i.jsx)(t.p,{children:"As I want to bring it to its limits, I reduced the memory resources once more by half."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-yaml",children:"    Limits:\n      cpu:     2\n      memory:  1Gi\n    Requests:\n      cpu:      2\n      memory:   1Gi\n\n"})}),"\n",(0,i.jsxs)(t.p,{children:["But to not change too many things at once (sorry for doing it earlier ",":D","), I kept the previous RocksDB configuration:"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"zeebe.broker.experimental.rocksdb.memoryLimit: 64MB\n"})}),"\n",(0,i.jsx)(t.h3,{id:"expected-3",children:"Expected"}),"\n",(0,i.jsx)(t.p,{children:"I felt that this might be quite low on its limits, but still expected it to work, looking at the JVM heap usage metrics."}),"\n",(0,i.jsx)(t.h3,{id:"actual-3",children:"Actual"}),"\n",(0,i.jsx)(t.p,{children:"As we can see, this was a tremendous fail. The pods were in an OOM loop and never became stable."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp4-general",src:s(55847).A+"",width:"2543",height:"814"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp-mem",src:s(6476).A+"",width:"2539",height:"674"})}),"\n",(0,i.jsx)(t.p,{children:"With this, we were able to find our limits."}),"\n",(0,i.jsx)(t.h2,{id:"5-experiment-half-rocksdb-once-more",children:"5. Experiment: Half RocksDb once more"}),"\n",(0,i.jsx)(t.p,{children:"Not accepting the previous failure, I simply wanted to try out what happens when I reduce once more the RocksDB memory limit."}),"\n",(0,i.jsx)(t.p,{children:"This means setting the limit to 32 MB."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"zeebe.broker.experimental.rocksdb.memoryLimit: 32MB\n"})}),"\n",(0,i.jsx)(t.h3,{id:"expected-4",children:"Expected"}),"\n",(0,i.jsx)(t.p,{children:"At this point, this was really exploration, I had the feeling that it might help if we reduce a little the RocksDB memory."}),"\n",(0,i.jsx)(t.h3,{id:"actual-4",children:"Actual"}),"\n",(0,i.jsxs)(t.p,{children:["Reducing the RocksDB memory limit allowed the Camunda application to perform as before! Without any performance impact ","\ud83d\ude80"," At the end we experienced a restart of all applications."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp5-general",src:s(66526).A+"",width:"2537",height:"815"})}),"\n",(0,i.jsx)(t.p,{children:"Looking at the process memory metrics, we can see that it is slowly increasing until it was OOM killed. This smells like a memory leak here."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp5-mem",src:s(93493).A+"",width:"2528",height:"673"})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-shell",children:"   Last State:     Terminated\n      Reason:       OOMKilled\n      Exit Code:    137\n      Started:      Wed, 04 Jun 2025 20:55:59 +0200\n      Finished:     Thu, 05 Jun 2025 12:07:51 +0200\n"})}),"\n",(0,i.jsx)(t.p,{children:"The JVM seem to perform correctly, and here we can observe any increasing usage. That indicates that there might be an issue with off heap (native) memory usage."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp5-jvm",src:s(35123).A+"",width:"2533",height:"950"})}),"\n",(0,i.jsx)(t.p,{children:"Our current native memory metrics don't highlight any specific ones either. While we can see that the metaspace uses a lot of space already, which also indicates that we likely can't reduce our memory usage more (except tuning this as well)."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp5-native",src:s(66865).A+"",width:"2515",height:"804"})}),"\n",(0,i.jsx)(t.p,{children:"The RocksDB memory usage looks stable as well."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"exp5-rocks",src:s(86816).A+"",width:"1266",height:"482"})}),"\n",(0,i.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(t.p,{children:"With today's experiments and investigations, we were able to show that we are able to reduce our memory consumption."}),"\n",(0,i.jsxs)(t.p,{children:["From previously used 12 Gi limit and 6 Gi request, we were able to show that it is running with 1 Gi limit and request, when we reduce the RocksDB memory limit as well. ",(0,i.jsx)(t.strong,{children:"This is an over 80-90% reduction for the assigned memory."})," Looking at our usage, we showed that the actual process memory usage has been reduced from ~4 Gi to 1 Gi, that is a ",(0,i.jsx)(t.strong,{children:"75% reduction"}),"!"]}),"\n",(0,i.jsxs)(t.p,{children:["To reduce the chance of getting OOM more frequently (until we investigated the potential resource leak), I propose to use 2 Gi as limits and requests and 64 MB RocksDb memory limit, which was running stable as well (see ",(0,i.jsx)(t.a,{href:"/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment#3-experiment--half-it",children:"Experiment 3"}),"). This showed a memory usage of around ~1.2 Gi, which is still a ",(0,i.jsx)(t.strong,{children:"70% reduction"})," to previously, and ~70-80% reduction of assigned resources."]}),"\n",(0,i.jsxs)(t.p,{children:["We can say this Chaos Day was a success, and I'm looking forward to the next one ","\ud83d\ude80"]}),"\n",(0,i.jsx)(t.h2,{id:"found-bugs",children:"Found Bugs"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Several panels were broken related to memory, and their tooltip and legends. I fixed this during the investigation."}),"\n",(0,i.jsx)(t.li,{children:"Potential native memory leak"}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},57949:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/base-cpu-6119877156e48cc719837cbea83f6406.png"},45395:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/base-general-1810c584609ed65136c09a5f71d8a37c.png"},8442:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/base-jvm-mem-d5f051e14aa368b1d2c62e34a2f63e9f.png"},92498:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/base-memory-0b344e9b0b510706893f539ddf6cd706.png"},53015:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/base-rocksdb-422c1286330affafa269cf82a78f67f6.png"},95458:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp1-general-7128fa232db70f766d4d151b7615dc66.png"},68655:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp1-jvm-c6425242afa3b3d55ee336e1d900a00b.png"},11545:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp1-memory-a0022c52dbde111fc9f7672bd0233358.png"},37090:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp2-mem-75890aed27482f1dd3ada3d784ede776.png"},41263:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp2-rocks-10e1ac00111667891aba4ac170e4c539.png"},92128:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp3-general-049ecb5e64be94e0a4f4b2e14a2e70df.png"},91305:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp3-jvm-bcd495a0ca21ad0c2e7e44008219aaff.png"},67051:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp3-mem-a52b60783e780da90b755ef2e27917aa.png"},50426:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp3-rocks-b70c8d2ef045b7aa63c78806f4251a84.png"},55847:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp4-general-1bdb5219c9d872145658c9433fe2c4ec.png"},6476:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp4-mem-9da511ab399a880d78f54e348223bed6.png"},66526:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp5-general-e40a0738408af6ee0eed2522794ecbfa.png"},35123:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp5-jvm-1174e721dc521313cec547d80141daf0.png"},93493:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp5-mem-32fb8d2fdc10b858231c3b3e2e2e6cf6.png"},66865:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp5-native-e04e3c78177c6a24890a79b847676d2a.png"},86816:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/exp5-rocks-63a34f86376ae4fa1713852623fd5ced.png"},25811:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/simpler-deployment-a09568ddfb813f18a49e6b930b43cd02.png"},28453:(e,t,s)=>{s.d(t,{R:()=>r,x:()=>o});var n=s(96540);const i={},a=n.createContext(i);function r(e){const t=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),n.createElement(a.Provider,{value:t},e.children)}},81758:e=>{e.exports=JSON.parse('{"permalink":"/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-06-05-Lower-memory-consumption-of-Camunda-deployment/index.md","source":"@site/blog/2025-06-05-Lower-memory-consumption-of-Camunda-deployment/index.md","title":"Lower memory consumption of Camunda deployment","description":"I\'m back to finally do some load testing again.","date":"2025-06-05T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":8.795,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Lower memory consumption of Camunda deployment","date":"2025-06-05T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"zell"},"unlisted":false,"nextItem":{"title":"News from Camunda Exporter project","permalink":"/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project"}}')}}]);