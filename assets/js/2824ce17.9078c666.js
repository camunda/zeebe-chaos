"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[3879],{55528:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>d,frontMatter:()=>r,metadata:()=>t,toc:()=>o});var t=a(5231),s=a(74848),i=a(28453);const r={layout:"posts",title:"Gateway Termination",date:new Date("2023-04-06T00:00:00.000Z"),categories:["chaos_experiment","gateway"],tags:["availability","resiliency"],authors:"zell"},c="Chaos Day Summary",l={authorsImageUrls:[void 0]},o=[{value:"Chaos Experiment",id:"chaos-experiment",level:2},{value:"Setup:",id:"setup",level:3},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"Termination",id:"termination",level:4},{value:"Scale down",id:"scale-down",level:4},{value:"Found Bugs",id:"found-bugs",level:2},{value:"Zbchaos print verbose logs",id:"zbchaos-print-verbose-logs",level:3},{value:"Segment creation impact",id:"segment-creation-impact",level:3}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"In today's chaos day, we wanted to experiment with the gateway and resiliency of workers."}),"\n",(0,s.jsxs)(n.p,{children:["We have seen in recent weeks some issues within our benchmarks when gateways have been restarted,\nsee ",(0,s.jsx)(n.a,{href:"https://github.com/camunda/camunda/issues/11975",children:"zeebe#11975"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["We did a similar experiment ",(0,s.jsx)(n.a,{href:"/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS",children:"in the past"}),",\ntoday we want to focus on self-managed (",(0,s.jsx)(n.a,{href:"https://helm.camunda.io/",children:"benchmarks with our helm charts"}),").\nIdeally, we can automate this as well soon."]}),"\n",(0,s.jsxs)(n.p,{children:["Today ",(0,s.jsx)(n.a,{href:"https://github.com/npepinpe",children:"Nicolas"})," joined me on the chaos day ","\ud83c\udf89"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"TL;DR;"})," We were able to show that the workers (clients) can reconnect after a gateway is shutdown ","\u2705","\nFurthermore, we have discovered a potential performance issue on lower load, which impacts process execution latency (",(0,s.jsx)(n.a,{href:"https://github.com/camunda/camunda/issues/12311",children:"zeebe#12311"}),")."]}),"\n",(0,s.jsx)(n.h2,{id:"chaos-experiment",children:"Chaos Experiment"}),"\n",(0,s.jsxs)(n.p,{children:["We will use our ",(0,s.jsx)(n.a,{href:"https://github.com/zeebe-io/benchmark-helm",children:"Zeebe benchmark helm charts"})," to set up the test cluster, and\nour helper scripts ",(0,s.jsx)(n.a,{href:"https://github.com/camunda/camunda/tree/main/benchmarks/setup",children:"here"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"setup",children:"Setup:"}),"\n",(0,s.jsx)(n.p,{children:"We will run with the default benchmark configuration, which means:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"three brokers"}),"\n",(0,s.jsx)(n.li,{children:"three partitions"}),"\n",(0,s.jsx)(n.li,{children:"replication count three"}),"\n",(0,s.jsx)(n.li,{children:"two gateways"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"We will run the benchmark with a low load, 10 process instances per second created and completed. For that,\nwe deploy one starter and worker. This reduces the blast radius and allows us to observe more easily how the workers\nbehave when a gateway is restarted."}),"\n",(0,s.jsxs)(n.p,{children:["During the experiment, we will use our ",(0,s.jsx)(n.a,{href:"https://github.com/camunda/camunda/tree/main/monitor/grafana",children:"grafana dashboard"})," to\nobserve to which gateway the worker will connect and which gateway we need to stop/restart."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"LAST DEPLOYED: Thu Apr  6 10:21:27 2023\nNAMESPACE: zell-chaos\nSTATUS: deployed\nREVISION: 1\nNOTES:\n# Zeebe Benchmark\n\nInstalled Zeebe cluster with:\n\n * 3 Brokers\n * 2 Gateways\n\nThe benchmark is running with:\n\n * Starter replicas=1\n * Worker replicas=1\n * Publisher replicas=0\n * Timer replicas=0\n"})}),"\n",(0,s.jsx)(n.h3,{id:"expected",children:"Expected"}),"\n",(0,s.jsxs)(n.p,{children:["When we terminate a gateway to which the worker has connected, ",(0,s.jsx)(n.strong,{children:"we expect"})," that the worker connects to the different\nreplica and starts completing jobs again."]}),"\n",(0,s.jsx)(n.p,{children:"The performance drop is expected to be not significant, or at least should recover fast."}),"\n",(0,s.jsx)(n.h3,{id:"actual",children:"Actual"}),"\n",(0,s.jsxs)(n.p,{children:["We will run the experiment in two ways, first via terminating the gateway (using ",(0,s.jsx)(n.a,{href:"https://github.com/zeebe-io/zeebe-chaos/releases/tag/zbchaos-v1.0.0",children:"zbchaos"}),")\nand later via scaling down the gateway deployment to one replica."]}),"\n",(0,s.jsx)(n.p,{children:"We want to verify whether this makes any difference, since terminating will cause Kubernetes to recreate immediately the pod."}),"\n",(0,s.jsx)(n.h4,{id:"termination",children:"Termination"}),"\n",(0,s.jsx)(n.p,{children:"Before we start the experiment we check our current deployed state:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"$ kgpo\nNAME                                        READY   STATUS      RESTARTS   AGE\ncamunda-platform-curator-28012860-zk72q     0/1     Completed   0          7m24s\nelasticsearch-master-0                      1/1     Running     0          45m\nelasticsearch-master-1                      1/1     Running     0          45m\nelasticsearch-master-2                      1/1     Running     0          45m\nleader-balancer-28012860-7cwmd              0/1     Completed   0          7m25s\nstarter-cb69c447f-l2zbh                     1/1     Running     0          45m\nworker-cb7f7c469-qvqqv                      1/1     Running     0          45m\nzell-chaos-zeebe-0                          1/1     Running     0          45m\nzell-chaos-zeebe-1                          1/1     Running     0          45m\nzell-chaos-zeebe-2                          1/1     Running     0          45m\nzell-chaos-zeebe-gateway-7bbdf9fd58-vs28f   1/1     Running     0          45m\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          45m\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"Via our Grafana dashboard (and the gRPC metrics) we are able to track to which gateway the worker connects to:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"grpc",src:a(71014).A+"",width:"1545",height:"840"})}),"\n",(0,s.jsxs)(n.p,{children:["It is ",(0,s.jsx)(n.code,{children:"zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f"}),".\nVia zbchaos we can easily terminate the gateway (it will always take the first in the pod list)."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"$ zbchaos terminate gateway \n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1680772377704 false false true false false 30 false -1 benchmark 30  }\nTerminated zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f\n"})}),"\n",(0,s.jsx)(n.p,{children:"After terminating we can see that a new gateway pod has started."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"$ kgpo\nNAME                                        READY   STATUS      RESTARTS   AGE\ncamunda-platform-curator-28012860-zk72q     0/1     Completed   0          13m\nelasticsearch-master-0                      1/1     Running     0          52m\nelasticsearch-master-1                      1/1     Running     0          52m\nelasticsearch-master-2                      1/1     Running     0          52m\nleader-balancer-28012860-7cwmd              0/1     Completed   0          13m\nstarter-cb69c447f-l2zbh                     1/1     Running     0          52m\nworker-cb7f7c469-qvqqv                      1/1     Running     0          52m\nzell-chaos-zeebe-0                          1/1     Running     0          52m\nzell-chaos-zeebe-1                          1/1     Running     0          52m\nzell-chaos-zeebe-2                          1/1     Running     0          52m\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          33s\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          52m\n"})}),"\n",(0,s.jsxs)(n.p,{children:["In the metrics, we can see that due to the restart, the throughput slightly dropped, but recovered pretty fast. The worker\nwas able to connect to the different gateway. ","\u2705"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"restart",src:a(28869).A+"",width:"1848",height:"883"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Note"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.em,{children:["In the panel ",(0,s.jsx)(n.code,{children:"Pod Restarts"})," on the top right, we don't see any restarts and that is something\nwe should always be aware of that the ",(0,s.jsx)(n.em,{children:"metrics are just samples of data"}),". If a pod, like a gateway, restarts fast enough\nand the metric collect interval is higher (per default we have ~30 s (?)) then you might not see a change."]})}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"scale-down",children:"Scale down"}),"\n",(0,s.jsx)(n.p,{children:"As described earlier we wanted to verify whether it makes a difference if we scale down the replica instead of terminating/restarting\nit, which causes restarting a new pod (which might get the same IP)."}),"\n",(0,s.jsxs)(n.p,{children:["For scaling down Nicolas found this annotation: ",(0,s.jsx)(n.code,{children:"controller.kubernetes.io/pod-deletion-cost"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#pod-deletion-cost",children:"That annotation allows giving hints to the schedule which pod to turn-down, because another pod might have a higher cost\nto be deleted (this is of course best-effort)."})}),"\n",(0,s.jsx)(n.p,{children:"This means we edit one pod and gave the following annotation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'annotations:\n  controller.kubernetes.io/pod-deletion-cost:  "-1"\n'})}),"\n",(0,s.jsx)(n.p,{children:"We have similarly chosen the pod as we have seen above, based on the gRPC metrics."}),"\n",(0,s.jsx)(n.p,{children:"Checking the running pods and editing the correct gateway:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\nNAME                                        READY   STATUS      RESTARTS   AGE\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          6m26s\nelasticsearch-master-0                      1/1     Running     0          59m\nelasticsearch-master-1                      1/1     Running     0          59m\nelasticsearch-master-2                      1/1     Running     0          59m\nleader-balancer-28012875-sctwq              0/1     Completed   0          6m26s\nstarter-cb69c447f-l2zbh                     1/1     Running     0          59m\nworker-cb7f7c469-qvqqv                      1/1     Running     0          59m\nzell-chaos-zeebe-0                          1/1     Running     0          59m\nzell-chaos-zeebe-1                          1/1     Running     0          59m\nzell-chaos-zeebe-2                          1/1     Running     0          59m\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          8m29s\nzell-chaos-zeebe-gateway-7bbdf9fd58-xc6d9   1/1     Running     0          59m\n"})}),"\n",(0,s.jsx)(n.p,{children:"When I did the following I was wondering why it didn't scale down the deployment, one pod was recreated."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k scale replicaset zell-chaos-zeebe-gateway-7bbdf9fd58 --replicas=1\nWarning: spec.template.spec.containers[0].env[16].name: duplicate name "ZEEBE_LOG_LEVEL"\nreplicaset.apps/zell-chaos-zeebe-gateway-7bbdf9fd58 scaled\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\nNAME                                        READY   STATUS            RESTARTS   AGE\ncamunda-platform-curator-28012875-tntdv     0/1     Completed         0          6m53s\nelasticsearch-master-0                      1/1     Running           0          60m\nelasticsearch-master-1                      1/1     Running           0          60m\nelasticsearch-master-2                      1/1     Running           0          60m\nleader-balancer-28012875-sctwq              0/1     Completed         0          6m53s\nstarter-cb69c447f-l2zbh                     1/1     Running           0          60m\nworker-cb7f7c469-qvqqv                      1/1     Running           0          60m\nzell-chaos-zeebe-0                          1/1     Running           0          60m\nzell-chaos-zeebe-1                          1/1     Running           0          60m\nzell-chaos-zeebe-2                          1/1     Running           0          60m\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   0/1     PodInitializing   0          7s\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running           0          8m56s\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Note:"})}),"\n",(0,s.jsxs)(n.p,{children:["During the experiment I learned that when you have deployed a ",(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",children:"deployment"}),", you need to scale down the deployment, not the ",(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/",children:"ReplicaSet"}),".\nOtherwise your Kubernetes deployment controller will recreate the ReplicaSet in the next reconcile loop, which means you\nwill have again the same replicas as defined in the deployment."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"So correct is to scale down the deployment (!), if you ever wonder."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k edit pod zell-chaos-zeebe-gateway-7bbdf9fd58-pkj48\npod/zell-chaos-zeebe-gateway-7bbdf9fd58-pkj48 edited\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\nNAME                                        READY   STATUS      RESTARTS   AGE\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          10m\nelasticsearch-master-0                      1/1     Running     0          63m\nelasticsearch-master-1                      1/1     Running     0          63m\nelasticsearch-master-2                      1/1     Running     0          63m\nleader-balancer-28012875-sctwq              0/1     Completed   0          10m\nstarter-cb69c447f-l2zbh                     1/1     Running     0          63m\nworker-cb7f7c469-qvqqv                      1/1     Running     0          63m\nzell-chaos-zeebe-0                          1/1     Running     0          63m\nzell-chaos-zeebe-1                          1/1     Running     0          63m\nzell-chaos-zeebe-2                          1/1     Running     0          63m\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   1/1     Running     0          3m40s\nzell-chaos-zeebe-gateway-7bbdf9fd58-pkj48   1/1     Running     0          12m\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ k scale deployment zell-chaos-zeebe-gateway --replicas=1\nWarning: spec.template.spec.containers[0].env[16].name: duplicate name "ZEEBE_LOG_LEVEL"\ndeployment.apps/zell-chaos-zeebe-gateway scaled\n[cqjawa 2023-04-06-gateway-termination/ cluster: zeebe-cluster ns:zell-chaos]$ kgpo\nNAME                                        READY   STATUS      RESTARTS   AGE\ncamunda-platform-curator-28012875-tntdv     0/1     Completed   0          10m\nelasticsearch-master-0                      1/1     Running     0          64m\nelasticsearch-master-1                      1/1     Running     0          64m\nelasticsearch-master-2                      1/1     Running     0          64m\nleader-balancer-28012875-sctwq              0/1     Completed   0          10m\nstarter-cb69c447f-l2zbh                     1/1     Running     0          64m\nworker-cb7f7c469-qvqqv                      1/1     Running     0          64m\nzell-chaos-zeebe-0                          1/1     Running     0          64m\nzell-chaos-zeebe-1                          1/1     Running     0          64m\nzell-chaos-zeebe-2                          1/1     Running     0          64m\nzell-chaos-zeebe-gateway-7bbdf9fd58-2v6gs   1/1     Running     0          4m\n'})}),"\n",(0,s.jsx)(n.p,{children:"With that, we have only one gateway pod left, and all traffic goes to that gateway. Based on the metrics\nwe can see that the workers recovered everytime when we restarted/terminated or scaled down."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"activate",src:a(13353).A+"",width:"917",height:"414"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"general",src:a(81656).A+"",width:"1845",height:"903"})}),"\n",(0,s.jsxs)(n.p,{children:["The experiment itself succeeded ","\ud83d\udcaa"," :white_check_marks:"]}),"\n",(0,s.jsx)(n.h2,{id:"found-bugs",children:"Found Bugs"}),"\n",(0,s.jsx)(n.h3,{id:"zbchaos-print-verbose-logs",children:"Zbchaos print verbose logs"}),"\n",(0,s.jsxs)(n.p,{children:["I realized that we still have ",(0,s.jsx)(n.a,{href:"https://github.com/zeebe-io/zeebe-chaos/issues/323",children:"the issue with zbchaos"})," which is printing verbose logs:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"$ zbchaos terminate gateway \n{1 LEADER -1  10  msg false 1 LEADER -1 2 LEADER -1 1680772377704 false false true false false 30 false -1 benchmark 30  }\nTerminated zell-chaos-zeebe-gateway-7bbdf9fd58-vs28f\n"})}),"\n",(0,s.jsx)(n.h3,{id:"segment-creation-impact",children:"Segment creation impact"}),"\n",(0,s.jsx)(n.p,{children:"During checking the metrics together with Nicolas, we realized that even on low load (10 PI/s) we have high spikes\nin our processing execution latency."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"latency",src:a(77978).A+"",width:"1852",height:"822"})}),"\n",(0,s.jsx)(n.p,{children:"The spikes are going up to 1-1.5 seconds, while the avg is at 0.06s. This happens every 6 minutes."}),"\n",(0,s.jsx)(n.p,{children:"We can see that the commit latency is as well at the same time high, which might be an issue because of the high IO."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"commit",src:a(57749).A+"",width:"1841",height:"250"})}),"\n",(0,s.jsx)(n.p,{children:"We first expected that to be related to snapshotting, but snapshots happen much more often."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"snapshot",src:a(36096).A+"",width:"923",height:"276"})}),"\n",(0,s.jsxs)(n.p,{children:["Interestingly is that it seems to be related to our segment creation (again), even if we have\nasync segment creation in our journal built recently. We need to investigate this further within ",(0,s.jsx)(n.a,{href:"https://github.com/camunda/camunda/issues/12311",children:"zeebe#12311"}),"."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"segment",src:a(8583).A+"",width:"1850",height:"402"})})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},13353:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/activate-bd04e48187522a661ce740667de5dae7.png"},57749:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/commit-541f81ca663ae80835fa6ef8357d94a7.png"},81656:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/general-eccb10353f089541f74a3f82593a836d.png"},71014:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/grpc-13224311ab5e142b1d17c5bfeb3369d7.png"},77978:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/latency-9fda0fdb0283c3af3831aef3813997e6.png"},28869:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/restart-fba3c5902c9b5523ca1f2fe9ba45def5.png"},8583:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/segment-176d6a262460c14aa4dce22bdce1dab2.png"},36096:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/snapshot-count-1b18a9a53eeaf2d45ab3a6f109b5d981.png"},28453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>c});var t=a(96540);const s={},i=t.createContext(s);function r(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:n},e.children)}},5231:e=>{e.exports=JSON.parse('{"permalink":"/zeebe-chaos/2023/04/06/gateway-termination","editUrl":"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-04-06-gateway-termination/index.md","source":"@site/blog/2023-04-06-gateway-termination/index.md","title":"Gateway Termination","description":"In today\'s chaos day, we wanted to experiment with the gateway and resiliency of workers.","date":"2023-04-06T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"},{"inline":true,"label":"resiliency","permalink":"/zeebe-chaos/tags/resiliency"}],"readingTime":7.24,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Gateway Termination","date":"2023-04-06T00:00:00.000Z","categories":["chaos_experiment","gateway"],"tags":["availability","resiliency"],"authors":"zell"},"unlisted":false,"prevItem":{"title":"SST Partitioning toggle","permalink":"/zeebe-chaos/2023/05/15/SST-Partitioning-toggle"},"nextItem":{"title":"Recursive call activity","permalink":"/zeebe-chaos/2023/02/23/Recursive-call-activity"}}')}}]);