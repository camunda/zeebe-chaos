"use strict";(globalThis.webpackChunkzell_chaos=globalThis.webpackChunkzell_chaos||[]).push([[6810],{1963:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/syncqueue-req-before-8d25531fa8c311440fa9c8e80e78d1e5.png"},13757:e=>{e.exports=JSON.parse('{"permalink":"/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-09-18-REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool/index.md","source":"@site/blog/2025-09-18-REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool/index.md","title":"REST API: From ForkJoin to a Dedicated Thread Pool","description":"During the latest REST API Performance load tests,","date":"2025-09-19T00:00:00.000Z","tags":[{"inline":true,"label":"availability","permalink":"/zeebe-chaos/tags/availability"}],"readingTime":6.7,"hasTruncateMarker":true,"authors":[{"name":"Berkay Can","title":"Software Engineer @ Zeebe","url":"https://github.com/berkaycanbc","imageURL":"https://github.com/berkaycanbc.png","key":"berkay","page":null}],"frontMatter":{"layout":"posts","title":"REST API: From ForkJoin to a Dedicated Thread Pool","date":"2025-09-19T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["availability"],"authors":"berkay"},"unlisted":false,"prevItem":{"title":"Resilience of dynamic scaling","permalink":"/zeebe-chaos/2025/10/02/Dynamic-Scaling-Resilience"},"nextItem":{"title":"Resiliency against ELS unavailability","permalink":"/zeebe-chaos/2025/08/26/Resiliency-against-ELS-unavailability"}}')},15092:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/max-pool-size-multiplier-8-f324c1eef2158ab2f20dd5d688a4e89f.png"},19744:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/final-decision-result-cpu-c579720539ebcb00496de95c9120dc49.png"},20030:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/syncqueue-req-after-172469b801a0b97b731b4b27ba44eedf.png"},20715:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/queue-capacity-64-f324c1eef2158ab2f20dd5d688a4e89f.png"},28453:(e,s,t)=>{t.d(s,{R:()=>l,x:()=>a});var n=t(96540);const i={},r=n.createContext(i);function l(e){const s=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),n.createElement(r.Provider,{value:s},e.children)}},32922:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/queue-capacity-256-19acdd7b6ca1df00ccaefb20fae5ce41.png"},35224:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/syncqueue-cpu-after-b2902d1db04dba87750b03e9dae1673f.png"},38030:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>o,contentTitle:()=>a,default:()=>d,frontMatter:()=>l,metadata:()=>n,toc:()=>c});var n=t(13757),i=t(74848),r=t(28453);const l={layout:"posts",title:"REST API: From ForkJoin to a Dedicated Thread Pool",date:new Date("2025-09-19T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"berkay"},a="REST API: From ForkJoin to a Dedicated Thread Pool",o={authorsImageUrls:[void 0]},c=[{value:"The Problem",id:"the-problem",level:2},{value:"The Solution Journey",id:"the-solution-journey",level:2},{value:"1. First Attempt: SynchronousQueue",id:"1-first-attempt-synchronousqueue",level:3},{value:"2. Experiment: ArrayBlockingQueue + AbortPolicy",id:"2-experiment-arrayblockingqueue--abortpolicy",level:3},{value:"3. Final Decision: CallerRunsPolicy + Higher Max Pool Size",id:"3-final-decision-callerrunspolicy--higher-max-pool-size",level:3},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"More Benchmarking",id:"more-benchmarking",level:2},{value:"1) Comparing Max Pool Size (\xd74, \xd78, \xd716)",id:"1-comparing-max-pool-size-4-8-16",level:3},{value:"3) Comparing Queue Capacity (16 vs 64 vs 256)",id:"3-comparing-queue-capacity-16-vs-64-vs-256",level:3},{value:"Measured summary",id:"measured-summary",level:4},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const s={a:"a",br:"br",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(s.p,{children:["During the latest ",(0,i.jsx)(s.a,{href:"https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance",children:"REST API Performance load tests"}),",\nwe discovered that REST API requests suffered from significantly higher latency under CPU pressure, even when throughput numbers looked comparable.\nWhile adding more CPU cores alleviated the issue, this wasn\u2019t a sustainable solution \u2014 it hinted at an inefficiency in how REST handled broker responses.\nSee related ",(0,i.jsx)(s.a,{href:"https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance#request-handling-execution-logic",children:"section"})," from the previous blog post."]}),"\n",(0,i.jsxs)(s.p,{children:["This blog post is about how we diagnosed the issue, what we found, and the fix we introduced in ",(0,i.jsx)(s.a,{href:"https://github.com/camunda/camunda/pull/36517",children:"PR #36517"}),"\nto close the performance gap."]}),"\n",(0,i.jsx)(s.h2,{id:"the-problem",children:"The Problem"}),"\n",(0,i.jsxs)(s.p,{children:["A difference we spotted early between ",(0,i.jsx)(s.strong,{children:"REST API"})," and ",(0,i.jsx)(s.strong,{children:"gRPC"})," request handling was the usage of the ",(0,i.jsx)(s.code,{children:"BrokerClient"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"gRPC:"})," ",(0,i.jsx)(s.code,{children:"BrokerClient"})," calls are wrapped with retries and handled directly in the request thread."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"REST:"})," requests are executed without retries, and responses are handled asynchronously using the common ",(0,i.jsx)(s.code,{children:"ForkJoinPool"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["On clusters with 2 CPUs, the JVM defaults to a single thread for the common ",(0,i.jsx)(s.code,{children:"ForkJoinPool"}),".\nOur expectation was that this could cause contention: one thread might not be fast enough to process responses in time, leading to delays in the Gateway \u2194 Broker request-response cycle."]}),"\n",(0,i.jsx)(s.h2,{id:"the-solution-journey",children:"The Solution Journey"}),"\n",(0,i.jsx)(s.p,{children:"Solving this issue wasn\u2019t a straight line \u2014 we tried a few approaches before landing on the final design.\nEach iteration gave us valuable insights about Java\u2019s thread pool usage and its impact on REST API performance."}),"\n",(0,i.jsx)(s.h3,{id:"1-first-attempt-synchronousqueue",children:"1. First Attempt: SynchronousQueue"}),"\n",(0,i.jsxs)(s.p,{children:["We began with a custom ",(0,i.jsx)(s.code,{children:"ThreadPoolExecutor"})," that used a ",(0,i.jsx)(s.code,{children:"SynchronousQueue"})," for task handoff:"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-java",children:"new ThreadPoolExecutor(\n    corePoolSize,\n    maxPoolSize,\n    keepAliveSeconds,\n    TimeUnit.SECONDS,\n    new SynchronousQueue<>(),\n    threadFactory,\n    new ThreadPoolExecutor.CallerRunsPolicy());\n"})}),"\n",(0,i.jsxs)(s.p,{children:["A ",(0,i.jsx)(s.code,{children:"SynchronousQueue"})," has ",(0,i.jsx)(s.strong,{children:"no capacity"})," \u2014 each task submission must immediately find a free worker thread.\nIf no thread is available, the caller blocks until one frees up."]}),"\n",(0,i.jsxs)(s.p,{children:["In practice, this meant concurrency was ",(0,i.jsx)(s.strong,{children:"artificially limited"}),": bursts of REST requests had to wait for a thread to become available, reducing parallelism.\nThe results were modest:"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Request rate:"})," unchanged"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Average latency:"})," improved slightly (from ~120 ms \u2192 ~100 ms)"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"CPU throttling:"})," dropped only marginally (100% \u2192 90%)"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Before any changes introduced, results looked like this:\n",(0,i.jsx)(s.img,{alt:"syncqueue-req-before",src:t(1963).A+"",width:"1052",height:"1210"}),"\n",(0,i.jsx)(s.img,{alt:"syncqueue-cpu-before",src:t(53929).A+"",width:"2112",height:"674"})]}),"\n",(0,i.jsxs)(s.p,{children:["Then we started benchmarking after introducing executor with ",(0,i.jsx)(s.code,{children:"SynchronousQueue"}),":\n",(0,i.jsx)(s.img,{alt:"syncqueue-req-after",src:t(20030).A+"",width:"1050",height:"1218"}),"\n",(0,i.jsx)(s.img,{alt:"syncqueue-cpu-after",src:t(35224).A+"",width:"2098",height:"684"})]}),"\n",(0,i.jsx)(s.p,{children:"This hinted we were on the right track \u2014 a dedicated executor helped \u2014 but the queue strategy was too restrictive."}),"\n",(0,i.jsx)(s.h3,{id:"2-experiment-arrayblockingqueue--abortpolicy",children:"2. Experiment: ArrayBlockingQueue + AbortPolicy"}),"\n",(0,i.jsxs)(s.p,{children:["Next, we switched to an ",(0,i.jsx)(s.code,{children:"ArrayBlockingQueue"})," with a capacity of 64.\nThis allowed the pool to ",(0,i.jsx)(s.strong,{children:"buffer short micro-bursts"})," of requests instead of blocking immediately.\nAt the same time, we replaced ",(0,i.jsx)(s.code,{children:"CallerRunsPolicy"})," with ",(0,i.jsx)(s.code,{children:"AbortPolicy"}),":"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-java",children:"new ThreadPoolExecutor.AbortPolicy();\n"})}),"\n",(0,i.jsxs)(s.p,{children:["The idea was to ",(0,i.jsx)(s.strong,{children:"fail fast"})," on saturation: if the queue filled up and no thread was free, the executor would throw ",(0,i.jsx)(s.code,{children:"RejectedExecutionException"})," immediately.\nThis boosted the measured request rate, but at a cost:"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Many requests were simply rejected outright."}),"\n",(0,i.jsx)(s.li,{children:"Measuring true performance became tricky, since high throughput numbers hid the rejections."}),"\n",(0,i.jsx)(s.li,{children:"Operationally, this wasn\u2019t acceptable \u2014 REST clients would constantly see errors under load."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Because of this, we abandoned the fail-fast approach."}),"\n",(0,i.jsx)(s.h3,{id:"3-final-decision-callerrunspolicy--higher-max-pool-size",children:"3. Final Decision: CallerRunsPolicy + Higher Max Pool Size"}),"\n",(0,i.jsxs)(s.p,{children:["Finally, we returned to ",(0,i.jsx)(s.code,{children:"CallerRunsPolicy"}),".\nInstead of rejecting tasks, this policy makes the caller thread execute the task itself when the pool is saturated.\nThis introduces ",(0,i.jsx)(s.strong,{children:"natural backpressure"}),": clients slow down automatically when the system is busy, without dropping requests."]}),"\n",(0,i.jsxs)(s.p,{children:["To give the executor more headroom, we also increased the maximum pool size from ",(0,i.jsx)(s.code,{children:"availableProcessors * 2"})," to ",(0,i.jsx)(s.code,{children:"availableProcessors * 8"}),"."]}),"\n",(0,i.jsx)(s.p,{children:"This combination made the breakthrough:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Request rate (REST):"})," stabilized around 150 RPS with spikes up to 200 RPS"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Average latency:"})," dropped dramatically (from ~120 ms \u2192 ~25 ms)"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"CPU throttling:"})," reduced significantly (100% \u2192 30-40%)"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Here are the final results:\n",(0,i.jsx)(s.img,{alt:"final-decision-result-rest",src:t(54566).A+"",width:"2338",height:"1202"}),"\n",(0,i.jsx)(s.img,{alt:"final-decision-result-cpu",src:t(19744).A+"",width:"2332",height:"672"})]}),"\n",(0,i.jsx)(s.p,{children:"This design struck the right balance: elastic concurrency, backpressure, and resource efficiency."}),"\n",(0,i.jsx)(s.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"SynchronousQueue limits concurrency"})," \u2014 good for handoff semantics, but too restrictive for REST workloads."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Fail-fast rejection looks good in benchmarks but fails in production"})," \u2014 clients can\u2019t handle widespread request errors."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"CallerRunsPolicy provides natural backpressure"})," \u2014 throughput stabilizes without dropping requests, and latency improves."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"CPU-aware max pool sizing matters"})," \u2014 scaling pool size relative to cores unlocks performance gains."]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"more-benchmarking",children:"More Benchmarking"}),"\n",(0,i.jsx)(s.p,{children:"To validate that our executor change holds up across configurations, we ran extra tests on our benchmark cluster provisioned with 2 vCPUs per Camunda application."}),"\n",(0,i.jsx)(s.h3,{id:"1-comparing-max-pool-size-4-8-16",children:"1) Comparing Max Pool Size (\xd74, \xd78, \xd716)"}),"\n",(0,i.jsxs)(s.p,{children:["We ran the same workload while varying ",(0,i.jsx)(s.code,{children:"maxPoolSize = availableProcessors \xd7 {4, 8, 16}"}),". Below are the observed tops from Grafana panels in the screenshots:"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"maxPoolSizeMultiplier=4"}),"\n",(0,i.jsx)(s.img,{alt:"max-pool-size-multiplier-4",src:t(64632).A+"",width:"2316",height:"1212"})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"maxPoolSizeMultiplier=8"}),"\n",(0,i.jsx)(s.img,{alt:"max-pool-size-multiplier-8",src:t(15092).A+"",width:"2338",height:"1202"})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"maxPoolSizeMultiplier=16"}),"\n",(0,i.jsx)(s.img,{alt:"max-pool-size-multiplier-16",src:t(59383).A+"",width:"2332",height:"1204"})]}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Multiplier"}),(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Request Rate (proc-instances)"}),(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Request Rate (completion)"}),(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Avg Latency (proc-instances)"}),(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Avg Latency (completion)"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"\xd74"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~51.6 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~42.3 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~40.2 ms"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~57.4 ms"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"\xd78"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~144.4 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~144.7 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~21.4 ms"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~24.1 ms"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"\xd716"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~22.7 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~19.5 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~38.4 ms"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~55.4 ms"})]})]})]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"What this suggests (in our setup):"})}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"\xd78"})," is the clear ",(0,i.jsx)(s.strong,{children:"sweet spot"}),": highest sustained throughput with the lowest average latencies."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"\xd74"})," under-provisions the pool (lower RPS, higher latency)."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"\xd716"})," shows ",(0,i.jsx)(s.strong,{children:"diminishing/negative returns"})," (likely scheduler contention or oversubscription): much lower RPS and latencies drifting back up."]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Takeaway:"})," In our setup, \xd78 balances elasticity and scheduling overhead, delivering the best throughput\u2013latency trade-off."]}),"\n",(0,i.jsx)(s.h3,{id:"3-comparing-queue-capacity-16-vs-64-vs-256",children:"3) Comparing Queue Capacity (16 vs 64 vs 256)"}),"\n",(0,i.jsxs)(s.p,{children:["We varied the executor ",(0,i.jsx)(s.strong,{children:"queue capacity"})," and compared ",(0,i.jsx)(s.strong,{children:"16"}),", ",(0,i.jsx)(s.strong,{children:"64"})," (our current/default for this run), and ",(0,i.jsx)(s.strong,{children:"256"})," under the same workload.",(0,i.jsx)(s.br,{}),"\n","Below are the observed tops from the Grafana panels for the two hot endpoints:"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"POST /v2/process-instances"}),"\n",(0,i.jsx)(s.li,{children:"POST /v2/jobs/{jobKey}/completion"}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"queueCapacity=16"}),(0,i.jsx)(s.br,{}),"\n",(0,i.jsx)(s.img,{alt:"queue-capacity-16",src:t(61140).A+"",width:"2338",height:"1208"})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"queueCapacity=64"}),(0,i.jsx)(s.br,{}),"\n",(0,i.jsx)(s.img,{alt:"queue-capacity-64",src:t(20715).A+"",width:"2338",height:"1202"})]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"queueCapacity=256"}),(0,i.jsx)(s.br,{}),"\n",(0,i.jsx)(s.img,{alt:"queue-capacity-256",src:t(32922).A+"",width:"2338",height:"1216"})]}),"\n",(0,i.jsx)(s.h4,{id:"measured-summary",children:"Measured summary"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Queue Capacity"}),(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Request Rate (proc-instances)"}),(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Request Rate (completion)"}),(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Avg Latency (proc-instances)"}),(0,i.jsx)(s.th,{style:{textAlign:"right"},children:"Avg Latency (completion)"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"16"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~78.2 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~56.0 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~28.2 ms"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~40.8 ms"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"64"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~144.4 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~144.7 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~21.4 ms"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~24.1 ms"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"256"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~80.2 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~61.2 req/s"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~29.3 ms"}),(0,i.jsx)(s.td,{style:{textAlign:"right"},children:"~43.0 ms"})]})]})]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"What this suggests (in our setup):"})}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Queue = 64"})," is the clear ",(0,i.jsx)(s.strong,{children:"sweet spot"}),": highest sustained throughput (~144 req/s on hot endpoints) with the lowest avg latencies (~21\u201324 ms). Likely large enough to absorb micro-bursts, but small enough to avoid long queue waits."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Queue = 16"})," under-buffers: lower RPS (~78 / ~56 req/s) and higher latency (~28\u201341 ms). With ",(0,i.jsx)(s.code,{children:"CallerRunsPolicy"}),", the queue fills quickly and the caller runs tasks often \u2192 frequent backpressure throttles producers."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Queue = 256"})," shows ",(0,i.jsx)(s.strong,{children:"diminishing/negative returns"})," relative to 64: lower RPS (~80 / ~61 req/s) and higher latency (~29\u201343 ms). The big buffer hides saturation, adding queueing delay before execution without delivering extra useful work at the same CPU budget."]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsxs)(s.p,{children:["Moving off the common ",(0,i.jsx)(s.code,{children:"ForkJoinPool"})," to a dedicated, CPU-aware executor with bounded queueing and ",(0,i.jsx)(s.code,{children:"CallerRunsPolicy"})," backpressure turned an overload problem into graceful degradation: fewer 5xxs, steadier RPS, and far lower tail latency under the same CPU budget."]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Final takeaways"})}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Isolation beats sharing."})," A dedicated pool prevents noisy neighbors from the common ",(0,i.jsx)(s.code,{children:"ForkJoinPool"}),"."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Backpressure beats drops."})," ",(0,i.jsx)(s.code,{children:"CallerRunsPolicy"})," slows producers when saturated, stabilizing the system without mass rejections."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Right-sized knobs matter."})," ",(0,i.jsx)(s.code,{children:"maxPoolSize \u2248 cores \xd7 8"})," and ",(0,i.jsx)(s.code,{children:"queueCapacity \u2248 64"})," hit the best throughput/latency balance in our runs; smaller queues over-throttle, larger queues hide saturation and add wait time."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Results are environment-specific."})," At higher core counts, the sweet spot may shift\u2014re-benchmark when CPUs or workload mix change."]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Note:"})," Results are environment-specific; at higher core counts, the sweet spot may shift\u2014re-benchmark when CPUs or workload mix change.\nIn this experiment, we focused on CPU-bound scenarios."]})]})}function d(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},53929:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/syncqueue-cpu-before-221093fdaa3aa155857222a6feec4e88.png"},54566:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/final-decision-result-rest-f324c1eef2158ab2f20dd5d688a4e89f.png"},59383:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/max-pool-size-multiplier-16-e1011e424e35c3178f468ef5bcc4656a.png"},61140:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/queue-capacity-16-516379e4fce7362827be0d3ad957e7dc.png"},64632:(e,s,t)=>{t.d(s,{A:()=>n});const n=t.p+"assets/images/max-pool-size-multiplier-4-61b29561f085e4079b30f5de0552bab5.png"}}]);