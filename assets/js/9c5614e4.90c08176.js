"use strict";(globalThis.webpackChunkzell_chaos=globalThis.webpackChunkzell_chaos||[]).push([[4258],{2427:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/88-one-task-3-cpu-7528fa79e363d11cee5cfa36e099c464.png"},9897:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/86-one-task-latency-673ec085a62fbff4b1e490088e70460e.png"},25627:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var a=t(62251),n=t(74848),i=t(28453);const r={layout:"posts",title:"Stress testing Camunda",date:new Date("2025-11-27T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["performance","stress-testing"],authors:"zell"},o="Chaos Day Summary",l={authorsImageUrls:[void 0]},c=[{value:"Chaos Experiment",id:"chaos-experiment",level:2},{value:"Setup",id:"setup",level:3},{value:"Load Generation",id:"load-generation",level:3},{value:"8.8.x Results",id:"88x-results",level:3},{value:"Single Service Task",id:"single-service-task",level:4},{value:"Increasing resources",id:"increasing-resources",level:5},{value:"8.7.x Results",id:"87x-results",level:3},{value:"Single Service Task",id:"single-service-task-1",level:4},{value:"8.6.x Results",id:"86x-results",level:3},{value:"Single Service Task",id:"single-service-task-2",level:4},{value:"Main",id:"main",level:3},{value:"Summary of Results",id:"summary-of-results",level:2},{value:"Next Steps",id:"next-steps",level:2}];function h(e){const s={a:"a",h2:"h2",h3:"h3",h4:"h4",h5:"h5",img:"img",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.p,{children:"In today's chaos experiment, we focused on stress-testing the Camunda 8 platform under high-load conditions. We simulated a large number of concurrent process instances to evaluate the system's performance and reliability."}),"\n",(0,n.jsxs)(s.p,{children:["Due to our recent work in supporting ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/issues/38829",children:"load tests for different versions"}),", we were able to compare how different Camunda versions handle stress."]}),"\n",(0,n.jsxs)(s.p,{children:[(0,n.jsx)(s.strong,{children:"TL;DR;"})," We found that Camunda 8.7.x performs best under high load, followed by the main branch and 8.6.x. The latest version 8.8.x showed lower throughput, but with increased resources, it was able to improve performance. Latency was lowest (best) in 8.8.x with increased resources."]}),"\n",(0,n.jsx)(s.h2,{id:"chaos-experiment",children:"Chaos Experiment"}),"\n",(0,n.jsxs)(s.p,{children:["Weekly, we run ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/blob/main/docs/testing/reliability-testing.md#variations",children:"endurance tests"})," to validate the stability of our systems. This time, we decided to push the limits further by increasing the load significantly and observing how Camunda handles the stress."]}),"\n",(0,n.jsx)(s.p,{children:"Additionally, we wanted to see how far we can go and how this differs between versions. Therefore, we compared the performance of Camunda 8.8.x and 8.7.x under identical stress conditions."}),"\n",(0,n.jsx)(s.h3,{id:"setup",children:"Setup"}),"\n",(0,n.jsxs)(s.p,{children:["Details on the setup can be read in our  ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/blob/main/docs/testing/reliability-testing.md#setup",children:"reliability testing"})," documentation."]}),"\n",(0,n.jsx)(s.p,{children:"Important to know is that the architecture has changed slightly over the versions. In 8.8.x, we have a single Camunda application deployed, whereas in earlier versions, we had a split architecture between broker and gateway. This change has implications for how the system handles load and scales."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"setup",src:t(44506).A+"",width:"1132",height:"553"})}),"\n",(0,n.jsx)(s.h3,{id:"load-generation",children:"Load Generation"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"setup-load",src:t(32776).A+"",width:"5338",height:"2427"})}),"\n",(0,n.jsxs)(s.p,{children:["We have a custom ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/tree/main/load-tests/load-tester",children:"load generation application"})," (split into starter and worker applications), which we deploy separately from Camunda. The starter creates process instances at a configurable rate, while the worker completes the corresponding service tasks."]}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"one-task",src:t(43653).A+"",width:"996",height:"270"})}),"\n",(0,n.jsxs)(s.p,{children:["We will start simple, with a process that has a single service task. This is not very realistic, but it gives us a good sense of maximum load, as this is one of the smallest processes (including a service task) we can model. Reducing the used feature set to a small amount allows easy comparison between tests, as fewer variations and outside factors can influence test results. Additionally, it is a model we use in ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/blob/main/docs/testing/reliability-testing.md#normal-artificial-load",children:"our endurance test"})," as well, allowing us to compare it and know where to start. Our endurance tests usually have a load of 150 process instances per second (PI/s). Where the ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/blob/main/load-tests/load-tester/src/main/resources/bpmn/typical_payload.json",children:"payload is rather small ~0.5 KB"}),". In our stress test, the starter application will create process instances at a rate of 300 per second, while we will have six worker applications deployed processing the service tasks."]}),"\n",(0,n.jsx)(s.h3,{id:"88x-results",children:"8.8.x Results"}),"\n",(0,n.jsx)(s.h4,{id:"single-service-task",children:"Single Service Task"}),"\n",(0,n.jsx)(s.p,{children:"After setting up the load test environment and starting the load generation, we monitored the system's performance metrics, including CPU usage, memory consumption, latency (gateway response time and process instance execution time), and throughput (how many process instances, tasks, and flow-node instances were completed per second)."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"88-one-task-results",src:t(30686).A+"",width:"2525",height:"859"})}),"\n",(0,n.jsx)(s.p,{children:"Looking at the dashboard, we can see that we have reached the limit of our cluster. We have high back pressure (and a cluster load of nearly 100%). Our system is heavily CPU-throttled (~100% CPU utilization). This means that the system is not able to keep up with the incoming load of 300 PI/s."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"88-one-task-latency",src:t(97044).A+"",width:"2532",height:"188"})}),"\n",(0,n.jsx)(s.p,{children:"Interesting (or important) to note is that our backpressure mechanism allows us to keep the latency always steady and low. But not that low as compared to other versions, we will see later."}),"\n",(0,n.jsx)(s.h5,{id:"increasing-resources",children:"Increasing resources"}),"\n",(0,n.jsx)(s.p,{children:"Out of interest, I increased the resources of the cluster (increasing CPU to 3 cores; memory increase was not necessary as we were not reaching our limit)."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"88-one-task-3-cpu",src:t(2427).A+"",width:"2516",height:"872"})}),"\n",(0,n.jsx)(s.p,{children:"When increasing the CPU resources by adding one core, we were able to increase the throughput by ~37% (220/160=1.375). We are still not able to handle the full load of 300 PI/s."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"88-one-task-3-cpu-latency",src:t(94898).A+"",width:"2527",height:"196"})}),"\n",(0,n.jsxs)(s.p,{children:["The p50 latency has been decreased significantly, while the p99 is similar to before. In general, we are seeing only one pod being CPU throttled. Likely related to the fact that we do not properly distribute our load across the gateways, see issue ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/issues/9870",children:"9870"}),"."]}),"\n",(0,n.jsx)(s.h3,{id:"87x-results",children:"8.7.x Results"}),"\n",(0,n.jsx)(s.h4,{id:"single-service-task-1",children:"Single Service Task"}),"\n",(0,n.jsx)(s.p,{children:"The results for 8.7.x are quite different. Here we can see that we are able to handle much higher load ~ 246 PI/s. The backpressure is lower, and the CPU usage (and throttling) is not as high as in 8.8.x."}),"\n",(0,n.jsxs)(s.p,{children:["Surprisingly, the memory usage is higher in 8.7.x compared to 8.8.x. Short research showed that the broker got in 8.7 ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/blob/stable/8.7/zeebe/benchmarks/camunda-platform-values.yaml#L163",children:"4GB of memory assigned"}),", and ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/blob/stable/8.7/zeebe/benchmarks/camunda-platform-values.yaml#L91",children:"25% is used by the JVM"}),". While in 8.8 we have ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/blob/main/load-tests/camunda-platform-values.yaml#L76",children:"2GB assigned to the Camunda application"}),". Additionally, in 8.8 we reduced the RocksDB memory (as ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/issues/31706#issuecomment-2944455152",children:"experiment"}),") to ",(0,n.jsx)(s.a,{href:"https://github.com/camunda/camunda/blob/main/load-tests/camunda-platform-values.yaml#L149",children:"64 MB per partition"})," (instead of previously 500MB). This should explain the difference."]}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"87-one-task-results",src:t(78115).A+"",width:"2524",height:"866"})}),"\n",(0,n.jsxs)(s.p,{children:["The latency is also lower in 8.7.x compared to 8.8.x. The p50 latency is approximately 170ms for gateway requests and 240ms for PI execution, while the p99 latency increases to 700ms under high load.\n",(0,n.jsx)(s.img,{alt:"87-one-task-latency",src:t(64646).A+"",width:"2524",height:"192"})]}),"\n",(0,n.jsx)(s.h3,{id:"86x-results",children:"8.6.x Results"}),"\n",(0,n.jsx)(s.h4,{id:"single-service-task-2",children:"Single Service Task"}),"\n",(0,n.jsx)(s.p,{children:"The results for 8.6.x are comparable to those of 8.8.x, with more resources, but still lower than those of 8.7.x. We can handle approximately 220 PI/s with similar backpressure and CPU throttling as in 8.8.x, using 3 CPU cores."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"86-one-task-results",src:t(71624).A+"",width:"2519",height:"848"})}),"\n",(0,n.jsx)(s.p,{children:"Even the latency is comparable to the first 8.8.x test, while the p99 is much higher."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"86-one-task-results",src:t(9897).A+"",width:"2526",height:"194"})}),"\n",(0,n.jsx)(s.h3,{id:"main",children:"Main"}),"\n",(0,n.jsx)(s.p,{children:"For comparison, I also ran the same test on the main branch (which will become 8.9.x). Here, the results are better than 8.8.x with 3 CPU cores. We are reaching ~230 PI/s with similar latency characteristics. Still, it is not as good as 8.7.x."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"main-one-task-results",src:t(99283).A+"",width:"2512",height:"851"})}),"\n",(0,n.jsx)(s.p,{children:"Looking at the latency, we can see that it is comparable to 8.7.x test results."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"main-one-task-results-latency",src:t(60918).A+"",width:"2524",height:"189"})}),"\n",(0,n.jsx)(s.h2,{id:"summary-of-results",children:"Summary of Results"}),"\n",(0,n.jsx)(s.p,{children:"In terms of throughput, 8.7.x performs best, followed by main and 8.6.x. Increasing resources in 8.8.x helps, but it still cannot match the performance of 8.7.x. Just looking at 8.7 and 8.8, this means a decrease of ~35% in throughput (160 / 246 = 0.65)."}),"\n",(0,n.jsx)(s.p,{children:"The latency is lowest in 8.8.x with increased resources, followed by main and 8.7.x. 8.6.x has the highest latency among the tested versions."}),"\n",(0,n.jsxs)(s.table,{children:[(0,n.jsx)(s.thead,{children:(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.th,{children:"Version"}),(0,n.jsx)(s.th,{children:"Throughput (PI/s)"}),(0,n.jsx)(s.th,{children:"p50 PI execution Latency (ms)"}),(0,n.jsx)(s.th,{children:"p99 PI execution Latency (ms)"}),(0,n.jsx)(s.th,{children:"CPU Throttling"})]})}),(0,n.jsxs)(s.tbody,{children:[(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:"8.7.x"}),(0,n.jsxs)(s.td,{children:["~",(0,n.jsx)(s.strong,{children:"246"})]}),(0,n.jsx)(s.td,{children:"~200"}),(0,n.jsx)(s.td,{children:"~700"}),(0,n.jsx)(s.td,{children:"80% one pod"})]}),(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:"8.6.x"}),(0,n.jsx)(s.td,{children:"~220"}),(0,n.jsx)(s.td,{children:"~400"}),(0,n.jsx)(s.td,{children:"~960"}),(0,n.jsx)(s.td,{children:"80+% all pods"})]}),(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:"8.8.x"}),(0,n.jsx)(s.td,{children:"~160"}),(0,n.jsx)(s.td,{children:"~370"}),(0,n.jsx)(s.td,{children:"~700"}),(0,n.jsx)(s.td,{children:"90+% all pods"})]}),(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:"8.8.x (3 CPU)"}),(0,n.jsx)(s.td,{children:"~220"}),(0,n.jsxs)(s.td,{children:["~",(0,n.jsx)(s.strong,{children:"90"})]}),(0,n.jsxs)(s.td,{children:["~",(0,n.jsx)(s.strong,{children:"490"})]}),(0,n.jsx)(s.td,{children:"80% one pod"})]}),(0,n.jsxs)(s.tr,{children:[(0,n.jsx)(s.td,{children:"Main"}),(0,n.jsx)(s.td,{children:"~230"}),(0,n.jsx)(s.td,{children:"~180"}),(0,n.jsx)(s.td,{children:"~497"}),(0,n.jsx)(s.td,{children:"90+% two pods"})]})]})]}),"\n",(0,n.jsxs)(s.p,{children:["Likely reasons for the performance differences could be related to ",(0,n.jsx)(s.a,{href:"https://camunda.com/blog/2025/03/streamlined-deployment-with-camunda-8-8/",children:"architectural changes"}),", like having identity part of the Camunda application, a new Camunda Exporter (doing all the work previously Importers have done), etc. Further investigation is needed to pinpoint the exact causes and potential optimizations."]}),"\n",(0,n.jsx)(s.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,n.jsx)(s.p,{children:"Based on the results above, we will continue our investigation into the performance differences between the versions. We will analyze the changes made in the architecture and codebase to identify potential bottlenecks or optimizations that could explain the observed performance variations. Likely, we will also need to examine the resource allocation and configuration settings to determine if any differences could impact performance."}),"\n",(0,n.jsx)(s.p,{children:"Furthermore, we plan to explore the impact of different process designs and workloads on the system's performance.\nAn everyday use case for Camunda is the implementation of straight-through processes that utilize service tasks. Therefore, we designed a simple BPMN process that consists of a start event, ten service tasks, some intermediate timer catch events, and an end event. The service tasks are configured to be handled by the worker application. In one of our next experiments, we will run the same stress test with this process model to see how the system handles more complex workflows under high load."}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"ten_tasks",src:t(60083).A+"",width:"2244",height:"1155"})})]})}function d(e={}){const{wrapper:s}={...(0,i.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},28453:(e,s,t)=>{t.d(s,{R:()=>r,x:()=>o});var a=t(96540);const n={},i=a.createContext(n);function r(e){const s=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),a.createElement(i.Provider,{value:s},e.children)}},30686:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/88-one-task-3fe2b9ee8e9ca7539d2de4c6301812f4.png"},32776:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/setup-load-test-5deae0786b63e79bdb4c7ea0d364deb9.jpg"},43653:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/one_task-aeef574e8d7b06603338f3bc9a58e1b2.png"},44506:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/setup-5eae3d5c5c46544bbf2c24214cb5634e.png"},60083:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/ten_tasks-41be0d9a138341d665c9ed2999e64ad2.png"},60918:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/main-one-task-latency-be0150b21d60104f732d3db3886003a3.png"},62251:e=>{e.exports=JSON.parse('{"permalink":"/zeebe-chaos/2025/11/27/Stress-testing-Camunda","editUrl":"https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-11-27-Stress-testing-Camunda/index.md","source":"@site/blog/2025-11-27-Stress-testing-Camunda/index.md","title":"Stress testing Camunda","description":"In today\'s chaos experiment, we focused on stress-testing the Camunda 8 platform under high-load conditions. We simulated a large number of concurrent process instances to evaluate the system\'s performance and reliability.","date":"2025-11-27T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/zeebe-chaos/tags/performance"},{"inline":true,"label":"stress-testing","permalink":"/zeebe-chaos/tags/stress-testing"}],"readingTime":6.535,"hasTruncateMarker":true,"authors":[{"name":"Christopher Kujawa","title":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","page":{"permalink":"/zeebe-chaos/authors/zell"},"imageURL":"https://github.com/ChrisKujawa.png","key":"zell"}],"frontMatter":{"layout":"posts","title":"Stress testing Camunda","date":"2025-11-27T00:00:00.000Z","categories":["chaos_experiment","bpmn"],"tags":["performance","stress-testing"],"authors":"zell"},"unlisted":false,"nextItem":{"title":"Testing retention of historical PIs in Camunda 8.8","permalink":"/zeebe-chaos/2025/10/31/Improvents-in-retention"}}')},64646:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/87-one-task-latency-8aca2a44fdf88d12a8168b274afee9a9.png"},71624:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/86-one-task-d709ee6d7bc6e2201b54b85031f9c846.png"},78115:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/87-one-task-1048bcfd445cb6cca31da22824cc485b.png"},94898:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/88-one-task-latency-3-cpu-564870c775b156cc68eb3f659a3b4ac9.png"},97044:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/88-latency-8568698a13378a7060876e7022a20b4c.png"},99283:(e,s,t)=>{t.d(s,{A:()=>a});const a=t.p+"assets/images/main-one-task-137e8390aa5feaf71f30329c0390afa8.png"}}]);