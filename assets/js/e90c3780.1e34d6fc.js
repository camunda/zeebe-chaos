"use strict";(self.webpackChunkzell_chaos=self.webpackChunkzell_chaos||[]).push([[9906],{50853:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var o=n(85893),a=n(11151);const i={layout:"posts",title:"Dynamic Scaling with Dataloss",date:new Date("2023-12-19T00:00:00.000Z"),categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"ole"},r="Chaos Day Summary",s={permalink:"/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss",editUrl:"https://github.com/zeebe-io/zeebe-chaos/blob/master/chaos-days/blog/2023-12-19-Dynamic-Scaling-with-Dataloss/index.md",source:"@site/blog/2023-12-19-Dynamic-Scaling-with-Dataloss/index.md",title:"Dynamic Scaling with Dataloss",description:"We continue our previous experiments with dynamically scaling by now also testing whether",date:"2023-12-19T00:00:00.000Z",formattedDate:"December 19, 2023",tags:[{label:"availability",permalink:"/zeebe-chaos/tags/availability"}],readingTime:4.425,hasTruncateMarker:!0,authors:[{name:"Ole Sch\xf6nburg",title:"Senior Software Engineer @ Zeebe",url:"https://github.com/oleschoenburg",imageURL:"https://github.com/oleschoenburg.png",key:"ole"}],frontMatter:{layout:"posts",title:"Dynamic Scaling with Dataloss",date:"2023-12-19T00:00:00.000Z",categories:["chaos_experiment","bpmn"],tags:["availability"],authors:"ole"},unlisted:!1,prevItem:{title:"Broker Scaling and Performance",permalink:"/zeebe-chaos/2023/12/20/Broker-scaling-performance"},nextItem:{title:"Dynamically scaling brokers",permalink:"/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers"}},l={authorsImageUrls:[void 0]},c=[{value:"Dataloss on the Coordinator",id:"dataloss-on-the-coordinator",level:2},{value:"Expected",id:"expected",level:3},{value:"Actual",id:"actual",level:3},{value:"Single point of failure during scaling",id:"single-point-of-failure-during-scaling",level:2}];function h(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",p:"p",pre:"pre",strong:"strong",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(t.p,{children:["We continue our ",(0,o.jsx)(t.a,{href:"/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers",children:"previous experiments"})," with dynamically scaling by now also testing whether\nthe cluster survives dataloss during the process."]}),"\n",(0,o.jsx)(t.p,{children:"One goal is to verify that we haven't accidentally introduced a single point of failure in the cluster.\nAnother is to ensure that data loss does not corrupt the cluster topology."}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.strong,{children:"TL;DR;"}),"\nEven with dataloss, the scaling completes successfully and with the expected results.\nWe found that during scaling, a single broker of the previous cluster configuration can become a single point of failure by preventing a partition from electing a leader.\nThis is not exactly a bug, but something that we want to improve."]}),"\n",(0,o.jsx)(t.h2,{id:"dataloss-on-the-coordinator",children:"Dataloss on the Coordinator"}),"\n",(0,o.jsx)(t.p,{children:"Zeebe uses Broker 0 as the coordinator for changes to the cluster topology.\nWhile changes can only be initiated by the coordinator, losing the coordinator and it's data should not prevent the scaling operation from completing.\nWhen the coordinator restarts without any data, it should be able to recover the cluster topology as well as the partition data from the remaining brokers."}),"\n",(0,o.jsxs)(t.p,{children:["To test this, we use the ",(0,o.jsx)(t.code,{children:"zbchaos dataloss delete"})," and ",(0,o.jsx)(t.code,{children:"zeebe dataloss recover"})," commands.\nAfter deleting, the broker will not restart directly, instead waiting for the ",(0,o.jsx)(t.code,{children:"zbchaos dataloss recover"})," command to be executed.\nThe ",(0,o.jsx)(t.code,{children:"zbchaos dataloss recover"})," command only unblocks the broker and allows it to start, it does not restore any data and we rely on normal replication for that."]}),"\n",(0,o.jsxs)(t.p,{children:["Shortly after triggering a scale up with ",(0,o.jsx)(t.code,{children:"zbchaos cluster scale --brokers 6"}),", we trigger dataloss on the coordinator with ",(0,o.jsx)(t.code,{children:"zbchaos broker dataloss delete --nodeId 0"}),".\nAfter observing the system for a while, we then restore the coordinator with ",(0,o.jsx)(t.code,{children:"zbchaos dataloss recover --nodeId 0"}),"."]}),"\n",(0,o.jsx)(t.h3,{id:"expected",children:"Expected"}),"\n",(0,o.jsx)(t.p,{children:"The scaling operation eventually completes with the expected result of 6 brokers and 6 partitions, evenly distributed.\nThe coordinator recovers after dataloss and eventually receives the cluster topology from the remaining brokers.\nThe scaling operation should make progress while the coordinator is down."}),"\n",(0,o.jsx)(t.h3,{id:"actual",children:"Actual"}),"\n",(0,o.jsxs)(t.p,{children:["After starting the operation with ",(0,o.jsx)(t.code,{children:"zbchaos cluster scale --brokers 6"})," we see that the operation has started:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"$ zbchaos cluster scale --brokers 6\nChange 18 is IN_PROGRESS with 0/24 operations complete\n...\n"})}),"\n",(0,o.jsxs)(t.p,{children:["We then trigger dataloss on the coordinator with ",(0,o.jsx)(t.code,{children:"zbchaos broker dataloss delete --nodeId 0"}),"."]}),"\n",(0,o.jsx)(t.p,{children:"After this, the operations do not make progress anymore and broker 5 is stuck trying to join partition 5:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-json",children:'{\n  "Version": 18,\n  ...\n  "PendingChange": {\n    "Id": 18,\n    "Status": "IN_PROGRESS",\n    "StartedAt": "",\n    "CompletedAt": "",\n    "InternalVersion": 0,\n    "Completed": [\n      {\n        "Operation": "BROKER_ADD",\n        "BrokerId": 3,\n        "PartitionId": 0,\n        "Priority": 0\n      },\n      {\n        "Operation": "BROKER_ADD",\n        "BrokerId": 4,\n        "PartitionId": 0,\n        "Priority": 0\n      },\n      {\n        "Operation": "BROKER_ADD",\n        "BrokerId": 5,\n        "PartitionId": 0,\n        "Priority": 0\n      }\n    ],\n    "Pending": [\n      {\n        "Operation": "PARTITION_JOIN",\n        "BrokerId": 5,\n        "PartitionId": 5,\n        "Priority": 2\n      },\n      ...\n    ]\n  }\n}\n'})}),"\n",(0,o.jsxs)(t.p,{children:["The coordinator is a member of partition 5 but there are two remaining members of partition 5 that should allow broker 5 to join.\n",(0,o.jsx)(t.img,{src:n(32055).Z+"",width:"1288",height:"493"})]}),"\n",(0,o.jsxs)(t.p,{children:["After restoring the coordinator again with ",(0,o.jsx)(t.code,{children:"zbchaos dataloss recover --nodeId 0"}),", joining eventually completes and the scaling operation finishes successfully."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:n(9839).Z+"",width:"840",height:"296"})}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-json",children:'{\n  "Version": 19,\n  "LastChange": {\n    "Id": 18,\n    "Status": "COMPLETED",\n    "StartedAt": "2023-12-18T17:05:55.849442157Z",\n    "CompletedAt": "2023-12-18T17:17:32.913050015Z"\n  },\n  "PendingChange": null\n}\n'})}),"\n",(0,o.jsx)(t.p,{children:"Overall, we achieve our goal that the scaling operation eventually completes with the expected result.\nThe coordinator recovers after dataloss and eventually receives the cluster topology from the remaining brokers."}),"\n",(0,o.jsx)(t.p,{children:"However, it was unexpected that the scaling did not make progress while the coordinator was down."}),"\n",(0,o.jsx)(t.h2,{id:"single-point-of-failure-during-scaling",children:"Single point of failure during scaling"}),"\n",(0,o.jsx)(t.p,{children:"The issue that scaling did not make progress while the coordinator was reproducible.\nEventually, we diagnosed it as the following edge case:"}),"\n",(0,o.jsxs)(t.p,{children:["When scaling up and and adding a new member to the replication group of a partition, the raft partition goes through joint consensus.\nThe details of this process are described in the ",(0,o.jsx)(t.a,{href:"https://raft.github.io/raft.pdf",children:"raft paper"}),", but here is a very short summary:\nJoint consensus is similar to a 2-phase commit, where the leader of the partition first introduces a new ",(0,o.jsx)(t.em,{children:"joint consensus"}),' configuration that requires quorum from both the old and new set of members.\nAfter committing the joint consensus configuration, the leader leaves joint consensus by "forgetting" the old member set and only using the new member set.\nOnly after this second configuration is committed, joining of the new member is complete.']}),"\n",(0,o.jsx)(t.p,{children:"In our example, the new set of members has size 4, one of which is the coordinator and one is the newly joining member.\nWith 4 members, the quorum is 3, meaning that the partition can only elect a leader and process if at least 3 members are available.\nIn our experiment, we made the coordinator unavailable, so we were already down to 3 members.\nAdditionally, the newly joining member did not start yet because it was waiting for a successful join response from the leader.\nThe newly joining member never received such a response because the joint-consensus phase was not completed.\nThis resulted in only 2 out of 4 members being available, which is not enough to elect a leader."}),"\n",(0,o.jsx)(t.p,{children:"We want to improve this behavior in the future but likely can't prevent it completely.\nThat means that there is an increased risk of unavailable partitions during scaling.\nHowever, this only occurs if another broker becomes unavailable with an unfortunate timing and resolves itself automatically once the broker is available again."}),"\n",(0,o.jsxs)(t.p,{children:["Zeebe issue: ",(0,o.jsx)(t.a,{href:"https://github.com/camunda/camunda/issues/15679",children:"https://github.com/camunda/camunda/issues/15679"})]})]})}function d(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},32055:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/p5-roles-b75bd7b3101a80b7f59746241d1cc2e8.png"},9839:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/scaleup-complete-0918e350110d8454d47295f3c48a2e8e.png"},11151:(e,t,n)=>{n.d(t,{Z:()=>s,a:()=>r});var o=n(67294);const a={},i=o.createContext(a);function r(e){const t=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),o.createElement(i.Provider,{value:t},e.children)}}}]);