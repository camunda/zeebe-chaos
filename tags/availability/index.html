<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">38 posts tagged with &quot;availability&quot; | Zeebe Chaos</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://camunda.github.io/zeebe-chaos/tags/availability"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="38 posts tagged with &quot;availability&quot; | Zeebe Chaos"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/zeebe-chaos/img/zeebe-logo.png"><link data-rh="true" rel="canonical" href="https://camunda.github.io/zeebe-chaos/tags/availability"><link data-rh="true" rel="alternate" href="https://camunda.github.io/zeebe-chaos/tags/availability" hreflang="en"><link data-rh="true" rel="alternate" href="https://camunda.github.io/zeebe-chaos/tags/availability" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/zeebe-chaos/rss.xml" title="Zeebe Chaos RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zeebe-chaos/atom.xml" title="Zeebe Chaos Atom Feed"><link rel="stylesheet" href="/zeebe-chaos/assets/css/styles.ca109a4b.css">
<script src="/zeebe-chaos/assets/js/runtime~main.07108964.js" defer="defer"></script>
<script src="/zeebe-chaos/assets/js/main.f0dee652.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/zeebe-chaos/img/zeebe-logo.png"><link rel="preload" as="image" href="https://github.com/berkaycanbc.png"><link rel="preload" as="image" href="https://github.com/ChrisKujawa.png"><link rel="preload" as="image" href="https://github.com/entangled90.png"><link rel="preload" as="image" href="https://github.com/rodrigo-lourenco-lopes.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zeebe-chaos/"><div class="navbar__logo"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Zeebe Chaos</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zeebe-chaos/">Chaos Summaries</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/camunda/zeebe-chaos" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/12/11/Building-Confidence-at-Scale:-How-Camunda-Ensures-Platform-Reliability-Through-Continuous-Testing">Building Confidence at Scale: How Camunda Ensures Platform Reliability Through Continuous Testing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/11/27/Stress-testing-Camunda">Stress testing Camunda</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/10/31/Improvents-in-retention">Testing retention of historical PIs in Camunda 8.8</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/10/02/Dynamic-Scaling-Resilience">Resilience of dynamic scaling</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool">REST API: From ForkJoin to a Dedicated Thread Pool</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/08/26/Resiliency-against-ELS-unavailability">Resiliency against ELS unavailability</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/07/11/linear-dynamic-scaling">Dynamic Scaling: probing linear scalability</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance">Follow up REST API performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/30/Performance-of-REST-API">Performance of REST API</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS">How does Zeebe behave with NFS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment">Lower memory consumption of Camunda deployment</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project">News from Camunda Exporter project</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance">Impact of Camunda Exporter on processing performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">Camunda Exporter MVP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark">Optimizing cluster sizing using a real world benchmark</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency">Improve Operate import latency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/16/Operate-load-handling">Operate load handling</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting">Using flow control to handle bottleneck on exporting</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops">Using flow control to handle uncontrolled process loops</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/01/19/Job-Activation-Latency">Reducing the job activation delay</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/20/Broker-scaling-performance">Broker Scaling and Performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss">Dynamic Scaling with Dataloss</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers">Dynamically scaling brokers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/06/Job-Push-resiliency">Job push resiliency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/30/Job-push-overloading">Job push overloading</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing">Hot backups impact on processing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance">Using Large Multi-Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle">Continuing SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/15/SST-Partitioning-toggle">SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/04/06/gateway-termination">Gateway Termination</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/02/23/Recursive-call-activity">Recursive call activity</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition">Message Correlation after Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/02/deployment-distribution">Bring Deployment distribution experiment back</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS">Standalone Gateway in CCSaaS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/01/High-Snapshot-Frequency">High Snapshot Frequency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/01/19/big-variables">Handling of Big Variables</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2021</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance">Worker count should not impact performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys">Not produce duplicate Keys</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/29/Throughput-on-big-state">Throughput on big state</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/05/recovery-time">Recovery (Fail Over) time</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/09/23/Old-Clients">Old-Clients</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/07/06/Slow-Network">Slow Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/06/08/Full-Disk">Full Disk Recovery</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/05/25/Reset-Clock">Time travel Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/29/Corrupted-Snapshot">Corrupted Snapshot Experiment Investigation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering">BPMN meets Chaos Engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/30/set-file-immutable">Set file immutable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/23/camunda-cloud-network-partition">Camunda Cloud network partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/09/cont-workflow-instance">Fault-tolerant processing of process instances</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/02/23/automate-deployments-dist">Automating Deployment Distribution Chaos Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/26/deployments">Deployment Distribution</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/19/network-partition">Network partitions</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/07/disconnect-leader-and-follower">Disconnect Leader and one Follower</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2020</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/24/message-correlation-after-failover">Message Correlation after Failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/11/job-timeouts">Many Job Timeouts</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/03/investigate-failing-tests">Investigate failing Chaos Tests</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/20/non-graceful-shutdown">Non-graceful Shutdown Broker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/27/standalone-gw-memory">Gateway memory consumption</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/13/multiple-leader-changes">Multiple Leader Changes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/06/toxi-proxy">Play around with ToxiProxy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud">Experiment with Camunda Cloud</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/06/low-load">Experiment with Low Load</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/30/experiment-without-exporters">Experiment without Exporters</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/16/big-multi-instance">Big Multi Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/09/timer-and-huge-variables">Experiment with Timers and Huge Variables</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/02/extract-k8-resources">Extract K8 resources from namespace</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/25/gateway-network-partition">Gateway Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/18/correlate-message-after-failover">Correlate Message after failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/11/high-cpu-gateway">High CPU load on Standalone Gateway</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/04/first-chaos-day">First Chaos Day!</a></li></ul></div></nav></aside><main class="col col--7"><header class="margin-bottom--xl"><h1>38 posts tagged with &quot;availability&quot;</h1><a href="/zeebe-chaos/tags">View All Tags</a></header><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool">REST API: From ForkJoin to a Dedicated Thread Pool</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-09-19T00:00:00.000Z">September 19, 2025</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/berkaycanbc" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/berkaycanbc.png" alt="Berkay Can"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/berkaycanbc" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Berkay Can</span></a></div><small class="authorTitle_nd0D" title="Software Engineer @ Zeebe">Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>During the latest <a href="https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance" target="_blank" rel="noopener noreferrer" class="">REST API Performance load tests</a>,
we discovered that REST API requests suffered from significantly higher latency under CPU pressure, even when throughput numbers looked comparable.
While adding more CPU cores alleviated the issue, this wasn’t a sustainable solution — it hinted at an inefficiency in how REST handled broker responses.
See related <a href="https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance#request-handling-execution-logic" target="_blank" rel="noopener noreferrer" class="">section</a> from the previous blog post.</p>
<p>This blog post is about how we diagnosed the issue, what we found, and the fix we introduced in <a href="https://github.com/camunda/camunda/pull/36517" target="_blank" rel="noopener noreferrer" class="">PR #36517</a>
to close the performance gap.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about REST API: From ForkJoin to a Dedicated Thread Pool" href="/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/08/26/Resiliency-against-ELS-unavailability">Resiliency against ELS unavailability</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-08-26T00:00:00.000Z">August 26, 2025</time> · <!-- -->11 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp" translate="no">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Principal Software Engineer @ Camunda">Principal Software Engineer @ Camunda</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>Due to recent initiatives and architecture changes, we coupled us even more against the secondary storage (often Elasticsearch, but can also be OpenSearch or in the future RDBMS).</p>
<p>We now have one single application to run Webapps, Gateway, Broker, Exporters, etc., together. Including the new Camunda Exporter exporting all necessary data to the secondary storage. On bootstrap we need to create an expected schema, so our components work as expected, allowing Operate and Tasklist Web apps to consume the data and the exporter to export correctly. Furthermore, we have a new query API (REST API) allowing the search for available data in the secondary storage.</p>
<p>We have seen in previous experiments and load tests that unavailable ELS and not properly configured replicas can cause issues like the exporter not catching up or queries not succeeding. See related <a href="https://github.com/camunda/camunda/issues/35080" target="_blank" rel="noopener noreferrer" class="">GitHub issue</a>.</p>
<p>In todays chaos day, we want to play around with the replicas setting of the indices, which can be set in the Camunda Exporter (which is in charge of writing the data to the secondary storage).</p>
<p><strong>TL;DR;</strong> Without the index replicas set, the Camunda Exporter is directly impacted by ELS node restarts. The query API seem to handle this transparently, but changing the resulting data. Having the replicas set will cause some performance impact, as the ELS node might run into CPU throttling (as they have much more to do). ELS slowing down has an impact on processing as well due to our write throttling mechanics. This means we need to be careful with this setting, while it gives us better availability (CamundaExporter can continue when ELS nodes restart), it might come with some cost.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Resiliency against ELS unavailability" href="/zeebe-chaos/2025/08/26/Resiliency-against-ELS-unavailability"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/07/11/linear-dynamic-scaling">Dynamic Scaling: probing linear scalability</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-07-11T00:00:00.000Z">July 11, 2025</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/entangled90" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/entangled90.png" alt="Carlo Sana"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/entangled90" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Carlo Sana</span></a></div><small class="authorTitle_nd0D" title="Senior Software Engineer @ Zeebe">Senior Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="hypothesis">Hypothesis<a href="#hypothesis" class="hash-link" aria-label="Direct link to Hypothesis" title="Direct link to Hypothesis" translate="no">​</a></h2>
<p>The objective of this chaos day is to estimate the scalability of Zeebe when brokers and partitions are
scaled together: we expect to be able to see the system scaling linearly with the number of brokers/partition
in terms of throughput and back pressure, while maintaining predictable latency.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="general-experiment-setup">General Experiment setup<a href="#general-experiment-setup" class="hash-link" aria-label="Direct link to General Experiment setup" title="Direct link to General Experiment setup" translate="no">​</a></h3>
<p>To test this, we ran a benchmark using the latest alpha version of Camunda 8.8.0-alpha6, with the old
<code>ElasticsearchExporter</code> disabled, and the new <code>CamundaExporter</code> enabled. We also made sure Raft
leadership was balanced before starting the test, meaning each broker is leader for exactly one partition,
and we turned on partition scaling by adding the following environment variable:</p>
<ul>
<li class=""><code>ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEPARTITIONSCALING=true</code></li>
</ul>
<p>Each broker also has a SSD-class volume with 32GB of disk space, limiting them to a few thousand IOPS.
The processing load was 150 processes per second, with a large payload of 32KiB each. Each process instance has
a single service task:</p>
<p><img decoding="async" loading="lazy" alt="one-task" src="/zeebe-chaos/assets/images/one_task-f083f237e568d87cc17eef056cb45d73.png" width="999" height="276" class="img_ev3q"></p>
<p>The processing load is generated by our own <a href="https://github.com/camunda/camunda/tree/9e723b21b0e408fc2b97fd7d3f6b092af8e62dbe/benchmarks" target="_blank" rel="noopener noreferrer" class="">benchmarking application</a>.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="initial-cluster-configuration">Initial cluster configuration<a href="#initial-cluster-configuration" class="hash-link" aria-label="Direct link to Initial cluster configuration" title="Direct link to Initial cluster configuration" translate="no">​</a></h4>
<p>To test this hypothesis, we will start with a <em>standard</em> configuration of the <em>Camunda orchestration cluster</em>:</p>
<ul>
<li class="">3 nodes</li>
<li class="">3 partitions</li>
<li class="">CPU limit: 2</li>
<li class="">Memory limit: 2 GB</li>
</ul>
<p>We will increase the load through a load generator in fixed increments until we start to see the nodes showing constant non zero backpressure,
which is a sign that the system has hit its throughput limits.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="target-cluster-configuration">Target cluster configuration<a href="#target-cluster-configuration" class="hash-link" aria-label="Direct link to Target cluster configuration" title="Direct link to Target cluster configuration" translate="no">​</a></h4>
<p>Once that level of throughput is increased, we will scale broker &amp; partitions <strong>while the cluster is under load</strong> to the new target value:</p>
<ul>
<li class="">6 nodes</li>
<li class="">6 partitions</li>
<li class="">CPU limit: 2</li>
<li class="">Memory limit: 2 GB</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="experiment">Experiment<a href="#experiment" class="hash-link" aria-label="Direct link to Experiment" title="Direct link to Experiment" translate="no">​</a></h4>
<p>We expect that during the scaling operation the backpressure/latencies might worsen,
but only temporarily, as once the scaling operation has completed,
the additional load it generate is not present anymore.</p>
<p>Then, we will execute the same procedure as above,
until we hit 2x the critical throughput hit before.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="expectation">Expectation<a href="#expectation" class="hash-link" aria-label="Direct link to Expectation" title="Direct link to Expectation" translate="no">​</a></h4>
<p>If the system scales linearly, we expect to see similar level of performance metrics
for similar values of the ratios <code>PI (created/complete) per second / nr. of partition</code>.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="steady-state">Steady state<a href="#steady-state" class="hash-link" aria-label="Direct link to Steady state" title="Direct link to Steady state" translate="no">​</a></h2>
<p>The system is started with a throughput of 150 Process instances created per second.
As this is a <strong>standard benchmark configuration</strong>, nothing unexpected happens:</p>
<ul>
<li class="">The same number of process instances are completed as the ones created</li>
<li class="">The expected number of jobs is completed per unit of time</li>
</ul>
<p>At this point, we have the following topology:</p>
<p><img decoding="async" loading="lazy" alt="initial-topology" src="/zeebe-chaos/assets/images/initial-topology-37fddc92ed46f6b20ae0cfb7ff0cac27.png" width="1585" height="192" class="img_ev3q"></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="first-benchmark-3-broker-and-3-partitions">First benchmark: 3 broker and 3 partitions<a href="#first-benchmark-3-broker-and-3-partitions" class="hash-link" aria-label="Direct link to First benchmark: 3 broker and 3 partitions" title="Direct link to First benchmark: 3 broker and 3 partitions" translate="no">​</a></h3>
<p>Let&#x27;s start increasing the load incrementally, by adding 30 Process instances/s for every step.</p>
<table><thead><tr><th>Time</th><th>Brokers</th><th>Partitions</th><th>Throughput</th><th>CPU Usage</th><th>Throttling (CPU)</th><th>Backpressure</th></tr></thead><tbody><tr><td>09:30</td><td>3</td><td>3</td><td>150 PI/s, 150 jobs/s</td><td>1.28 / 1.44 / 1.02</td><td>12% / 7% / 1%</td><td>0</td></tr><tr><td>09:49</td><td>3</td><td>3</td><td>180 PI/s, 180 jobs/s</td><td>1.34 / 1.54 / 1.12</td><td>20% / 17% / 2%</td><td>0</td></tr><tr><td>10:00</td><td>3</td><td>3</td><td>210 PI/s, 210 jobs/s</td><td>1.79 / 1.62 / 1.33</td><td>28% / 42% / 4%</td><td>0</td></tr><tr><td>10:12</td><td>3</td><td>3</td><td>240 PI/s, 240 jobs/s</td><td>1.77 / 1.95 / 1.62</td><td>45% / 90% / 26%</td><td>0/0.5%</td></tr></tbody></table>
<p>At 240 Process Instances spawned per second, the system starts to hit the limits:
<img decoding="async" loading="lazy" alt="CPU usage @ 240 PI/s" src="/zeebe-chaos/assets/images/config_1_240_cpu-8520ff99e75fea10d3add1bf720b21ab.png" width="1702" height="1127" class="img_ev3q">
<img decoding="async" loading="lazy" alt="CPU throttling@ 240 PI/s" src="/zeebe-chaos/assets/images/config_1_240_cpu_throttling-0f8daf817e83828dffb1f5468577e753.png" width="1702" height="1127" class="img_ev3q"></p>
<p>And the backpressure is not zero anymore:
<img decoding="async" loading="lazy" alt="Backpressure @ 240 PI/s" src="/zeebe-chaos/assets/images/config_1_240_backpressure-dafc5cbae6bd06f5ed054fa4a1b87377.png" width="1702" height="1127" class="img_ev3q"></p>
<ul>
<li class="">The CPU throttling reaches almost 90% on one node (this is probably caused by only one node being selected as <strong>gateway</strong> as previously noted)</li>
<li class="">Backpressure is now constantly above zero, even if it&#x27;s just 0.5%, it&#x27;s a sign that we are reaching the throughput limits.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="second-part-of-the-benchmark-scaling-to-6-brokers-and-6-partitions">Second part of the benchmark: scaling to 6 brokers and 6 partitions<a href="#second-part-of-the-benchmark-scaling-to-6-brokers-and-6-partitions" class="hash-link" aria-label="Direct link to Second part of the benchmark: scaling to 6 brokers and 6 partitions" title="Direct link to Second part of the benchmark: scaling to 6 brokers and 6 partitions" translate="no">​</a></h3>
<p>With 240 process instances per second being spawned, we send the commands to scale the cluster.</p>
<p>We first scale the <code>zeebe</code> <em>statefulset</em> to 6 brokers. As soon as the new brokers are running, even before they are healthy,
we can send the command to include them in the cluster and to increase the number of partition to 6.</p>
<p>This can be done following the <a href="https://docs.camunda.io/docs/next/self-managed/components/orchestration-cluster/zeebe/operations/cluster-scaling/#2b-scaling-brokers-and-partitions" target="_blank" rel="noopener noreferrer" class="">guide in the official docs</a>.</p>
<p>Once the scaling has been completed, as can be seen from the <strong>Cluster operation</strong> section in the dashboard, we see the newly created
partitions participate in the workload.</p>
<p>We now have the following topology:</p>
<p><img decoding="async" loading="lazy" alt="six-partitions-topology" src="/zeebe-chaos/assets/images/six-partitions-topology-c06f2c4b77537e86ff32e533466494ed.png" width="1569" height="298" class="img_ev3q"></p>
<p>As we did before, let&#x27;s start increasing the load incrementally as we did with the other cluster configuration.</p>
<table><thead><tr><th>Time</th><th>Brokers</th><th>Partitions</th><th>Throughput</th><th>CPU Usage</th><th>Throttling (CPU)</th><th>Backpressure</th><th>Notes</th></tr></thead><tbody><tr><td>10:27</td><td>6</td><td>6</td><td>240 PI/s</td><td>0.92/1.26/0.74/0.94/0.93/0.93</td><td>2.8/6.0/0.3/2.8/3.4/3.18</td><td>0</td><td>After scale up</td></tr><tr><td>11:05</td><td>6</td><td>6</td><td>300 PI/s</td><td>1.17/1.56/1.06/1.23/1.19/1.18</td><td>9%/29%/0.6%/9%/11%/10%</td><td>0</td><td>Stable</td></tr><tr><td>11:10</td><td>6</td><td>6</td><td>360 PI/s</td><td>1.39/1.76/1.26/1.43/1.37/1.42</td><td>19%/42%/2%/16%/21%/22%</td><td>0</td><td>Stable</td></tr><tr><td>11:10</td><td>6</td><td>6</td><td>420 PI/s</td><td>1.76/1.89/1.50/1.72/1.50/1.70</td><td>76%/84%/52%/71%/60%/65%</td><td>0 (spurts on 1 partition)</td><td>Pushing hard</td></tr></tbody></table>
<p>However, at 11:32 one of the workers restarted, causing a spike in the processing due to jobs being yielded back to the engine, less jobs to be activated,
and thus less to be completed. This caused a job backlog to build up in the engine. Once the worker restarted, the backlog was drained, leading to a spike in
job completion requests: around 820 req/s, as opposed to the expected 420 req/s.</p>
<p>Because of this extra load, the cluster started to consume even more CPU, resulting in heavy CPU throttling from the cloud provider.</p>
<p><img decoding="async" loading="lazy" alt="CPU usage @ 420 PI/s" src="/zeebe-chaos/assets/images/config_2_420_cpu-7179ead09ae826515cfc6a8aeafec600.png" width="1649" height="1128" class="img_ev3q">
<img decoding="async" loading="lazy" alt="CPU throttling @ 420 PI/s" src="/zeebe-chaos/assets/images/config_2_420_cpu_throttling-f4f8d922f4cdf6859965acb1c6a3b78f.png" width="1649" height="1128" class="img_ev3q"></p>
<p>On top of this, eventually a broker restarted (most likely as we run on spot VMs). In order to continue with our test, we scaled the load down to 60 PI/s
to give the cluster the time to heal.</p>
<p>Once the cluster was healthy again, we raised the throughput back to 480 PI/s to verify the scalability with twice as much throughput as the initial configuration.</p>
<p>The cluster was able to sustain 480 process instances per second with similar levels of backpressure of the initial configuration:</p>
<p><img decoding="async" loading="lazy" alt="Backpressure @ 480 PI/s" src="/zeebe-chaos/assets/images/config_2_480_backpressure-50f97c503678bcf1a51c1c316029be52.png" width="1705" height="1135" class="img_ev3q"></p>
<p>We can see below that CPU usage is high, and there is still some throttling, indicating we might be able to do more with a little bit of vertical scaling, or by scaling out and reducing the number of partitions per broker:</p>
<p><img decoding="async" loading="lazy" alt="CPU usage @ 480 PI/s" src="/zeebe-chaos/assets/images/config_2_480_cpu-33e89ad48a794be879a4977d4c5d1a5f.png" width="1702" height="1127" class="img_ev3q">
<img decoding="async" loading="lazy" alt="CPU throttling" src="/zeebe-chaos/assets/images/config_2_480_cpu_throttling-dc7f857c52e898fb159f70761f142499.png" width="1702" height="1127" class="img_ev3q"></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" translate="no">​</a></h2>
<p>We were able to verify that the cluster can scale almost linearly with new brokers and partitions, so long as the other components, like the secondary storage, workers, connectors, etc., are able to sustain a similar.</p>
<p>In particular, making sure that the secondary storage is able to keep up with the throughput turned out to be crucial to keep the cluster stable in order to
avoid filling up the Zeebe disks, which would bring to a halt the cluster.</p>
<p>We encountered a similar issue when one worker restarts: initially it creates a backlog of unhandled jobs, which turns into a massive increase in requests per second when the worker comes back, as it starts activating jobs faster than the cluster can complete them.</p>
<p>Finally, with this specific test, it would be interesting to explore the limits of vertical scalability, as we often saw CPU throttling being a major blocker for processing. This would make for an interesting future experiment.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/scalability">scalability</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance">Follow up REST API performance</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-07-02T00:00:00.000Z">July 2, 2025</time> · <!-- -->26 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp" translate="no">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Principal Software Engineer @ Camunda">Principal Software Engineer @ Camunda</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="investigating-rest-api-performance">Investigating REST API performance<a href="#investigating-rest-api-performance" class="hash-link" aria-label="Direct link to Investigating REST API performance" title="Direct link to Investigating REST API performance" translate="no">​</a></h2>
<p>This post collates the experiments, findings, and lessons learned during the REST API performance investigation.</p>
<p>There wasn&#x27;t one explicit root cause identified. As it is often the case with such performance issues, it is the combination of several things.</p>
<p><strong>Quint essence:</strong> REST API is more CPU intense/heavy than gRPC. You can read more about this in the <a href="#conclusion" class="">conclusion part</a>. We have discovered ~10 issues we have to follow up with, where at least 2-3 might have a significant impact in the performance. Details can be found in the <a href="#discovered-issues" class="">Discovered issues</a> section</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Follow up REST API performance" href="/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/06/30/Performance-of-REST-API">Performance of REST API</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-30T00:00:00.000Z">June 30, 2025</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp" translate="no">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Principal Software Engineer @ Camunda">Principal Software Engineer @ Camunda</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>In today&#x27;s Chaos day we wanted to experiment with the new REST API (v2) as a replacement for our previous used gRPC API.</p>
<p>Per default, our load tests make use of the gRPC, but as we want to make REST API the default and release this fully with 8.8, we want to make sure to test this accordingly in regard to reliability.</p>
<p><strong>TL;DR;</strong> We observed severe performance regression when using the REST API, even when job streaming is in use by the job workers (over gRPC). Our client seems to have a higher memory consumption, which caused some instabilities in our tests as well. With the new API, we lack certain observability, which makes it harder to dive into certain details. We should investigate this further and find potential bottlenecks and improvements.</p>
<p><img decoding="async" loading="lazy" alt="general" src="/zeebe-chaos/assets/images/general-overview-a0c6c48be94b9a8aa8cbbd3b44830a67.png" width="1879" height="879" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Performance of REST API" href="/zeebe-chaos/2025/06/30/Performance-of-REST-API"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS">How does Zeebe behave with NFS</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-12T00:00:00.000Z">June 12, 2025</time> · <!-- -->15 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp" translate="no">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Principal Software Engineer @ Camunda">Principal Software Engineer @ Camunda</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>This week, we (<a href="https://github.com/lenaschoenburg" target="_blank" rel="noopener noreferrer" class="">Lena</a>, <a href="https://github.com/npepinpe" target="_blank" rel="noopener noreferrer" class="">Nicolas</a>, <a href="https://github.com/romansmirnov" target="_blank" rel="noopener noreferrer" class="">Roman</a>, and <a href="https://github.com/ChrisKujawa" target="_blank" rel="noopener noreferrer" class="">I</a>) held a workshop where we looked into how Zeebe behaves with network file storage (NFS).</p>
<p>We ran several experiments with NFS and Zeebe, and messing around with connectivity.</p>
<p><strong>TL;DR;</strong> We were able to show that NFS can handle certain connectivity issues, just causing Zeebe to process slower. IF we completely lose the connection to the NFS server, several issues can arise, like IOExceptions on flush (where RAFT goes into inactive mode) or SIGBUS errors on reading (like replay), causing the JVM to crash.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about How does Zeebe behave with NFS" href="/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment">Lower memory consumption of Camunda deployment</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-05T00:00:00.000Z">June 5, 2025</time> · <!-- -->10 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp" translate="no">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Principal Software Engineer @ Camunda">Principal Software Engineer @ Camunda</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>I&#x27;m back to finally do some load testing again.</p>
<p>In the past months, we have changed our architecture. This was to deploy instead all of our components as a separate deployment,
we now have one single statefulset. This statefulset is running our single Camunda standalone application,
combining all components together.</p>
<p><img decoding="async" loading="lazy" alt="simpler deployment" src="/zeebe-chaos/assets/images/simpler-deployment-a09568ddfb813f18a49e6b930b43cd02.png" width="1550" height="744" class="img_ev3q"></p>
<p>More details on this change we will share on a separate blog post. For simplicity, in our load tests (benchmark helm charts), we
combined all the resources we had split over multiple deployments together, see related PR <a href="https://github.com/camunda/zeebe-benchmark-helm/pull/213" target="_blank" rel="noopener noreferrer" class="">#213</a>.</p>
<p>We are currently running our test with the following resources by default:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">Limits</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">cpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">     </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">memory</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">  12Gi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">Requests</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">cpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">      </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">memory</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">   6Gi</span><br></span></code></pre></div></div>
<p>In today&#x27;s Chaos day, I want to look into our resource consumption and whether we can reduce our used requests and limits.</p>
<p><strong>TL;DR;</strong> We have focused on experimenting with different memory resources, and were able to show that we can reduce the used memory by 75%, and our previous provisioned resources by more than 80% for our load tests.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Lower memory consumption of Camunda deployment" href="/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project">News from Camunda Exporter project</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-12-12T00:00:00.000Z">December 12, 2024</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp" translate="no">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Principal Software Engineer @ Camunda">Principal Software Engineer @ Camunda</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>In this Chaos day, we want to verify the current state of the exporter project and run benchmarks with it. Comparing
with a previous version (v8.6.6) should give us a good hint on the current state and potential improvements.</p>
<p><strong>TL;DR;</strong> The latency of user data availability has improved due to our architecture change, but we still need to fix some bugs before our planned release of the Camunda Exporter. This experiment allows us to detect three new bugs, fixing this should allow us to make the system more stable.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about News from Camunda Exporter project" href="/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting">Using flow control to handle bottleneck on exporting</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-25T00:00:00.000Z">July 25, 2024</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/rodrigo-lourenco-lopes.png" alt="Rodrigo Lopes"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Rodrigo Lopes</span></a></div><small class="authorTitle_nd0D" title="Associate Software Engineer @ Zeebe">Associate Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).
Limiting the write rate is a new feature that can be used to prevent building up an excessive exporting backlog.
There are two ways to limit the write rate, either by setting a static limit or by enabling throttling that dynamically adjust the write rate based on the exporting backlog and rate.
In these experiments, we will test both ways of limiting the write rate and observe the effects on processing and exporting.</p>
<p><strong>TL;DR;</strong>
Both setting a static write rate limit and enabling throttling of the write rate can be used to prevent building up an excessive exporting backlog.
For users, this will be seen as backpressure because processing speed is limited by the rate at which it can write processing results.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Using flow control to handle bottleneck on exporting" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops">Using flow control to handle uncontrolled process loops</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-25T00:00:00.000Z">July 25, 2024</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/rodrigo-lourenco-lopes.png" alt="Rodrigo Lopes"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp" translate="no">Rodrigo Lopes</span></a></div><small class="authorTitle_nd0D" title="Associate Software Engineer @ Zeebe">Associate Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).</p>
<p>Limiting the write rate is a new feature that can be used to prevent building up an excessive exporting backlog.</p>
<p>In these experiments we will test what happens with the deployment of endless
loops that result in high processing load, and how we can use the new
flow control to keep the cluster stable.</p>
<p><strong>TL;DR;</strong></p>
<p>Enabling the write rate limiting can help mitigate the effects caused by
process instances that contain uncontrolled loops by preventing building up an
excessive exporting backlog.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Using flow control to handle uncontrolled process loops" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops"><b>Read more</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/zeebe-chaos/tags/availability/page/2"><div class="pagination-nav__label">Older entries</div></a></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/zeebe" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://forum.camunda.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://twitter.com/Camunda" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/camunda/zeebe-chaos/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Zeebe Chaos. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>