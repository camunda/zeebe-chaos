<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">REST API: From ForkJoin to a Dedicated Thread Pool | Zeebe Chaos</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://camunda.github.io/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="REST API: From ForkJoin to a Dedicated Thread Pool | Zeebe Chaos"><meta data-rh="true" name="description" content="During the latest REST API Performance load tests,"><meta data-rh="true" property="og:description" content="During the latest REST API Performance load tests,"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-09-19T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/berkaycanbc"><meta data-rh="true" property="article:tag" content="availability"><link data-rh="true" rel="icon" href="/zeebe-chaos/img/zeebe-logo.png"><link data-rh="true" rel="canonical" href="https://camunda.github.io/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool"><link data-rh="true" rel="alternate" href="https://camunda.github.io/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool" hreflang="en"><link data-rh="true" rel="alternate" href="https://camunda.github.io/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool","url":"https://camunda.github.io/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool","headline":"REST API: From ForkJoin to a Dedicated Thread Pool","name":"REST API: From ForkJoin to a Dedicated Thread Pool","description":"During the latest REST API Performance load tests,","datePublished":"2025-09-19T00:00:00.000Z","author":{"@type":"Person","name":"Berkay Can","description":"Software Engineer @ Zeebe","url":"https://github.com/berkaycanbc","image":"https://github.com/berkaycanbc.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://camunda.github.io/zeebe-chaos/","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/zeebe-chaos/rss.xml" title="Zeebe Chaos RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zeebe-chaos/atom.xml" title="Zeebe Chaos Atom Feed"><link rel="stylesheet" href="/zeebe-chaos/assets/css/styles.09e3c82e.css">
<script src="/zeebe-chaos/assets/js/runtime~main.7d298d8f.js" defer="defer"></script>
<script src="/zeebe-chaos/assets/js/main.58ee3eee.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zeebe-chaos/"><div class="navbar__logo"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Zeebe Chaos</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zeebe-chaos/">Chaos Summaries</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/camunda/zeebe-chaos" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/10/31/Improvents-in-retention">Testing retention of historical PIs in Camunda 8.8</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/10/02/Dynamic-Scaling-Resilience">Resilience of dynamic scaling</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/zeebe-chaos/2025/09/18/REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool">REST API: From ForkJoin to a Dedicated Thread Pool</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/08/26/Resiliency-against-ELS-unavailability">Resiliency against ELS unavailability</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/07/11/linear-dynamic-scaling">Dynamic Scaling: probing linear scalability</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance">Follow up REST API performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/30/Performance-of-REST-API">Performance of REST API</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS">How does Zeebe behave with NFS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment">Lower memory consumption of Camunda deployment</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project">News from Camunda Exporter project</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance">Impact of Camunda Exporter on processing performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">Camunda Exporter MVP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark">Optimizing cluster sizing using a real world benchmark</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency">Improve Operate import latency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/16/Operate-load-handling">Operate load handling</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting">Using flow control to handle bottleneck on exporting</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops">Using flow control to handle uncontrolled process loops</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/01/19/Job-Activation-Latency">Reducing the job activation delay</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/20/Broker-scaling-performance">Broker Scaling and Performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss">Dynamic Scaling with Dataloss</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers">Dynamically scaling brokers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/06/Job-Push-resiliency">Job push resiliency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/30/Job-push-overloading">Job push overloading</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing">Hot backups impact on processing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance">Using Large Multi-Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle">Continuing SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/15/SST-Partitioning-toggle">SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/04/06/gateway-termination">Gateway Termination</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/02/23/Recursive-call-activity">Recursive call activity</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition">Message Correlation after Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/02/deployment-distribution">Bring Deployment distribution experiment back</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS">Standalone Gateway in CCSaaS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/01/High-Snapshot-Frequency">High Snapshot Frequency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/01/19/big-variables">Handling of Big Variables</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2021</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance">Worker count should not impact performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys">Not produce duplicate Keys</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/29/Throughput-on-big-state">Throughput on big state</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/05/recovery-time">Recovery (Fail Over) time</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/09/23/Old-Clients">Old-Clients</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/07/06/Slow-Network">Slow Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/06/08/Full-Disk">Full Disk Recovery</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/05/25/Reset-Clock">Time travel Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/29/Corrupted-Snapshot">Corrupted Snapshot Experiment Investigation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering">BPMN meets Chaos Engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/30/set-file-immutable">Set file immutable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/23/camunda-cloud-network-partition">Camunda Cloud network partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/09/cont-workflow-instance">Fault-tolerant processing of process instances</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/02/23/automate-deployments-dist">Automating Deployment Distribution Chaos Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/26/deployments">Deployment Distribution</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/19/network-partition">Network partitions</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/07/disconnect-leader-and-follower">Disconnect Leader and one Follower</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2020</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/24/message-correlation-after-failover">Message Correlation after Failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/11/job-timeouts">Many Job Timeouts</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/03/investigate-failing-tests">Investigate failing Chaos Tests</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/20/non-graceful-shutdown">Non-graceful Shutdown Broker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/27/standalone-gw-memory">Gateway memory consumption</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/13/multiple-leader-changes">Multiple Leader Changes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/06/toxi-proxy">Play around with ToxiProxy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud">Experiment with Camunda Cloud</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/06/low-load">Experiment with Low Load</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/30/experiment-without-exporters">Experiment without Exporters</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/16/big-multi-instance">Big Multi Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/09/timer-and-huge-variables">Experiment with Timers and Huge Variables</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/02/extract-k8-resources">Extract K8 resources from namespace</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/25/gateway-network-partition">Gateway Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/18/correlate-message-after-failover">Correlate Message after failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/11/high-cpu-gateway">High CPU load on Standalone Gateway</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/04/first-chaos-day">First Chaos Day!</a></li></ul></div></nav></aside><main class="col col--7"><article><header><h1 class="title_f1Hy">REST API: From ForkJoin to a Dedicated Thread Pool</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-09-19T00:00:00.000Z">September 19, 2025</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/berkaycanbc" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/berkaycanbc.png" alt="Berkay Can"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/berkaycanbc" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Berkay Can</span></a></div><small class="authorTitle_nd0D" title="Software Engineer @ Zeebe">Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>During the latest <a href="https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance" target="_blank" rel="noopener noreferrer">REST API Performance load tests</a>,
we discovered that REST API requests suffered from significantly higher latency under CPU pressure, even when throughput numbers looked comparable.
While adding more CPU cores alleviated the issue, this wasn’t a sustainable solution — it hinted at an inefficiency in how REST handled broker responses.
See related <a href="https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance#request-handling-execution-logic" target="_blank" rel="noopener noreferrer">section</a> from the previous blog post.</p>
<p>This blog post is about how we diagnosed the issue, what we found, and the fix we introduced in <a href="https://github.com/camunda/camunda/pull/36517" target="_blank" rel="noopener noreferrer">PR #36517</a>
to close the performance gap.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-problem">The Problem<a href="#the-problem" class="hash-link" aria-label="Direct link to The Problem" title="Direct link to The Problem">​</a></h2>
<p>A difference we spotted early between <strong>REST API</strong> and <strong>gRPC</strong> request handling was the usage of the <code>BrokerClient</code>:</p>
<ul>
<li><strong>gRPC:</strong> <code>BrokerClient</code> calls are wrapped with retries and handled directly in the request thread.</li>
<li><strong>REST:</strong> requests are executed without retries, and responses are handled asynchronously using the common <code>ForkJoinPool</code>.</li>
</ul>
<p>On clusters with 2 CPUs, the JVM defaults to a single thread for the common <code>ForkJoinPool</code>.
Our expectation was that this could cause contention: one thread might not be fast enough to process responses in time, leading to delays in the Gateway ↔ Broker request-response cycle.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-solution-journey">The Solution Journey<a href="#the-solution-journey" class="hash-link" aria-label="Direct link to The Solution Journey" title="Direct link to The Solution Journey">​</a></h2>
<p>Solving this issue wasn’t a straight line — we tried a few approaches before landing on the final design.
Each iteration gave us valuable insights about Java’s thread pool usage and its impact on REST API performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-first-attempt-synchronousqueue">1. First Attempt: SynchronousQueue<a href="#1-first-attempt-synchronousqueue" class="hash-link" aria-label="Direct link to 1. First Attempt: SynchronousQueue" title="Direct link to 1. First Attempt: SynchronousQueue">​</a></h3>
<p>We began with a custom <code>ThreadPoolExecutor</code> that used a <code>SynchronousQueue</code> for task handoff:</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">new ThreadPoolExecutor(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    corePoolSize,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    maxPoolSize,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    keepAliveSeconds,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    TimeUnit.SECONDS,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    new SynchronousQueue&lt;&gt;(),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    threadFactory,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    new ThreadPoolExecutor.CallerRunsPolicy());</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>A <code>SynchronousQueue</code> has <strong>no capacity</strong> — each task submission must immediately find a free worker thread.
If no thread is available, the caller blocks until one frees up.</p>
<p>In practice, this meant concurrency was <strong>artificially limited</strong>: bursts of REST requests had to wait for a thread to become available, reducing parallelism.
The results were modest:</p>
<ul>
<li><strong>Request rate:</strong> unchanged</li>
<li><strong>Average latency:</strong> improved slightly (from ~120 ms → ~100 ms)</li>
<li><strong>CPU throttling:</strong> dropped only marginally (100% → 90%)</li>
</ul>
<p>Before any changes introduced, results looked like this:
<img decoding="async" loading="lazy" alt="syncqueue-req-before" src="/zeebe-chaos/assets/images/syncqueue-req-before-8d25531fa8c311440fa9c8e80e78d1e5.png" width="1052" height="1210" class="img_ev3q">
<img decoding="async" loading="lazy" alt="syncqueue-cpu-before" src="/zeebe-chaos/assets/images/syncqueue-cpu-before-221093fdaa3aa155857222a6feec4e88.png" width="2112" height="674" class="img_ev3q"></p>
<p>Then we started benchmarking after introducing executor with <code>SynchronousQueue</code>:
<img decoding="async" loading="lazy" alt="syncqueue-req-after" src="/zeebe-chaos/assets/images/syncqueue-req-after-172469b801a0b97b731b4b27ba44eedf.png" width="1050" height="1218" class="img_ev3q">
<img decoding="async" loading="lazy" alt="syncqueue-cpu-after" src="/zeebe-chaos/assets/images/syncqueue-cpu-after-b2902d1db04dba87750b03e9dae1673f.png" width="2098" height="684" class="img_ev3q"></p>
<p>This hinted we were on the right track — a dedicated executor helped — but the queue strategy was too restrictive.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-experiment-arrayblockingqueue--abortpolicy">2. Experiment: ArrayBlockingQueue + AbortPolicy<a href="#2-experiment-arrayblockingqueue--abortpolicy" class="hash-link" aria-label="Direct link to 2. Experiment: ArrayBlockingQueue + AbortPolicy" title="Direct link to 2. Experiment: ArrayBlockingQueue + AbortPolicy">​</a></h3>
<p>Next, we switched to an <code>ArrayBlockingQueue</code> with a capacity of 64.
This allowed the pool to <strong>buffer short micro-bursts</strong> of requests instead of blocking immediately.
At the same time, we replaced <code>CallerRunsPolicy</code> with <code>AbortPolicy</code>:</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">new ThreadPoolExecutor.AbortPolicy();</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The idea was to <strong>fail fast</strong> on saturation: if the queue filled up and no thread was free, the executor would throw <code>RejectedExecutionException</code> immediately.
This boosted the measured request rate, but at a cost:</p>
<ul>
<li>Many requests were simply rejected outright.</li>
<li>Measuring true performance became tricky, since high throughput numbers hid the rejections.</li>
<li>Operationally, this wasn’t acceptable — REST clients would constantly see errors under load.</li>
</ul>
<p>Because of this, we abandoned the fail-fast approach.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-final-decision-callerrunspolicy--higher-max-pool-size">3. Final Decision: CallerRunsPolicy + Higher Max Pool Size<a href="#3-final-decision-callerrunspolicy--higher-max-pool-size" class="hash-link" aria-label="Direct link to 3. Final Decision: CallerRunsPolicy + Higher Max Pool Size" title="Direct link to 3. Final Decision: CallerRunsPolicy + Higher Max Pool Size">​</a></h3>
<p>Finally, we returned to <code>CallerRunsPolicy</code>.
Instead of rejecting tasks, this policy makes the caller thread execute the task itself when the pool is saturated.
This introduces <strong>natural backpressure</strong>: clients slow down automatically when the system is busy, without dropping requests.</p>
<p>To give the executor more headroom, we also increased the maximum pool size from <code>availableProcessors * 2</code> to <code>availableProcessors * 8</code>.</p>
<p>This combination made the breakthrough:</p>
<ul>
<li><strong>Request rate (REST):</strong> stabilized around 150 RPS with spikes up to 200 RPS</li>
<li><strong>Average latency:</strong> dropped dramatically (from ~120 ms → ~25 ms)</li>
<li><strong>CPU throttling:</strong> reduced significantly (100% → 30-40%)</li>
</ul>
<p>Here are the final results:
<img decoding="async" loading="lazy" alt="final-decision-result-rest" src="/zeebe-chaos/assets/images/final-decision-result-rest-f324c1eef2158ab2f20dd5d688a4e89f.png" width="2338" height="1202" class="img_ev3q">
<img decoding="async" loading="lazy" alt="final-decision-result-cpu" src="/zeebe-chaos/assets/images/final-decision-result-cpu-c579720539ebcb00496de95c9120dc49.png" width="2332" height="672" class="img_ev3q"></p>
<p>This design struck the right balance: elastic concurrency, backpressure, and resource efficiency.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways">​</a></h3>
<ol>
<li><strong>SynchronousQueue limits concurrency</strong> — good for handoff semantics, but too restrictive for REST workloads.</li>
<li><strong>Fail-fast rejection looks good in benchmarks but fails in production</strong> — clients can’t handle widespread request errors.</li>
<li><strong>CallerRunsPolicy provides natural backpressure</strong> — throughput stabilizes without dropping requests, and latency improves.</li>
<li><strong>CPU-aware max pool sizing matters</strong> — scaling pool size relative to cores unlocks performance gains.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="more-benchmarking">More Benchmarking<a href="#more-benchmarking" class="hash-link" aria-label="Direct link to More Benchmarking" title="Direct link to More Benchmarking">​</a></h2>
<p>To validate that our executor change holds up across configurations, we ran extra tests on our benchmark cluster provisioned with 2 vCPUs per Camunda application.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-comparing-max-pool-size-4-8-16">1) Comparing Max Pool Size (×4, ×8, ×16)<a href="#1-comparing-max-pool-size-4-8-16" class="hash-link" aria-label="Direct link to 1) Comparing Max Pool Size (×4, ×8, ×16)" title="Direct link to 1) Comparing Max Pool Size (×4, ×8, ×16)">​</a></h3>
<p>We ran the same workload while varying <code>maxPoolSize = availableProcessors × {4, 8, 16}</code>. Below are the observed tops from Grafana panels in the screenshots:</p>
<p><code>maxPoolSizeMultiplier=4</code>
<img decoding="async" loading="lazy" alt="max-pool-size-multiplier-4" src="/zeebe-chaos/assets/images/max-pool-size-multiplier-4-61b29561f085e4079b30f5de0552bab5.png" width="2316" height="1212" class="img_ev3q"></p>
<p><code>maxPoolSizeMultiplier=8</code>
<img decoding="async" loading="lazy" alt="max-pool-size-multiplier-8" src="/zeebe-chaos/assets/images/max-pool-size-multiplier-8-f324c1eef2158ab2f20dd5d688a4e89f.png" width="2338" height="1202" class="img_ev3q"></p>
<p><code>maxPoolSizeMultiplier=16</code>
<img decoding="async" loading="lazy" alt="max-pool-size-multiplier-16" src="/zeebe-chaos/assets/images/max-pool-size-multiplier-16-e1011e424e35c3178f468ef5bcc4656a.png" width="2332" height="1204" class="img_ev3q"></p>
<table><thead><tr><th style="text-align:right">Multiplier</th><th style="text-align:right">Request Rate (proc-instances)</th><th style="text-align:right">Request Rate (completion)</th><th style="text-align:right">Avg Latency (proc-instances)</th><th style="text-align:right">Avg Latency (completion)</th></tr></thead><tbody><tr><td style="text-align:right">×4</td><td style="text-align:right">~51.6 req/s</td><td style="text-align:right">~42.3 req/s</td><td style="text-align:right">~40.2 ms</td><td style="text-align:right">~57.4 ms</td></tr><tr><td style="text-align:right">×8</td><td style="text-align:right">~144.4 req/s</td><td style="text-align:right">~144.7 req/s</td><td style="text-align:right">~21.4 ms</td><td style="text-align:right">~24.1 ms</td></tr><tr><td style="text-align:right">×16</td><td style="text-align:right">~22.7 req/s</td><td style="text-align:right">~19.5 req/s</td><td style="text-align:right">~38.4 ms</td><td style="text-align:right">~55.4 ms</td></tr></tbody></table>
<p><strong>What this suggests (in our setup):</strong></p>
<ul>
<li><strong>×8</strong> is the clear <strong>sweet spot</strong>: highest sustained throughput with the lowest average latencies.</li>
<li><strong>×4</strong> under-provisions the pool (lower RPS, higher latency).</li>
<li><strong>×16</strong> shows <strong>diminishing/negative returns</strong> (likely scheduler contention or oversubscription): much lower RPS and latencies drifting back up.</li>
</ul>
<p><strong>Takeaway:</strong> In our setup, ×8 balances elasticity and scheduling overhead, delivering the best throughput–latency trade-off.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-comparing-queue-capacity-16-vs-64-vs-256">3) Comparing Queue Capacity (16 vs 64 vs 256)<a href="#3-comparing-queue-capacity-16-vs-64-vs-256" class="hash-link" aria-label="Direct link to 3) Comparing Queue Capacity (16 vs 64 vs 256)" title="Direct link to 3) Comparing Queue Capacity (16 vs 64 vs 256)">​</a></h3>
<p>We varied the executor <strong>queue capacity</strong> and compared <strong>16</strong>, <strong>64</strong> (our current/default for this run), and <strong>256</strong> under the same workload.<br>
<!-- -->Below are the observed tops from the Grafana panels for the two hot endpoints:</p>
<ul>
<li>POST /v2/process-instances</li>
<li>POST /v2/jobs/{jobKey}/completion</li>
</ul>
<p><code>queueCapacity=16</code><br>
<img decoding="async" loading="lazy" alt="queue-capacity-16" src="/zeebe-chaos/assets/images/queue-capacity-16-516379e4fce7362827be0d3ad957e7dc.png" width="2338" height="1208" class="img_ev3q"></p>
<p><code>queueCapacity=64</code><br>
<img decoding="async" loading="lazy" alt="queue-capacity-64" src="/zeebe-chaos/assets/images/queue-capacity-64-f324c1eef2158ab2f20dd5d688a4e89f.png" width="2338" height="1202" class="img_ev3q"></p>
<p><code>queueCapacity=256</code><br>
<img decoding="async" loading="lazy" alt="queue-capacity-256" src="/zeebe-chaos/assets/images/queue-capacity-256-19acdd7b6ca1df00ccaefb20fae5ce41.png" width="2338" height="1216" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="measured-summary">Measured summary<a href="#measured-summary" class="hash-link" aria-label="Direct link to Measured summary" title="Direct link to Measured summary">​</a></h4>
<table><thead><tr><th style="text-align:right">Queue Capacity</th><th style="text-align:right">Request Rate (proc-instances)</th><th style="text-align:right">Request Rate (completion)</th><th style="text-align:right">Avg Latency (proc-instances)</th><th style="text-align:right">Avg Latency (completion)</th></tr></thead><tbody><tr><td style="text-align:right">16</td><td style="text-align:right">~78.2 req/s</td><td style="text-align:right">~56.0 req/s</td><td style="text-align:right">~28.2 ms</td><td style="text-align:right">~40.8 ms</td></tr><tr><td style="text-align:right">64</td><td style="text-align:right">~144.4 req/s</td><td style="text-align:right">~144.7 req/s</td><td style="text-align:right">~21.4 ms</td><td style="text-align:right">~24.1 ms</td></tr><tr><td style="text-align:right">256</td><td style="text-align:right">~80.2 req/s</td><td style="text-align:right">~61.2 req/s</td><td style="text-align:right">~29.3 ms</td><td style="text-align:right">~43.0 ms</td></tr></tbody></table>
<p><strong>What this suggests (in our setup):</strong></p>
<ul>
<li><strong>Queue = 64</strong> is the clear <strong>sweet spot</strong>: highest sustained throughput (~144 req/s on hot endpoints) with the lowest avg latencies (~21–24 ms). Likely large enough to absorb micro-bursts, but small enough to avoid long queue waits.</li>
<li><strong>Queue = 16</strong> under-buffers: lower RPS (~78 / ~56 req/s) and higher latency (~28–41 ms). With <code>CallerRunsPolicy</code>, the queue fills quickly and the caller runs tasks often → frequent backpressure throttles producers.</li>
<li><strong>Queue = 256</strong> shows <strong>diminishing/negative returns</strong> relative to 64: lower RPS (~80 / ~61 req/s) and higher latency (~29–43 ms). The big buffer hides saturation, adding queueing delay before execution without delivering extra useful work at the same CPU budget.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Moving off the common <code>ForkJoinPool</code> to a dedicated, CPU-aware executor with bounded queueing and <code>CallerRunsPolicy</code> backpressure turned an overload problem into graceful degradation: fewer 5xxs, steadier RPS, and far lower tail latency under the same CPU budget.</p>
<p><strong>Final takeaways</strong></p>
<ul>
<li><strong>Isolation beats sharing.</strong> A dedicated pool prevents noisy neighbors from the common <code>ForkJoinPool</code>.</li>
<li><strong>Backpressure beats drops.</strong> <code>CallerRunsPolicy</code> slows producers when saturated, stabilizing the system without mass rejections.</li>
<li><strong>Right-sized knobs matter.</strong> <code>maxPoolSize ≈ cores × 8</code> and <code>queueCapacity ≈ 64</code> hit the best throughput/latency balance in our runs; smaller queues over-throttle, larger queues hide saturation and add wait time.</li>
<li><strong>Results are environment-specific.</strong> At higher core counts, the sweet spot may shift—re-benchmark when CPUs or workload mix change.</li>
</ul>
<p><strong>Note:</strong> Results are environment-specific; at higher core counts, the sweet spot may shift—re-benchmark when CPUs or workload mix change.
In this experiment, we focused on CPU-bound scenarios.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-09-18-REST-API-From-ForkJoin-to-a-Dedicated-Thread-Pool/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/zeebe-chaos/2025/10/02/Dynamic-Scaling-Resilience"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Resilience of dynamic scaling</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/zeebe-chaos/2025/08/26/Resiliency-against-ELS-unavailability"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Resiliency against ELS unavailability</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-problem" class="table-of-contents__link toc-highlight">The Problem</a></li><li><a href="#the-solution-journey" class="table-of-contents__link toc-highlight">The Solution Journey</a><ul><li><a href="#1-first-attempt-synchronousqueue" class="table-of-contents__link toc-highlight">1. First Attempt: SynchronousQueue</a></li><li><a href="#2-experiment-arrayblockingqueue--abortpolicy" class="table-of-contents__link toc-highlight">2. Experiment: ArrayBlockingQueue + AbortPolicy</a></li><li><a href="#3-final-decision-callerrunspolicy--higher-max-pool-size" class="table-of-contents__link toc-highlight">3. Final Decision: CallerRunsPolicy + Higher Max Pool Size</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li></ul></li><li><a href="#more-benchmarking" class="table-of-contents__link toc-highlight">More Benchmarking</a><ul><li><a href="#1-comparing-max-pool-size-4-8-16" class="table-of-contents__link toc-highlight">1) Comparing Max Pool Size (×4, ×8, ×16)</a></li><li><a href="#3-comparing-queue-capacity-16-vs-64-vs-256" class="table-of-contents__link toc-highlight">3) Comparing Queue Capacity (16 vs 64 vs 256)</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/zeebe" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://forum.camunda.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/Camunda" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/camunda/zeebe-chaos/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Zeebe Chaos. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>