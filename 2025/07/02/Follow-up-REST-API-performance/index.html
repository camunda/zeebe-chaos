<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">Follow up REST API performance | Zeebe Chaos</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Follow up REST API performance | Zeebe Chaos"><meta data-rh="true" name="description" content="Investigating REST API performance"><meta data-rh="true" property="og:description" content="Investigating REST API performance"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-07-02T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/ChrisKujawa"><meta data-rh="true" property="article:tag" content="availability"><link data-rh="true" rel="icon" href="/zeebe-chaos/img/zeebe-logo.png"><link data-rh="true" rel="canonical" href="https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance"><link data-rh="true" rel="alternate" href="https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance" hreflang="en"><link data-rh="true" rel="alternate" href="https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance","url":"https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance","headline":"Follow up REST API performance","name":"Follow up REST API performance","description":"Investigating REST API performance","datePublished":"2025-07-02T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://camunda.github.io/zeebe-chaos/","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/zeebe-chaos/rss.xml" title="Zeebe Chaos RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zeebe-chaos/atom.xml" title="Zeebe Chaos Atom Feed"><link rel="stylesheet" href="/zeebe-chaos/assets/css/styles.09e3c82e.css">
<script src="/zeebe-chaos/assets/js/runtime~main.cb50d33d.js" defer="defer"></script>
<script src="/zeebe-chaos/assets/js/main.d2c762a6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zeebe-chaos/"><div class="navbar__logo"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Zeebe Chaos</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zeebe-chaos/">Chaos Summaries</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/camunda/zeebe-chaos" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/07/11/linear-dynamic-scaling">Dynamic Scaling: probing linear scalability</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance">Follow up REST API performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/30/Performance-of-REST-API">Performance of REST API</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS">How does Zeebe behave with NFS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment">Lower memory consumption of Camunda deployment</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project">News from Camunda Exporter project</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance">Impact of Camunda Exporter on processing performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">Camunda Exporter MVP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark">Optimizing cluster sizing using a real world benchmark</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency">Improve Operate import latency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/16/Operate-load-handling">Operate load handling</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting">Using flow control to handle bottleneck on exporting</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops">Using flow control to handle uncontrolled process loops</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/01/19/Job-Activation-Latency">Reducing the job activation delay</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/20/Broker-scaling-performance">Broker Scaling and Performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss">Dynamic Scaling with Dataloss</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers">Dynamically scaling brokers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/06/Job-Push-resiliency">Job push resiliency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/30/Job-push-overloading">Job push overloading</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing">Hot backups impact on processing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance">Using Large Multi-Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle">Continuing SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/15/SST-Partitioning-toggle">SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/04/06/gateway-termination">Gateway Termination</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/02/23/Recursive-call-activity">Recursive call activity</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition">Message Correlation after Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/02/deployment-distribution">Bring Deployment distribution experiment back</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS">Standalone Gateway in CCSaaS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/01/High-Snapshot-Frequency">High Snapshot Frequency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/01/19/big-variables">Handling of Big Variables</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2021</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance">Worker count should not impact performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys">Not produce duplicate Keys</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/29/Throughput-on-big-state">Throughput on big state</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/05/recovery-time">Recovery (Fail Over) time</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/09/23/Old-Clients">Old-Clients</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/07/06/Slow-Network">Slow Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/06/08/Full-Disk">Full Disk Recovery</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/05/25/Reset-Clock">Time travel Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/29/Corrupted-Snapshot">Corrupted Snapshot Experiment Investigation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering">BPMN meets Chaos Engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/30/set-file-immutable">Set file immutable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/23/camunda-cloud-network-partition">Camunda Cloud network partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/09/cont-workflow-instance">Fault-tolerant processing of process instances</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/02/23/automate-deployments-dist">Automating Deployment Distribution Chaos Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/26/deployments">Deployment Distribution</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/19/network-partition">Network partitions</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/07/disconnect-leader-and-follower">Disconnect Leader and one Follower</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2020</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/24/message-correlation-after-failover">Message Correlation after Failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/11/job-timeouts">Many Job Timeouts</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/03/investigate-failing-tests">Investigate failing Chaos Tests</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/20/non-graceful-shutdown">Non-graceful Shutdown Broker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/27/standalone-gw-memory">Gateway memory consumption</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/13/multiple-leader-changes">Multiple Leader Changes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/06/toxi-proxy">Play around with ToxiProxy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud">Experiment with Camunda Cloud</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/06/low-load">Experiment with Low Load</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/30/experiment-without-exporters">Experiment without Exporters</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/16/big-multi-instance">Big Multi Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/09/timer-and-huge-variables">Experiment with Timers and Huge Variables</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/02/extract-k8-resources">Extract K8 resources from namespace</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/25/gateway-network-partition">Gateway Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/18/correlate-message-after-failover">Correlate Message after failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/11/high-cpu-gateway">High CPU load on Standalone Gateway</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/04/first-chaos-day">First Chaos Day!</a></li></ul></div></nav></aside><main class="col col--7"><article><header><h1 class="title_f1Hy">Follow up REST API performance</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-07-02T00:00:00.000Z">July 2, 2025</time> · <!-- -->20 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="investigating-rest-api-performance">Investigating REST API performance<a href="#investigating-rest-api-performance" class="hash-link" aria-label="Direct link to Investigating REST API performance" title="Direct link to Investigating REST API performance">​</a></h2>
<p>This post collates the experiments, findings, and lessons learned during the REST API performance investigation.</p>
<p>There wasn&#x27;t one explicit root cause identified. As it is often the case with such performance issues, it is the combination of several things.</p>
<p><strong>Quint essence:</strong> REST API is more CPU intense/heavy than gRPC. You can read more about this in the <a href="#conclusion">conclusion part</a>. We have discovered ~10 issues we have to follow up with, where at least 2-3 might have a significant impact in the performance. Details can be found in the <a href="#discovered-issues">Discovered issues</a> section</p>
<p><em>Short summary of what we have done and validated</em></p>
<ul>
<li>Investigations<!-- -->
<ul>
<li>Investigated existing REST api metrics<!-- -->
<ul>
<li>Breakdown metrics to have a better overview of where time is spent (created a tmp dashboard)</li>
</ul>
</li>
<li>Investigated worker failing with OOM</li>
<li>Investigated deployments and anti-affinities</li>
<li>Investigated command distribution</li>
<li>Investigated JFR recordings and profiles<!-- -->
<ul>
<li>Take JFR recordings and profile the system</li>
<li>Make use of the async profiler</li>
</ul>
</li>
</ul>
</li>
<li>Experiments<!-- -->
<ul>
<li>Increase CPU resources to understand whether it is resource contention - it is.</li>
<li>Improve Spring request filtering and execution<!-- -->
<ul>
<li>Use virtual threads for Spring</li>
<li>Use PathPattern instead of legacy AntPathPattern</li>
<li>Use direct response handling instead of asynchronous</li>
<li>Combine some of them</li>
</ul>
</li>
<li>Experiment with different setups to better distribute the load</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="day-1-investigation-rest-api-performance">Day 1: Investigation REST API Performance<a href="#day-1-investigation-rest-api-performance" class="hash-link" aria-label="Direct link to Day 1: Investigation REST API Performance" title="Direct link to Day 1: Investigation REST API Performance">​</a></h2>
<p>This blog post aims to summarize the investigation of the REST API performance and give some hints and suggestions on what to improve.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rest-api-metrics">REST API Metrics<a href="#rest-api-metrics" class="hash-link" aria-label="Direct link to REST API Metrics" title="Direct link to REST API Metrics">​</a></h3>
<p>One remark from the last experiments was that we do not have good insights for the REST API. Actually, we have the necessary metrics already exposed, but not yet available in our Dashboard.</p>
<p>This is currently prepared with <a href="https://github.com/camunda/camunda/pull/33907" target="_blank" rel="noopener noreferrer">#33907</a>. Based on this, I was able to further investigate the REST API performance.</p>
<p><img decoding="async" loading="lazy" alt="rest-api" src="/zeebe-chaos/assets/images/rest-api-922105b50aa5f7c2c731f488a8999976.png" width="2539" height="607" class="img_ev3q"></p>
<p>What we can see is that our requests take on average more than 50ms to complete. This is causing our throughput to go down, we are not able to create 150 PI/s even.</p>
<p>Looking at a different Benchmark using gRPC, we can see that requests take 5-10ms to complete, and have a stable throughput</p>
<p><img decoding="async" loading="lazy" alt="grpc-latency" src="/zeebe-chaos/assets/images/grpc-latency-097288055f8e099cc22f7268d67b53a9.png" width="2526" height="333" class="img_ev3q">
<img decoding="async" loading="lazy" alt="grpc" src="/zeebe-chaos/assets/images/grpc-93fbafebfd4eebb459d95c8c713b67d7.png" width="1273" height="486" class="img_ev3q"></p>
<p>Due to the slower workers (on completion), we can see error reports of the workers not being able to accept further job pushes. This has been mentioned in the previous blog post as well.  This, in consequence, means the worker sends FAIL commands for such jobs, to give them back. It has a cascading effect, as jobs are sent back and forth, impacting the general process instance execution latency (which grows up to 60s compared to 0.2s).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="investigating-worker-errors">Investigating Worker Errors<a href="#investigating-worker-errors" class="hash-link" aria-label="Direct link to Investigating Worker Errors" title="Direct link to Investigating Worker Errors">​</a></h3>
<p>In our previous experiments, we have seen the following exceptions</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">13:25:14.684 [pool-4-thread-3] WARN  io.camunda.client.job.worker - Worker benchmark failed to handle job with key 4503599628992806 of type benchmark-task, sending fail command to broker</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">java.lang.IllegalStateException: Queue full</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:329) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.camunda.zeebe.Worker.lambda$handleJob$1(Worker.java:122) ~[classes/:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.camunda.client.impl.worker.JobRunnableFactoryImpl.executeJob(JobRunnableFactoryImpl.java:45) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.camunda.client.impl.worker.JobRunnableFactoryImpl.lambda$create$0(JobRunnableFactoryImpl.java:40) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.camunda.client.impl.worker.BlockingExecutor.lambda$execute$0(BlockingExecutor.java:50) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This is actually coming from the Worker (benchmark) application, as it is collecting all <a href="https://github.com/camunda/camunda/blob/main/zeebe/benchmarks/project/src/main/java/io/camunda/zeebe/Worker.java#L54" target="_blank" rel="noopener noreferrer">the request futures in a blocking queue</a>.</p>
<p>As the performance is lower of handling requests, we collect more futures in the worker, causing to fill the queue. This in the end causes also to fail more jobs - causing even more work.</p>
<p>This allows explains why our workers have a higher memory consumption - we had to increase the worker memory to have a stable worker.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="profiling-the-system">Profiling the System<a href="#profiling-the-system" class="hash-link" aria-label="Direct link to Profiling the System" title="Direct link to Profiling the System">​</a></h3>
<p>With the previous results, we were encouraged to do some profiling. For the start we used <a href="https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170" target="_blank" rel="noopener noreferrer">JFR</a> for some basic profiling.</p>
<p>You can do this by:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  kubectl exec -it &quot;$1&quot; -- jcmd 1 JFR.start duration=100s filename=/usr/local/camunda/data/flight-$(date +%d%m%y-%H%M).jfr</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If the flight recording is done, you can copy the recording (via <code>kubectl cp</code>) and open it with Intellij (JMC didn&#x27;t work for me)</p>
<p><img decoding="async" loading="lazy" alt="first-profile" src="/zeebe-chaos/assets/images/first-profile-6385c51bbf0d219a4a48da47d2505805.png" width="2012" height="626" class="img_ev3q"></p>
<p>We see that the Spring filter chaining is dominating the profile, which is not unexpected as every request has gone through this chain. As this is a CPU based sampling profile, it is likely to be part of the profile. Still, it was something interesting to note and investigate.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="path-pattern-matching">Path pattern matching<a href="#path-pattern-matching" class="hash-link" aria-label="Direct link to Path pattern matching" title="Direct link to Path pattern matching">​</a></h4>
<p>Some research showed that it might be interesting to look into other path pattern matchers, as we use the (legacy) <a href="https://github.com/camunda/camunda/blob/main/dist/src/main/resources/application.properties#L17" target="_blank" rel="noopener noreferrer">ant path matcher</a> with <a href="https://github.com/camunda/camunda/blob/main/authentication/src/main/java/io/camunda/authentication/config/WebSecurityConfig.java#L86" target="_blank" rel="noopener noreferrer">regex</a>.</p>
<p><strong>Resources:</strong></p>
<ul>
<li>PathPattern - <a href="https://spring.io/blog/2020/06/30/url-matching-with-pathpattern-in-spring-mvc#pathpattern" target="_blank" rel="noopener noreferrer">https://spring.io/blog/2020/06/30/url-matching-with-pathpattern-in-spring-mvc#pathpattern</a></li>
<li><a href="https://github.com/spring-projects/spring-framework/issues/31098#issuecomment-1891737375" target="_blank" rel="noopener noreferrer">Results of using PathPattern and related discussion on GH</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="gateway---broker-request-latency">Gateway - Broker request latency<a href="#gateway---broker-request-latency" class="hash-link" aria-label="Direct link to Gateway - Broker request latency" title="Direct link to Gateway - Broker request latency">​</a></h4>
<p>As we have such a high request-response latency, we have to find out where the time is spent. Ideally, we would have some sort of tracing (which we didn&#x27;t have yet), or we would look at metrics that cover sub-parts of the system and the request-response cycle.</p>
<p>The REST API request-response latency metric, we can take it as the complete round trip, accepting the request on the gateway edge, converting it to a Broker request, sending it to the Broker, the Broker processes, sends the response back, etc.</p>
<p>Luckily, we have a metric that is covering the part of sending the Broker request (from the other side of the Gateway) to the Broker and wait for the response. See related <a href="https://github.com/camunda/camunda/blob/main/zeebe/broker-client/src/main/java/io/camunda/zeebe/broker/client/impl/BrokerRequestManager.java#L153" target="_blank" rel="noopener noreferrer">code here</a>.</p>
<p>The difference shows us that there is not a small overhead, meaning that actually the Gateway to Broker request-response is slower with REST as well, which is unexpected.</p>
<p>This can either be because different data is sent, or a different API is used, or some other execution mechanics, etc.</p>
<p>Using the same cluster and enabling the REST API later, we can see the immediate effect on performance.</p>
<p><img decoding="async" loading="lazy" alt="rest-enabled" src="/zeebe-chaos/assets/images/rest-enabled-845502a54c3df66f99d1e87d99480221.png" width="2512" height="652" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="request-handling-execution-logic">Request handling execution logic<a href="#request-handling-execution-logic" class="hash-link" aria-label="Direct link to Request handling execution logic" title="Direct link to Request handling execution logic">​</a></h5>
<p>A difference we have spotted with REST API and gRPC is the usage of the BrokerClient.</p>
<p>While we use on the gRPC side the <a href="https://github.com/camunda/camunda/blob/main/zeebe/gateway-grpc/src/main/java/io/camunda/zeebe/gateway/EndpointManager.java#L457" target="_blank" rel="noopener noreferrer">BrokerClient with retries</a> and direct response handling, on the REST API we use no retries and <a href="https://github.com/camunda/camunda/blob/main/service/src/main/java/io/camunda/service/ApiServices.java#L55" target="_blank" rel="noopener noreferrer">handle the response async with the ForkJoinPool</a>.</p>
<p>As our benchmark clusters have two CPUs, <a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html" target="_blank" rel="noopener noreferrer">meaning 1 Thread for the common ForkJoin thread pool</a> we expected some contention on the thread.</p>
<p>For testing purposes, we increased the thread count by: <code>-Djava.util.concurrent.ForkJoinPool.common.parallelism=8</code></p>
<p>In a profile we can see that more threads are used, but it doesn&#x27;t change anything in the performance.</p>
<p><img decoding="async" loading="lazy" alt="profile-inc-fork-join" src="/zeebe-chaos/assets/images/profile-inc-fork-join-f567119ebd3858f2f5414425b7212228.png" width="2537" height="630" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="rest-gw-metrics-after-increaese-thread-pool" src="/zeebe-chaos/assets/images/rest-gw-metrics-after-increaese-thread-pool-f43f9d58ea3cc7f791d9e8a6aa85e32f.png" width="1907" height="761" class="img_ev3q"></p>
<p>The assumption was that we might not be able to handle the response in time with one thread, and this causes some contention also on the Gateway-Broker request-response cycle, but this is not the case.</p>
<p>We seem to spend time somewhere else or have a general resource contention issue. What we can see is that we have to work with more CPU throttling, then without REST API usage.</p>
<p><img decoding="async" loading="lazy" alt="rest-api-cpu-throttling.png" src="/zeebe-chaos/assets/images/rest-api-cpu-throttling-f4210b87ca53b8fcab9741cf12e76a60.png" width="2541" height="950" class="img_ev3q"></p>
<p>Increasing the CPU resolves the general performance problem, hinting even more that we might have some issues with threads competing with resources, etc.</p>
<p>In the following screenshot, you see the test with 6 CPUs per Camunda application.</p>
<p><img decoding="async" loading="lazy" alt="six-cpus" src="/zeebe-chaos/assets/images/six-cpus-fcbf79199e8adfeb2dd3f073ae0020fe.png" width="1896" height="933" class="img_ev3q"></p>
<p>Compared to the previous run with 2 CPUs per Camunda application, where it had to fight with a lot of CPU throttling. The request-response latency was five times higher on average.</p>
<p><img decoding="async" loading="lazy" alt="two-cpus" src="/zeebe-chaos/assets/images/two-cpus-a003a95c94a18bd11326736ffd163332.png" width="1901" height="925" class="img_ev3q"></p>
<p>We have to further investigate this based on this knowledge.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="day-2-profiling-and-experimenting">Day 2: Profiling and Experimenting<a href="#day-2-profiling-and-experimenting" class="hash-link" aria-label="Direct link to Day 2: Profiling and Experimenting" title="Direct link to Day 2: Profiling and Experimenting">​</a></h2>
<p>Yesterday I was taking profiles with 100s, to reduce the noise. Still, we can see that the filter chain is taking ~40% of the complete profile.</p>
<p><img decoding="async" loading="lazy" alt="jfr-10-minutes-filter-chain.png" src="/zeebe-chaos/assets/images/jfr-10-minutes-filter-chain-c3a4c75586a6c3ce023bad52889c5590.png" width="2540" height="665" class="img_ev3q"></p>
<p>When opening the JFR recording with JMC, we get some hints, related to context switches, CPU throttling (which we already know) and the inverted parallelism of GC (also mentioning high IO).</p>
<p><img decoding="async" loading="lazy" alt="locks-and-contention-context-switch.png" src="/zeebe-chaos/assets/images/locks-and-contention-context-switch-b3ebe8dc71f088fc16b4bf39f5defd2e.png" width="640" height="107" class="img_ev3q">
<img decoding="async" loading="lazy" alt="jfr-cpu-throttling-detection.png" src="/zeebe-chaos/assets/images/jfr-cpu-throttling-detection-5cfaa70f6b7e3b06c6ac745b8d317413.png" width="658" height="193" class="img_ev3q">
<img decoding="async" loading="lazy" alt="gc-ineffeciency-high-io.png" src="/zeebe-chaos/assets/images/gc-ineffeciency-high-io-8e03650f088004e21e0e549bf8cf985c.png" width="636" height="240" class="img_ev3q"></p>
<p>We have already seen in our metrics, for example, that we fight with high CPU throttling</p>
<p><img decoding="async" loading="lazy" alt="rest-base-cpu" src="/zeebe-chaos/assets/images/rest-base-cpu-c50ce9a1e9a0bb6715386fe81d11e5d8.png" width="2540" height="308" class="img_ev3q"></p>
<p>To better analyze (and circumvent that we have no tracing), I added some more metrics to understand where time is spent. Furthermore, I created a temporary dashboard to break down where time is spent.</p>
<p>When we look at the base with gRPC (taking our weekly benchmarks), we can see all latencies are low, and mostly under 5 ms.</p>
<p><img decoding="async" loading="lazy" alt="grpc-break-down.png" src="/zeebe-chaos/assets/images/grpc-break-down-5f9058a44a49e0fd5f38f87489eda3e7.png" width="2526" height="644" class="img_ev3q"></p>
<p>As soon as we enable the REST API, we can see the latencies go up. The most significant increase we see is in the job activations.</p>
<p><img decoding="async" loading="lazy" alt="rest-break-down" src="/zeebe-chaos/assets/images/rest-break-down-b8b23cb51fdc0b22bda557fd5f3d7836.png" width="2533" height="796" class="img_ev3q"></p>
<p>Fascinating is that the write to process latency, the time from acceptance by the CommandAPI until the processor processes this command, also increases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="virtual-threads">Virtual threads<a href="#virtual-threads" class="hash-link" aria-label="Direct link to Virtual threads" title="Direct link to Virtual threads">​</a></h3>
<p>To remove some thoughts about potential IO and CPU contention, I experimented with virtual threads, which we can <a href="https://www.baeldung.com/spring-6-virtual-threads" target="_blank" rel="noopener noreferrer">easily enable for Spring</a>.</p>
<p>I set the following system property on the statefulset.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">-Dspring.threads.virtual.enabled=true</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Taking a new profile, we can see that all the http threads are gone, but still the filtering is prominent.</p>
<p><img decoding="async" loading="lazy" alt="jfr-virtual-threads.png" src="/zeebe-chaos/assets/images/jfr-virtual-threads-c97b70375d089d057d3edecdd0ad2ad5.png" width="2522" height="714" class="img_ev3q"></p>
<p>Checking our metrics break-down again we see there is no benefit here.</p>
<p><img decoding="async" loading="lazy" alt="virtual-threads-break-down.png" src="/zeebe-chaos/assets/images/virtual-threads-break-down-2d3df2e117f534eeb119f4614c377e90.png" width="2532" height="849" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="direct-handling">Direct handling<a href="#direct-handling" class="hash-link" aria-label="Direct link to Direct handling" title="Direct link to Direct handling">​</a></h3>
<p>Investigating the code basis, we saw several times <code>#handleAsync</code> without using an extra executor, causing to use of the ForkJoinPool (as mentioned the other day). One idea was to <a href="https://github.com/camunda/camunda/commit/265d7164f5384be8c443c30b20e432582df09c24" target="_blank" rel="noopener noreferrer">directly handle the future completions</a>, meaning the response handling, etc.</p>
<p>We didn&#x27;t observe any benefits with this.</p>
<p><img decoding="async" loading="lazy" alt="direct-handling-breakdown.png" src="/zeebe-chaos/assets/images/direct-handling-breakdown-f5af66c79a5d1064e3d1e603651d8797.png" width="2530" height="808" class="img_ev3q"></p>
<p>In the JFR recording, we can see that less Threads are used, but the Spring filter chain is also super prominent.
<img decoding="async" loading="lazy" alt="direct-handling-v2-profile-too-much-filtering.png" src="/zeebe-chaos/assets/images/direct-handling-v2-profile-too-much-filtering-d3a5da919b5220949e925bbb9042dbb4.png" width="2529" height="692" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spring-pathpattern-parser-for-mvc">Spring PathPattern parser for MVC<a href="#spring-pathpattern-parser-for-mvc" class="hash-link" aria-label="Direct link to Spring PathPattern parser for MVC" title="Direct link to Spring PathPattern parser for MVC">​</a></h3>
<p>At the end of the day I finally came to try the <code>PathPattern</code> parser. As mentioned the other day, it is recommended to use it over the legacy <code>AntPathMatcher</code>.</p>
<p>The migration was <a href="https://github.com/camunda/camunda/commit/357522d8355a624a1c07e1fb889561254b0305ba" target="_blank" rel="noopener noreferrer">rather simple</a>, we can replace the <code>spring.mvc.pathmatch.matching-strategy=ant_path_matcher</code> with
<code>spring.mvc.pathmatch.matching-strategy=path_pattern_parser</code>, we only had to fix some occurrences of regex combinations with <code>**</code>, as it is only allowed to have <code>**</code> at the end (no regex after).</p>
<p>See related branch <a href="https://github.com/camunda/camunda/commits/ck-pattern-path-parse/" target="_blank" rel="noopener noreferrer">ck-pattern-path-parse</a>.</p>
<p><img decoding="async" loading="lazy" alt="path-pattern-breakdown" src="/zeebe-chaos/assets/images/path-pattern-breakdown-9d9c15dc4c5d21d70472756d370ca533.png" width="2512" height="817" class="img_ev3q"></p>
<p>We were able to reduce the latencies by half, which also allowed us to bring back our throughput.</p>
<p><img decoding="async" loading="lazy" alt="path-pattern-general.png" src="/zeebe-chaos/assets/images/path-pattern-general-a9e20dc4a52c74b0488488aa78fa6e62.png" width="2528" height="667" class="img_ev3q"></p>
<p>I did a cross-check with the current SNAPSHOT, and weirdly the SNAPSHOT now behaved the same. I will run this for a while to see the results, as it might fail after a certain period of time. As this might also be related to where the pods are scheduled (noisy neighbours etc.)</p>
<p><img decoding="async" loading="lazy" alt="rest-base-v2-breakdown.png" src="/zeebe-chaos/assets/images/rest-base-v2-breakdown-fe4ebdd50122d4c32e30517e8e66c2ce.png" width="2543" height="796" class="img_ev3q">
<img decoding="async" loading="lazy" alt="rest-base-v2-general.png" src="/zeebe-chaos/assets/images/rest-base-v2-general-b19aece8c6549aa08c4caef4ba45bf02.png" width="2527" height="679" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="combination-of-direct-handle-and-pathpattern">Combination of direct handle and PathPattern<a href="#combination-of-direct-handle-and-pathpattern" class="hash-link" aria-label="Direct link to Combination of direct handle and PathPattern" title="Direct link to Combination of direct handle and PathPattern">​</a></h3>
<p>On top of the above, I <a href="https://github.com/camunda/camunda/commits/ck-direct-handle/" target="_blank" rel="noopener noreferrer">combined the direct handling and PathPattern usage</a>, and this gave us the best results.</p>
<p>The latencies are only two times higher than gRPC vs before 5 times (and more).</p>
<p><img decoding="async" loading="lazy" alt="combination-of-all-breakdown.png" src="/zeebe-chaos/assets/images/combination-of-all-breakdown-3796dbecb61e2f55349b7e2b2fe9c58e.png" width="2502" height="907" class="img_ev3q"></p>
<p>The throttling of the CPU was reduced by half as well.</p>
<p><img decoding="async" loading="lazy" alt="combination-of-all-cpu.png" src="/zeebe-chaos/assets/images/combination-of-all-cpu-76b39f387018b794f171bb676b8b3832.png" width="2538" height="324" class="img_ev3q"></p>
<p>This gives a great stable throughput again.</p>
<p><img decoding="async" loading="lazy" alt="combination-of-all-general.png" src="/zeebe-chaos/assets/images/combination-of-all-general-285aa7b2d45b50d79f995ebffbbd697f.png" width="2531" height="684" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="day-3-observing-load-tests-and-further-experimenting">Day 3: Observing load tests and further experimenting<a href="#day-3-observing-load-tests-and-further-experimenting" class="hash-link" aria-label="Direct link to Day 3: Observing load tests and further experimenting" title="Direct link to Day 3: Observing load tests and further experimenting">​</a></h2>
<p>Yesterday, I have started several load tests for things I have tried out in code (like PathPattern or direct response handling), but also from different commits of the main branch (the current SNASPHOT, some commits that touch the rest gateway, and from begin of the week).</p>
<p>From what we observed is that some load tests can run stable for quite a while, until they break down. It is often related to restarts/rescheduling, or is already in general suboptimal resource distribution. At some point, the CPU throttling increases, and then the performance breaks down.</p>
<p><img decoding="async" loading="lazy" alt="all-namespaces-throughput" src="/zeebe-chaos/assets/images/all-namespaces-throughput-1e62889524faaeec5cd7fb3052db4f04.png" width="2561" height="860" class="img_ev3q"></p>
<p>Interesting was that on all JFR recordings (with and without PathPattern), I still saw the Spring filter chain take a big chunk of the profile. This is because the filter chain itself doesn&#x27;t change with using a different pattern parser.</p>
<p><img decoding="async" loading="lazy" alt="rest-base-v3-jfr.png" src="/zeebe-chaos/assets/images/rest-base-v3-jfr-03b09d7ca4ff602d0f55716b5e13c64f.png" width="1861" height="710" class="img_ev3q">
<img decoding="async" loading="lazy" alt="path-pattern-jfr.png" src="/zeebe-chaos/assets/images/path-pattern-jfr-b3978d0f445b946a903d52f8651efbca.png" width="1886" height="724" class="img_ev3q">
<img decoding="async" loading="lazy" alt="combination-of-all-jfr.png" src="/zeebe-chaos/assets/images/combination-of-all-jfr-96fe5ae899bbbab0a0739b88385e562e.png" width="1863" height="702" class="img_ev3q"></p>
<p>Based on the profile, we do not see much of a difference.</p>
<p>Today, I will validate the following:</p>
<ul>
<li>Is anti-affinity still enabled with our platform charts</li>
<li>Combination of virtual threads (to reduce the thread count and blocking behavior), with PathPattern (as this was the most stable test)</li>
<li>Maybe increasing the CPU limits again, to remove the K8 CPU throttling to better understand the system performance (until which the CPU consumption grows) and profile again</li>
<li>Investigate further - likely try a different profiler like asyncProfiler</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="anti-affinity">Anti-affinity<a href="#anti-affinity" class="hash-link" aria-label="Direct link to Anti-affinity" title="Direct link to Anti-affinity">​</a></h3>
<p>Simply as a sanity check, I wanted to validate whether we still use our <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" target="_blank" rel="noopener noreferrer">anti-affinity configuration</a> in our charts. This is to make sure that brokers are not scheduled on the same node. Unfortunately, this only works on the namespace level.</p>
<p>Indeed, we still have the configuration set:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">affinity</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token key atrule" style="color:#00a4db">podAntiAffinity</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          </span><span class="token key atrule" style="color:#00a4db">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">labelSelector</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token key atrule" style="color:#00a4db">matchExpressions</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">key</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> app.kubernetes.io/component</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token key atrule" style="color:#00a4db">operator</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> In</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token key atrule" style="color:#00a4db">values</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> core</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token key atrule" style="color:#00a4db">topologyKey</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> kubernetes.io/hostname</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>While this helps in the same namespace, this doesn&#x27;t prevent to have brokers from different namespaces from being scheduled on the same node (AFAIK). Potential for a noisy neighbor. But this is also the reason why we use smaller nodes, and try to assign most of the resources to the corresponding broker pods (which makes them effectively alone on the node).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rest-base-more-cpu">REST Base more CPU<a href="#rest-base-more-cpu" class="hash-link" aria-label="Direct link to REST Base more CPU" title="Direct link to REST Base more CPU">​</a></h3>
<p>To validate once more how the base (simply with REST API enabled) performs with more CPU, we have set up a test with 6 CPUs (request + limit). This is an increase of factor three (from 2 CPU to 6 CPU).</p>
<p>In general, the test was stable.</p>
<p><img decoding="async" loading="lazy" alt="rest-base-more-cpu-general.png" src="/zeebe-chaos/assets/images/rest-base-more-cpu-general-2805c929e8b0c3e022f51d058680ce2c.png" width="2541" height="699" class="img_ev3q">
<img decoding="async" loading="lazy" alt="rest-base-more-cpu-latency.png" src="/zeebe-chaos/assets/images/rest-base-more-cpu-latency-7c06f38617becaa9911e4ac564384777.png" width="2516" height="869" class="img_ev3q"></p>
<p>As soon as we increased the CPU the throttling went down.
<img decoding="async" loading="lazy" alt="rest-base-more-cpu-throttle.png" src="/zeebe-chaos/assets/images/rest-base-more-cpu-throttle-97d5c48839b561b3e766f343e638c035.png" width="2543" height="332" class="img_ev3q"></p>
<p>The consumption went up to 3 CPU, comparing to our gRPC benchmarks, this is an increase of factor two!</p>
<p><img decoding="async" loading="lazy" alt="rest-base-more-cpu-usage.png" src="/zeebe-chaos/assets/images/rest-base-more-cpu-usage-99b9d828203b094e813586b4c0dc2e90.png" width="1272" height="348" class="img_ev3q"></p>
<p>While observing the test, we noticed some weird behavior of the workers. There are multiple regular job activation requests sent (while we still have Job Push enabled and in use).</p>
<p><img decoding="async" loading="lazy" alt="rest-base-more-cpu-throughput.png" src="/zeebe-chaos/assets/images/rest-base-more-cpu-throughput-7a7a9453614418df52669c0d94004e67.png" width="2541" height="293" class="img_ev3q"></p>
<p>This is also causing to have much higher job COMPLETE command rate, where most of them are actually rejected. We see ~500 job completion rejections per second!</p>
<p><img decoding="async" loading="lazy" alt="rest-base-more-cpu-logstream.png" src="/zeebe-chaos/assets/images/rest-base-more-cpu-logstream-7c26d6f0c3c061a89b206556a799ead6.png" width="2537" height="374" class="img_ev3q"></p>
<p>Why we have this behavior is not yet fully clear. The load test is stabilizing at a later point and running straight for several days.</p>
<p><img decoding="async" loading="lazy" alt="rest-base-more-cpu-longer-general.png" src="/zeebe-chaos/assets/images/rest-base-more-cpu-longer-general-94f15c958548f397d84b602000ade539.png" width="1899" height="891" class="img_ev3q"></p>
<p>At some-point it went into struggle again, as it run out of disk space. The exporter had a too big backlog, and was not able to catch up.</p>
<p><img decoding="async" loading="lazy" alt="rest-base-more-cpu-exporting.png" src="/zeebe-chaos/assets/images/rest-base-more-cpu-exporting-337702395ccb834bb43806e79ec4c233.png" width="388" height="226" class="img_ev3q"></p>
<p>This might be related to the huge number of commands and rejections that need to be skipped.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="combination-of-vt-and-pathpattern">Combination of VT and PathPattern<a href="#combination-of-vt-and-pathpattern" class="hash-link" aria-label="Direct link to Combination of VT and PathPattern" title="Direct link to Combination of VT and PathPattern">​</a></h3>
<p>As another experiment, we run a load test with enabling virtual threads and PathPattern parser on Spring.</p>
<p>To summarize, it doesn&#x27;t help to reduce the CPU consumption to a level that the system can run stable.
At the beginning, the worker was able to complete at least ~30 jobs per second, but later it fully stopped.</p>
<p><img decoding="async" loading="lazy" alt="vt-pathpattern-cpu-general.png" src="/zeebe-chaos/assets/images/vt-pathpattern-cpu-general-cb50efe294975dd6c0356af178a3715f.png" width="1905" height="678" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="vt-pathpattern-cpu-usage.png" src="/zeebe-chaos/assets/images/vt-pathpattern-latency-4a7b9872c3a15fde686374dca02a88d4.png" width="1911" height="843" class="img_ev3q">
<img decoding="async" loading="lazy" alt="vt-pathpattern-cpu-usage.png" src="/zeebe-chaos/assets/images/vt-pathpattern-throughput-f55ad671910ebaf6ac413e97fae7cadc.png" width="1905" height="222" class="img_ev3q"></p>
<p>In our JFR recording, we see a similar pattern, where the Spring filtering is still taking most of the samples.</p>
<p><img decoding="async" loading="lazy" alt="vt-pathpattern-cpu-jfr.png" src="/zeebe-chaos/assets/images/vt-pathpattern-cpu-jfr-f072b798ee53305315e2f2111b2ba499.png" width="2541" height="697" class="img_ev3q"></p>
<p>Enabling the virtual threads on spring, at least seem to remove the HTTP threads we normally had in our profiles.</p>
<p>The CPU throttling is rather high, causing the performance problems we see here.</p>
<p><img decoding="async" loading="lazy" alt="vt-pathpattern-cpu-throttle.png" src="/zeebe-chaos/assets/images/vt-pathpattern-cpu-throttle-674b1d2f70160c8a482470a50fe09a6f.png" width="2537" height="314" class="img_ev3q"></p>
<p>Zeebe-2 is often between 50-80% CPU throttling, as it is consuming 1.8 CPU (limit is 2).</p>
<p><img decoding="async" loading="lazy" alt="vt-pathpattern-cpu-usage.png" src="/zeebe-chaos/assets/images/vt-pathpattern-cpu-usage-fb0fc34ea832fb6f36ecc5c5f507220f.png" width="1259" height="328" class="img_ev3q"></p>
<p>The workers stopped working at some point completely. Investigating this, we can see that it fails with some OOM as well.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Jul 05, 2025 07:16:48.330 [pool-4-thread-8] WARN  io.camunda.client.job.worker - Worker benchmark failed to handle job with key 2251799882041322 of type benchmark-task, sending fail command to broker</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">java.lang.IllegalStateException: Queue full</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Exception in thread &quot;prometheus-http-1-6&quot; java.lang.OutOfMemoryError: Java heap space</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Jul 05, 2025 7:18:48 AM io.prometheus.metrics.exporter.httpserver.HttpExchangeAdapter sendErrorResponseWithStackTrace</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">SEVERE: The Prometheus metrics HTTPServer caught an Exception while trying to send the metrics response.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">java.io.IOException: Broken pipe</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/sun.net.httpserver.Request$WriteStream.write(Request.java:421)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/sun.net.httpserver.ChunkedOutputStream.writeChunk(ChunkedOutputStream.java:131)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/sun.net.httpserver.ChunkedOutputStream.flush(ChunkedOutputStream.java:165)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/sun.net.httpserver.ChunkedOutputStream.close(ChunkedOutputStream.java:140)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/sun.net.httpserver.PlaceholderOutputStream.close(ExchangeImpl.java:477)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:272)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.prometheus.metrics.exporter.common.PrometheusScrapeHandler.handleRequest(PrometheusScrapeHandler.java:74)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.prometheus.metrics.exporter.httpserver.MetricsHandler.handle(MetricsHandler.java:33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:98)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:101)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:871)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:98)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:847)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.lang.Thread.run(Thread.java:1583)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="async-profiler">Async profiler<a href="#async-profiler" class="hash-link" aria-label="Direct link to Async profiler" title="Direct link to Async profiler">​</a></h3>
<p>To enrich our insights and inputs (have more data to investigate), we tried to set up <a href="https://github.com/async-profiler/async-profiler" target="_blank" rel="noopener noreferrer">async profiler</a> with our load tests.</p>
<p>We had some <a href="https://github.com/camunda/camunda/tree/main/zeebe/benchmarks/docs/debug#async-profiler" target="_blank" rel="noopener noreferrer">out dated documentation</a> in our mono repository. Due to several refactorings, restructurings, etc. this guide was no longer working.</p>
<p>I was able to create a script to set it up for now:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">#!/bin/bash -xeu</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Usage:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#   ./executeProfiling.sh &lt;POD-NAME&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">set -oxe pipefail</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">node=$1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Download and extract latest async profiler</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">curl -L https://github.com/jvm-profiling-tools/async-profiler/releases/download/v4.0/async-profiler-4.0-linux-x64.tar.gz -o profiler.tar.gz</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cat profiler.tar.gz | tar xzv </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Copy async profiler to pod</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl cp async-profiler-4.0-linux-x64/bin/asprof &quot;$node&quot;:/usr/local/camunda/data/asprof</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl exec &quot;$node&quot; -- mkdir -p /usr/local/camunda/data/lib</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl cp async-profiler-4.0-linux-x64/lib/libasyncProfiler.so &quot;$node&quot;:/usr/local/camunda/data/libasyncProfiler.so</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl exec &quot;$node&quot; -- chmod +x /usr/local/camunda/data/asprof</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Run profiling</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">filename=flamegraph-$(date +%Y-%m-%d_%H-%M-%S).html</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">PID=$(kubectl exec &quot;$node&quot; -- jps | grep Standalone | cut -d &quot; &quot; -f 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl exec &quot;$node&quot; -- ./data/asprof -e itimer -d 100 -t -f &quot;/usr/local/camunda/data/$filename&quot; --libpath /usr/local/camunda/data/libasyncProfiler.so &quot;$PID&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Copy result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl cp &quot;$node:/usr/local/camunda/data/$filename&quot; &quot;$node-$filename&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The results need to be investigated next.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="follow-up-questions">Follow-up questions<a href="#follow-up-questions" class="hash-link" aria-label="Direct link to Follow-up questions" title="Direct link to Follow-up questions">​</a></h3>
<ol>
<li>Why are benchmark applications targeted at the same gateway? How does the IP resolution work with the headless service (which returns an array of IPs)? It looks like it is picking always the same gateway.</li>
<li>Why are the workers sending so often job activations, while the job push is active?</li>
<li>Why do we have 500+ job completions per second? Overloading the cluster?</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="day-4-investigate-profiles-and-experiments">Day 4: Investigate profiles and experiments<a href="#day-4-investigate-profiles-and-experiments" class="hash-link" aria-label="Direct link to Day 4: Investigate profiles and experiments" title="Direct link to Day 4: Investigate profiles and experiments">​</a></h2>
<p>We will continue with investigating certain areas of our REST API, checking profiles, and experimenting with ideas.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="combination-with-more-cpu">Combination with more CPU<a href="#combination-with-more-cpu" class="hash-link" aria-label="Direct link to Combination with more CPU" title="Direct link to Combination with more CPU">​</a></h3>
<p>The virtual threads and PathPattern parser setting test was combined with more CPUs (from 2 to 3 CPUs).</p>
<p><img decoding="async" loading="lazy" alt="vt-pp-more-cpu-general" src="/zeebe-chaos/assets/images/vt-pp-more-cpu-general-26a90fdcf6b39068121037b78a2ae693.png" width="1894" height="699" class="img_ev3q">
<img decoding="async" loading="lazy" alt="vt-pp-more-cpu-latency" src="/zeebe-chaos/assets/images/vt-pp-more-cpu-latency-426cff7ce93d998d9cfecb7a04050d32.png" width="1889" height="904" class="img_ev3q"></p>
<p>The test is running stable, but needs to be further observed (as we have seen, they might fail at a later point in time).</p>
<p><img decoding="async" loading="lazy" alt="vt-pp-cpu" src="/zeebe-chaos/assets/images/vt-pp-cpu-5079fd7215db24f34be6f4812f807b42.png" width="1911" height="573" class="img_ev3q"></p>
<p>The CPU consumption and throttling looks rather stable.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="running-for-a-day">Running for a day<a href="#running-for-a-day" class="hash-link" aria-label="Direct link to Running for a day" title="Direct link to Running for a day">​</a></h4>
<p><img decoding="async" loading="lazy" alt="vt-pp-later-general.png" src="/zeebe-chaos/assets/images/vt-pp-latern-general-96509b51933ea37e9642d25dfe82fb68.png" width="1889" height="682" class="img_ev3q"></p>
<p>The test was running stably for a good amount of time, but suddenly broke down.
The CPU usage increases heavily, causing throttling and breaking the system again, but this is just a symptom.</p>
<p><img decoding="async" loading="lazy" alt="vt-pp-later-cpu.png" src="/zeebe-chaos/assets/images/vt-pp-later-cpu-1d750c077153e327bbe6c29226326d0d.png" width="1884" height="572" class="img_ev3q"></p>
<p>When we investigate further the metrics, we can see that the latency, especially the commit latency, is increasing at the same time.</p>
<p><img decoding="async" loading="lazy" alt="vt-pp-later-latency.png" src="/zeebe-chaos/assets/images/vt-pp-later-latency-9f2d4ab1fb68c535739983f273c9756f.png" width="1894" height="265" class="img_ev3q"></p>
<p>This is likely because we might exhaust our available I/O. Indeed, we write or commit more at this point in time.</p>
<p><img decoding="async" loading="lazy" alt="vt-pp-later-commit-rate.png" src="/zeebe-chaos/assets/images/vt-pp-later-commit-rate-d32992c19ffeb7b2a86f1665427b13ce.png" width="1918" height="299" class="img_ev3q"></p>
<p>Further investigation highlights that the rejections of JOB completions are spiking high</p>
<p><img decoding="async" loading="lazy" alt="vt-pp-increase-of-rejections.png" src="/zeebe-chaos/assets/images/vt-pp-increase-of-rejections-94d9092237d99dcb7295e1bed9eabdaf.png" width="1889" height="351" class="img_ev3q"></p>
<p>It looks like that a new job stream has been started, at which time the cluster starts to go into a failure mode.</p>
<p><img decoding="async" loading="lazy" src="/zeebe-chaos/assets/images/vt-pp-later-stream-start-0570880679983aa837aa093b9f11dd3a.png" width="645" height="213" class="img_ev3q"></p>
<p>Additionally, more jobs are pushed out to the clients, causing more to complete (duplicate), increasing the rejections.
<img decoding="async" loading="lazy" src="/zeebe-chaos/assets/images/vt-pp-later-more-push-4bb3245ec49ef3e42502c639502c74c6.png" width="1908" height="649" class="img_ev3q"></p>
<p>We can also see that the workers start to crash the loop, because they receive too many jobs.
<img decoding="async" loading="lazy" src="/zeebe-chaos/assets/images/vt-pp-later-worker-restart-1e6674c009b72589cb8a5c9f71721ffb.png" width="552" height="228" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="investigating-command-distribution">Investigating command distribution<a href="#investigating-command-distribution" class="hash-link" aria-label="Direct link to Investigating command distribution" title="Direct link to Investigating command distribution">​</a></h3>
<p>Running all of these tests, I investigate several things, and realized that for all of these tests, there is always one Camunda pod doing more than the others. To me, it looks like our load is not even distributed.</p>
<p><img decoding="async" loading="lazy" alt="gateway-cmd-distribution" src="/zeebe-chaos/assets/images/gateway-cmd-distribution-ad6a04a30d2e5d8b1d6731a4cbb861e9.png" width="2555" height="845" class="img_ev3q"></p>
<p>Due to the imbalanc,e one pod is doing more than the others, this pod is sacrificing of CPU throttling.</p>
<p><img decoding="async" loading="lazy" alt="gateway-cmd-distribution-cpu.png" src="/zeebe-chaos/assets/images/gateway-cmd-distribution-cpu-b1007184fb6188c17ff378e5efa9fb7a.png" width="2544" height="640" class="img_ev3q"></p>
<p>I think the challenge we face here is related to our setup using <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" target="_blank" rel="noopener noreferrer">a headless service</a> in our <a href="https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform-8.8/templates/core/service.yaml" target="_blank" rel="noopener noreferrer">Camunda Platform Helm Chart</a>.</p>
<p>This means we have a service deployed in K8 that is returning all IPs for all PODs when resolving. Likely, our client applications simply use the first IP they retrieve (instead of doing some more clever).
I think this would be something we should further investigate. Potential options are to have client load balancing (with the multiple IPs), use a different service, or
Deploy the standalone gateway (again) to better separate the concerns.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="profiling-with-async-profiler">Profiling with Async Profiler<a href="#profiling-with-async-profiler" class="hash-link" aria-label="Direct link to Profiling with Async Profiler" title="Direct link to Profiling with Async Profiler">​</a></h3>
<p>As mentioned the other day, I have run the async profiler to get some more information/details from a different angle of the application execution.</p>
<p>Again, what we can see is that the web filter chaining is taking a big chunk of the samples.</p>
<p><img decoding="async" loading="lazy" alt="async-profile-rest-more-cpu-filter.png" src="/zeebe-chaos/assets/images/async-profile-rest-more-cpu-filter-45875207d59ca6d1e49f1b4752852e4f.png" width="1905" height="946" class="img_ev3q"></p>
<p>Furthermore, logging is also a big part of the profile.</p>
<p><img decoding="async" loading="lazy" alt="async-profile-rest-more-cpu-logging.png" src="/zeebe-chaos/assets/images/async-profile-rest-more-cpu-logging-2cd0ca8b7e1bc0ba2fc52e5172e0f642.png" width="1910" height="947" class="img_ev3q"></p>
<p>At the time of profiling, we were retrieving a lot of errors from the Brokers, due to rejections, etc. (see above).</p>
<p>We can see that we repeatedly log exceptions with no message at all.</p>
<p><img decoding="async" loading="lazy" alt="logging-null" src="/zeebe-chaos/assets/images/logging-null-a55ddfcc3bdc1d3cf5e009f64b017f0d.png" width="1497" height="588" class="img_ev3q">
<img decoding="async" loading="lazy" alt="logs-repeating" src="/zeebe-chaos/assets/images/logs-repeating-68bf15987c6eb6f37f31c339862a5b09.png" width="979" height="576" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="usage-metrics">Usage metrics<a href="#usage-metrics" class="hash-link" aria-label="Direct link to Usage metrics" title="Direct link to Usage metrics">​</a></h4>
<p>When I investigated this further and checked our logging, I saw that we wrote a LOT of usage metrics logs</p>
<p><img decoding="async" loading="lazy" alt="usage-metrics-rest-logs.png" src="/zeebe-chaos/assets/images/usage-metrics-rest-logs-7e8059c9f2da56a472abcaadb56a1226.png" width="1845" height="518" class="img_ev3q"></p>
<p>Based on the metrics, the exporting of usage metrics seem to be correlating to the state size growing.
<img decoding="async" loading="lazy" alt="usage-metrics-rest-state-size.png" src="/zeebe-chaos/assets/images/usage-metrics-rest-test-223a5e1c7017d4883483e06d63640a63.png" width="2523" height="342" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="usage-metrics-rest-state-size.png" src="/zeebe-chaos/assets/images/usage-metrics-rest-state-size-1e3f9754c526ac949b8fe6ec73af64ab.png" width="1262" height="290" class="img_ev3q"></p>
<p>This needs to be further clarified, whether this is expected.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="last-day-experimenting">Last day (experimenting)<a href="#last-day-experimenting" class="hash-link" aria-label="Direct link to Last day (experimenting)" title="Direct link to Last day (experimenting)">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="load-balance">Load balance<a href="#load-balance" class="hash-link" aria-label="Direct link to Load balance" title="Direct link to Load balance">​</a></h3>
<p>As discovered on the previous day, we are sending requests mainly to one node. This is because of the usage of a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" target="_blank" rel="noopener noreferrer">headless service</a> in our <a href="https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform-8.8/templates/core/service.yaml" target="_blank" rel="noopener noreferrer">Camunda Platform Helm Chart</a>.</p>
<p>Today, I experimented with using a different service to access the Gateway API with the clients.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Manifest</summary><div><div class="collapsibleContent_i85q"><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Service</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">annotations</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">meta.helm.sh/release-name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ck</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">rest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">baseload</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">balancer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">meta.helm.sh/release-namespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ck</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">rest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">baseload</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">balancer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">labels</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> camunda</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">platform</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/component</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> gateway</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/instance</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ck</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">rest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">baseload</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">balancer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/managed-by</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Helm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> camunda</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">platform</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/part-of</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> camunda</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">platform</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/version</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ck</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">rest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">base</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">more</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">cpu</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">cd459997</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">helm.sh/chart</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> camunda</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">platform</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13.0.0</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">alpha4.2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ck</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">rest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">baseload</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">balancer</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">core</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gw</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">namespace</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ck</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">rest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">baseload</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">balancer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">type</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ClusterIP </span><span class="token comment" style="color:#999988;font-style:italic"># &lt;---- That is the important part</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">ports</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> http</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">port</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">protocol</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> TCP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">targetPort</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> internal</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">port</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">26502</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">protocol</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> TCP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">targetPort</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">26502</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> command</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">port</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">26501</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">protocol</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> TCP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">targetPort</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">26501</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">port</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">9600</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">protocol</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> TCP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">targetPort</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">9600</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> gateway</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">port</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">26500</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">protocol</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> TCP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">targetPort</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">26500</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">selector</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> camunda</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">platform</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/component</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> core</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/instance</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> ck</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">rest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">baseload</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">balancer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/managed-by</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Helm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> camunda</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">platform</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">app.kubernetes.io/part-of</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> camunda</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">platform</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details>
<p>By deploying this service and changing the client application deployments, we can see the effect directly.</p>
<p><img decoding="async" loading="lazy" alt="service-differences-load.png" src="/zeebe-chaos/assets/images/service-differences-load-613fc9b84be02bf1c85f19c869ed738f.png" width="2546" height="434" class="img_ev3q"></p>
<p>The load is well distributed, and the CPU is as well.</p>
<p><img decoding="async" loading="lazy" alt="service-differences-cpu.png" src="/zeebe-chaos/assets/images/service-differences-cpu-7847598c889896f2c9aed9e0d7aca023.png" width="2521" height="304" class="img_ev3q"></p>
<p>In general the performance looks pretty stable and good.
<img decoding="async" loading="lazy" alt="service-differences-general.png" src="/zeebe-chaos/assets/images/service-differences-general-92bbf0df630e027f8c27be8c0d6d86cc.png" width="2557" height="685" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="after-the-night">After the night<a href="#after-the-night" class="hash-link" aria-label="Direct link to After the night" title="Direct link to After the night">​</a></h4>
<p>After running it for a while, the cluster was still looking quite stable.</p>
<p><img decoding="async" loading="lazy" alt="service-differences-general-later.png" src="/zeebe-chaos/assets/images/service-differences-general-later-17413ad623850d0f188ebf6ab33e1d2c.png" width="2530" height="679" class="img_ev3q">
<img decoding="async" loading="lazy" alt="service-differences-cpu-later.png" src="/zeebe-chaos/assets/images/service-differences-cpu-later-e057bf19890c0e660dee2745d03dde4e.png" width="2549" height="577" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="after-lunch">After lunch<a href="#after-lunch" class="hash-link" aria-label="Direct link to After lunch" title="Direct link to After lunch">​</a></h4>
<p><img decoding="async" loading="lazy" alt="service-differences-general-later2.png" src="/zeebe-chaos/assets/images/service-differences-general-later2-b010f5338701d060a2226e5386a3fe12.png" width="2530" height="677" class="img_ev3q">
<img decoding="async" loading="lazy" alt="service-differences-cpu-later2.png" src="/zeebe-chaos/assets/images/service-differences-cpu-later2-807ffd401630456ce1001fe66f6e8545.png" width="2530" height="576" class="img_ev3q"></p>
<p>The general throughput looks still stable, even if the latency is much higher than for gRPC (was mostly ~3-5 ms).</p>
<p><img decoding="async" loading="lazy" alt="service-differences-latency-later.png" src="/zeebe-chaos/assets/images/service-differences-latency-later-c46ffdd546684517b98470c1488c1707.png" width="2533" height="814" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="exporter-runs-into-an-issue">Exporter runs into an issue<a href="#exporter-runs-into-an-issue" class="hash-link" aria-label="Direct link to Exporter runs into an issue" title="Direct link to Exporter runs into an issue">​</a></h4>
<p>Observing the cluster, we have detected that the disk usage is increasing over time.</p>
<p><img decoding="async" loading="lazy" alt="service-differences-disk.png" src="/zeebe-chaos/assets/images/service-differences-disk-93e93917ee91b55454cf3e505e809588.png" width="829" height="229" class="img_ev3q"></p>
<p>The reason seem to be that the exporter is not able to catch up. Something that needs to be investigated separately.</p>
<p><img decoding="async" loading="lazy" alt="service-differences-exporter-not-catch-up.png" src="/zeebe-chaos/assets/images/service-differences-exporter-not-catch-up-bb0dfa673863e0eac699b7f89ff12bfb.png" width="524" height="223" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p><strong>Quint essence:</strong> REST API is more CPU intense/heavy than gRPC.</p>
<p>In general, this is not surprising. The REST API (and incl. Spring) works completely differently and is not as optimized for performance as gRPC is.
We can see this also in our latencies, which are twice+ higher even when we have enough resources available.</p>
<p>When correctly and evenly distributing the load we are able to handle the expected load on the cluster. Of course, this goes just until a certain load (even higher load) until it dips (as the CPU is exhausted again).</p>
<p>With gRPC, the bad request distribution was not an issue, as the overhead is low and has less of an impact.</p>
<p>The same behavior we had when assigning more resources to the cluster. Indicating CPU as the bottleneck (the issue is parallelizable)
The CPU consumption can&#x27;t be pinpointed to one single thing, but multiple inefficiencies coming together.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="discovered-issues">Discovered issues:<a href="#discovered-issues" class="hash-link" aria-label="Direct link to Discovered issues:" title="Direct link to Discovered issues:">​</a></h3>
<p>During the investigation, the following issues have been discovered, which we should look at (and ideally fix).</p>
<ul>
<li><a href="https://github.com/camunda/camunda-platform-helm/issues/3784" target="_blank" rel="noopener noreferrer">Helm Chart 8.8 is using a headless service for the single Application</a></li>
<li><strong>REST API</strong>
<ul>
<li><a href="https://github.com/camunda/camunda/issues/35067" target="_blank" rel="noopener noreferrer">Investigate and improve web filter chain - as this was the dominator in all our profiles</a></li>
<li><a href="https://github.com/camunda/camunda/issues/35069" target="_blank" rel="noopener noreferrer">Sub-optimal logging in REST API v2</a></li>
<li><a href="https://github.com/camunda/camunda/issues/35076" target="_blank" rel="noopener noreferrer">REST API response handling is running into contention</a></li>
</ul>
</li>
<li><strong>Zeebe</strong>
<ul>
<li><a href="https://github.com/camunda/camunda/issues/35071" target="_blank" rel="noopener noreferrer">Usage metrics heavily looping</a></li>
<li><a href="https://github.com/camunda/camunda/issues/35072" target="_blank" rel="noopener noreferrer">Usage metrics records are divided unnecessary</a></li>
<li><a href="https://github.com/camunda/camunda/issues/35074" target="_blank" rel="noopener noreferrer">Job push vicious circle</a></li>
<li><a href="https://github.com/camunda/camunda/issues/35080" target="_blank" rel="noopener noreferrer">Camunda Exporter is not able to catch up on all partitions</a></li>
</ul>
</li>
<li>Clients<!-- -->
<ul>
<li><a href="https://github.com/camunda/camunda/issues/34597" target="_blank" rel="noopener noreferrer">Client failure handling ends in Stackoverflow (causing the client to completelty stop working)</a></li>
<li><a href="https://github.com/camunda/camunda/issues/35077" target="_blank" rel="noopener noreferrer">Workers and Starters are holding futures in a queue</a></li>
</ul>
</li>
</ul></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/camunda/zeebe-chaos/blob/master/chaos-days/blog/2025-07-02-Follow-up-REST-API-performance/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/zeebe-chaos/2025/07/11/linear-dynamic-scaling"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Dynamic Scaling: probing linear scalability</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/zeebe-chaos/2025/06/30/Performance-of-REST-API"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Performance of REST API</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#investigating-rest-api-performance" class="table-of-contents__link toc-highlight">Investigating REST API performance</a></li><li><a href="#day-1-investigation-rest-api-performance" class="table-of-contents__link toc-highlight">Day 1: Investigation REST API Performance</a><ul><li><a href="#rest-api-metrics" class="table-of-contents__link toc-highlight">REST API Metrics</a></li><li><a href="#investigating-worker-errors" class="table-of-contents__link toc-highlight">Investigating Worker Errors</a></li><li><a href="#profiling-the-system" class="table-of-contents__link toc-highlight">Profiling the System</a></li></ul></li><li><a href="#day-2-profiling-and-experimenting" class="table-of-contents__link toc-highlight">Day 2: Profiling and Experimenting</a><ul><li><a href="#virtual-threads" class="table-of-contents__link toc-highlight">Virtual threads</a></li><li><a href="#direct-handling" class="table-of-contents__link toc-highlight">Direct handling</a></li><li><a href="#spring-pathpattern-parser-for-mvc" class="table-of-contents__link toc-highlight">Spring PathPattern parser for MVC</a></li><li><a href="#combination-of-direct-handle-and-pathpattern" class="table-of-contents__link toc-highlight">Combination of direct handle and PathPattern</a></li></ul></li><li><a href="#day-3-observing-load-tests-and-further-experimenting" class="table-of-contents__link toc-highlight">Day 3: Observing load tests and further experimenting</a><ul><li><a href="#anti-affinity" class="table-of-contents__link toc-highlight">Anti-affinity</a></li><li><a href="#rest-base-more-cpu" class="table-of-contents__link toc-highlight">REST Base more CPU</a></li><li><a href="#combination-of-vt-and-pathpattern" class="table-of-contents__link toc-highlight">Combination of VT and PathPattern</a></li><li><a href="#async-profiler" class="table-of-contents__link toc-highlight">Async profiler</a></li><li><a href="#follow-up-questions" class="table-of-contents__link toc-highlight">Follow-up questions</a></li></ul></li><li><a href="#day-4-investigate-profiles-and-experiments" class="table-of-contents__link toc-highlight">Day 4: Investigate profiles and experiments</a><ul><li><a href="#combination-with-more-cpu" class="table-of-contents__link toc-highlight">Combination with more CPU</a></li><li><a href="#investigating-command-distribution" class="table-of-contents__link toc-highlight">Investigating command distribution</a></li><li><a href="#profiling-with-async-profiler" class="table-of-contents__link toc-highlight">Profiling with Async Profiler</a></li></ul></li><li><a href="#last-day-experimenting" class="table-of-contents__link toc-highlight">Last day (experimenting)</a><ul><li><a href="#load-balance" class="table-of-contents__link toc-highlight">Load balance</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a><ul><li><a href="#discovered-issues" class="table-of-contents__link toc-highlight">Discovered issues:</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/zeebe" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://forum.camunda.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/Camunda" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/camunda/zeebe-chaos/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Zeebe Chaos. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>