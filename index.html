<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">Blog | Zeebe Chaos</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://camunda.github.io/zeebe-chaos/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | Zeebe Chaos"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/zeebe-chaos/img/zeebe-logo.png"><link data-rh="true" rel="canonical" href="https://camunda.github.io/zeebe-chaos/"><link data-rh="true" rel="alternate" href="https://camunda.github.io/zeebe-chaos/" hreflang="en"><link data-rh="true" rel="alternate" href="https://camunda.github.io/zeebe-chaos/" hreflang="x-default"><script data-rh="true">function insertBanner(){var n=document.createElement("div");n.id="__docusaurus-base-url-issue-banner-container";n.innerHTML='\n<div id="__docusaurus-base-url-issue-banner" style="border: thick solid red; background-color: rgb(255, 230, 179); margin: 20px; padding: 20px; font-size: 20px;">\n   <p style="font-weight: bold; font-size: 30px;">Your Docusaurus site did not load properly.</p>\n   <p>A very common reason is a wrong site <a href="https://docusaurus.io/docs/docusaurus.config.js/#baseUrl" style="font-weight: bold;">baseUrl configuration</a>.</p>\n   <p>Current configured baseUrl = <span style="font-weight: bold; color: red;">/zeebe-chaos/</span> </p>\n   <p>We suggest trying baseUrl = <span id="__docusaurus-base-url-issue-banner-suggestion-container" style="font-weight: bold; color: green;"></span></p>\n</div>\n',document.body.prepend(n);var e=document.getElementById("__docusaurus-base-url-issue-banner-suggestion-container"),s=window.location.pathname,o="/"===s.substr(-1)?s:s+"/";e.innerHTML=o}document.addEventListener("DOMContentLoaded",(function(){void 0===window.docusaurus&&insertBanner()}))</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"Blog","@id":"https://camunda.github.io/zeebe-chaos/","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/","headline":"Blog","description":"Blog","blogPost":[{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance","url":"https://camunda.github.io/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance","headline":"Follow up REST API performance","name":"Follow up REST API performance","description":"This blog post aims to summarize the investigation of the REST API performance, and give some hints and collections of what to improve.","datePublished":"2025-07-02T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2025/06/30/Performance-of-REST-API","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2025/06/30/Performance-of-REST-API","url":"https://camunda.github.io/zeebe-chaos/2025/06/30/Performance-of-REST-API","headline":"Performance of REST API","name":"Performance of REST API","description":"In today's Chaos day we wanted to experiment with the new REST API (v2) as a replacement for our previous used gRPC API.","datePublished":"2025-06-30T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS","url":"https://camunda.github.io/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS","headline":"How does Zeebe behave with NFS","name":"How does Zeebe behave with NFS","description":"This week, we (Lena, Nicolas, Roman, and I) held a workshop where we looked into how Zeebe behaves with network file storage (NFS).","datePublished":"2025-06-12T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment","url":"https://camunda.github.io/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment","headline":"Lower memory consumption of Camunda deployment","name":"Lower memory consumption of Camunda deployment","description":"I'm back to finally do some load testing again.","datePublished":"2025-06-05T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project","url":"https://camunda.github.io/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project","headline":"News from Camunda Exporter project","name":"News from Camunda Exporter project","description":"In this Chaos day, we want to verify the current state of the exporter project and run benchmarks with it. Comparing","datePublished":"2024-12-12T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance","url":"https://camunda.github.io/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance","headline":"Impact of Camunda Exporter on processing performance","name":"Impact of Camunda Exporter on processing performance","description":"In our last Chaos day we experimented with the Camunda Exporter MVP. After our MVP we continued with Iteration 2, where we migrated the Archiver deployments and added a new Migration component (allows us to harmonize indices).","datePublished":"2024-11-14T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP","url":"https://camunda.github.io/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP","headline":"Camunda Exporter MVP","name":"Camunda Exporter MVP","description":"After a long pause, I come back with an interesting topic to share and experiment with. Right now we are re-architecture","datePublished":"2024-10-24T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark","url":"https://camunda.github.io/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark","headline":"Optimizing cluster sizing using a real world benchmark","name":"Optimizing cluster sizing using a real world benchmark","description":"Our first goal of this experiment is to use a benchmarks to","datePublished":"2024-10-14T00:00:00.000Z","author":{"@type":"Person","name":"Rodrigo Lopes","description":"Associate Software Engineer @ Zeebe","url":"https://github.com/rodrigo-lourenco-lopes","image":"https://github.com/rodrigo-lourenco-lopes.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2024/08/19/Operate-improve-import-latency","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2024/08/19/Operate-improve-import-latency","url":"https://camunda.github.io/zeebe-chaos/2024/08/19/Operate-improve-import-latency","headline":"Improve Operate import latency","name":"Improve Operate import latency","description":"In our last Chaos Day we experimented with Operate and different load (Zeebe throughput). We observed that a higher load caused a lower import latency in Operate. The conclusion was that it might be related to Zeebe's exporting configuration, which is affected by a higher load.","datePublished":"2024-08-19T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://camunda.github.io/zeebe-chaos/2024/08/16/Operate-load-handling","mainEntityOfPage":"https://camunda.github.io/zeebe-chaos/2024/08/16/Operate-load-handling","url":"https://camunda.github.io/zeebe-chaos/2024/08/16/Operate-load-handling","headline":"Operate load handling","name":"Operate load handling","description":"Happy to announce that we are broadening the scope of our Chaos days, to look holistically at the whole Camunda Platform, starting today.","datePublished":"2024-08-16T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/ChrisKujawa","image":"https://github.com/ChrisKujawa.png"},"keywords":[]}]}</script><link rel="alternate" type="application/rss+xml" href="/zeebe-chaos/rss.xml" title="Zeebe Chaos RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zeebe-chaos/atom.xml" title="Zeebe Chaos Atom Feed"><link rel="stylesheet" href="/zeebe-chaos/assets/css/styles.09e3c82e.css">
<script src="/zeebe-chaos/assets/js/runtime~main.f2d6d19e.js" defer="defer"></script>
<script src="/zeebe-chaos/assets/js/main.4991d34b.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zeebe-chaos/"><div class="navbar__logo"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Zeebe Chaos</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zeebe-chaos/">Chaos Summaries</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/camunda/zeebe-chaos" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance">Follow up REST API performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/30/Performance-of-REST-API">Performance of REST API</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS">How does Zeebe behave with NFS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment">Lower memory consumption of Camunda deployment</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project">News from Camunda Exporter project</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance">Impact of Camunda Exporter on processing performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">Camunda Exporter MVP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark">Optimizing cluster sizing using a real world benchmark</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency">Improve Operate import latency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/16/Operate-load-handling">Operate load handling</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting">Using flow control to handle bottleneck on exporting</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops">Using flow control to handle uncontrolled process loops</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/01/19/Job-Activation-Latency">Reducing the job activation delay</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/20/Broker-scaling-performance">Broker Scaling and Performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss">Dynamic Scaling with Dataloss</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers">Dynamically scaling brokers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/06/Job-Push-resiliency">Job push resiliency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/30/Job-push-overloading">Job push overloading</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing">Hot backups impact on processing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance">Using Large Multi-Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle">Continuing SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/15/SST-Partitioning-toggle">SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/04/06/gateway-termination">Gateway Termination</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/02/23/Recursive-call-activity">Recursive call activity</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition">Message Correlation after Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/02/deployment-distribution">Bring Deployment distribution experiment back</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS">Standalone Gateway in CCSaaS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/01/High-Snapshot-Frequency">High Snapshot Frequency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/01/19/big-variables">Handling of Big Variables</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2021</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance">Worker count should not impact performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys">Not produce duplicate Keys</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/29/Throughput-on-big-state">Throughput on big state</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/05/recovery-time">Recovery (Fail Over) time</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/09/23/Old-Clients">Old-Clients</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/07/06/Slow-Network">Slow Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/06/08/Full-Disk">Full Disk Recovery</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/05/25/Reset-Clock">Time travel Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/29/Corrupted-Snapshot">Corrupted Snapshot Experiment Investigation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering">BPMN meets Chaos Engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/30/set-file-immutable">Set file immutable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/23/camunda-cloud-network-partition">Camunda Cloud network partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/09/cont-workflow-instance">Fault-tolerant processing of process instances</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/02/23/automate-deployments-dist">Automating Deployment Distribution Chaos Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/26/deployments">Deployment Distribution</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/19/network-partition">Network partitions</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/07/disconnect-leader-and-follower">Disconnect Leader and one Follower</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2020</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/24/message-correlation-after-failover">Message Correlation after Failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/11/job-timeouts">Many Job Timeouts</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/03/investigate-failing-tests">Investigate failing Chaos Tests</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/20/non-graceful-shutdown">Non-graceful Shutdown Broker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/27/standalone-gw-memory">Gateway memory consumption</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/13/multiple-leader-changes">Multiple Leader Changes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/06/toxi-proxy">Play around with ToxiProxy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud">Experiment with Camunda Cloud</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/06/low-load">Experiment with Low Load</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/30/experiment-without-exporters">Experiment without Exporters</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/16/big-multi-instance">Big Multi Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/09/timer-and-huge-variables">Experiment with Timers and Huge Variables</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/02/extract-k8-resources">Extract K8 resources from namespace</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/25/gateway-network-partition">Gateway Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/18/correlate-message-after-failover">Correlate Message after failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/11/high-cpu-gateway">High CPU load on Standalone Gateway</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/04/first-chaos-day">First Chaos Day!</a></li></ul></div></nav></aside><main class="col col--7"><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/07/02/Follow-up-REST-API-performance">Follow up REST API performance</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-07-02T00:00:00.000Z">July 2, 2025</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>This blog post aims to summarize the investigation of the REST API performance, and give some hints and collections of what to improve.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="rest-api-metrics">REST API Metrics<a href="#rest-api-metrics" class="hash-link" aria-label="Direct link to REST API Metrics" title="Direct link to REST API Metrics">​</a></h2>
<p>One remark from the last experiments was that we do not have good insights for the REST API. Actually, we have the necessary metrics already exposed, but not yet available in our Dashboard.</p>
<p>This is currently prepared with <a href="https://github.com/camunda/camunda/pull/33907" target="_blank" rel="noopener noreferrer">#33907</a>. Based on this, I was able to further investigate the REST API performance.</p>
<p><img decoding="async" loading="lazy" alt="rest-api" src="/zeebe-chaos/assets/images/rest-api-922105b50aa5f7c2c731f488a8999976.png" width="2539" height="607" class="img_ev3q"></p>
<p>What we can see is that our requests take on average more than 50ms to complete. This is causing our throughput to go down, we are not able to create 150 PI/s even.</p>
<p>Looking at a different Benchmark using gRPC, we can see that requests take 5-10ms to complete, and have a stable throughput</p>
<p><img decoding="async" loading="lazy" alt="grpc-latency" src="/zeebe-chaos/assets/images/grpc-latency-097288055f8e099cc22f7268d67b53a9.png" width="2526" height="333" class="img_ev3q">
<img decoding="async" loading="lazy" alt="grpc" src="/zeebe-chaos/assets/images/grpc-93fbafebfd4eebb459d95c8c713b67d7.png" width="1273" height="486" class="img_ev3q"></p>
<p>Due to the slower workers (on completion), we can see error reports of the workers not being able to accept further job pushes. This has been mentioned in the previous blog post as well.  This, in consequence, means the worker sends FAIL commands for such jobs, to give them back. It has a cascading effect, as jobs are sent back and forth and impacting the general process instance execution latency (which grows up to 60s compared to 0.2 s).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="investigating-worker-errors">Investigating Worker errors<a href="#investigating-worker-errors" class="hash-link" aria-label="Direct link to Investigating Worker errors" title="Direct link to Investigating Worker errors">​</a></h2>
<p>In our previous experiments, we have seen the following exceptions</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">13:25:14.684 [pool-4-thread-3] WARN  io.camunda.client.job.worker - Worker benchmark failed to handle job with key 4503599628992806 of type benchmark-task, sending fail command to broker</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">java.lang.IllegalStateException: Queue full</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.ArrayBlockingQueue.add(ArrayBlockingQueue.java:329) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.camunda.zeebe.Worker.lambda$handleJob$1(Worker.java:122) ~[classes/:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.camunda.client.impl.worker.JobRunnableFactoryImpl.executeJob(JobRunnableFactoryImpl.java:45) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.camunda.client.impl.worker.JobRunnableFactoryImpl.lambda$create$0(JobRunnableFactoryImpl.java:40) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at io.camunda.client.impl.worker.BlockingExecutor.lambda$execute$0(BlockingExecutor.java:50) ~[camunda-client-java-8.8.0-SNAPSHOT.jar:8.8.0-SNAPSHOT]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This is actually coming from the Worker (benchmark) application, as it is collecting all <a href="https://github.com/camunda/camunda/blob/main/zeebe/benchmarks/project/src/main/java/io/camunda/zeebe/Worker.java#L54" target="_blank" rel="noopener noreferrer">the request futures in a blocking queue</a>.</p>
<p>As the performance is lower of handling requests, we collect more futures in the worker, causing to fill the queue. This in the end causes also to fail more jobs - causing even more work.</p>
<p>This allows explains why our workers have a higher memory consumption - we had to increase the worker memory to have a stable worker.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="profiling-the-system">Profiling the System<a href="#profiling-the-system" class="hash-link" aria-label="Direct link to Profiling the System" title="Direct link to Profiling the System">​</a></h2>
<p>With the previous results, we were encouraged to do some profiling. For the start we used <a href="https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170" target="_blank" rel="noopener noreferrer">JFR</a> for some basic profiling.</p>
<p>You can do this by:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  kubectl exec -it &quot;$1&quot; -- jcmd 1 JFR.start duration=100s filename=/usr/local/camunda/data/flight-$(date +%d%m%y-%H%M).jfr</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If the flight recording is done, you can copy the recording (via <code>kubectl cp</code>) and open it with Intellij (JMC didn&#x27;t work for me)</p>
<p><img decoding="async" loading="lazy" alt="first-profile" src="/zeebe-chaos/assets/images/first-profile-6385c51bbf0d219a4a48da47d2505805.png" width="2012" height="626" class="img_ev3q"></p>
<p>We see that the Spring filter chaining is dominating the profile, which is not unexpected as every request has gone through this chain. As this is a CPU based sampling profile, it is likely to be part of the profile. Still, it was something interesting to note and investigate.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="path-pattern-matching">Path pattern matching<a href="#path-pattern-matching" class="hash-link" aria-label="Direct link to Path pattern matching" title="Direct link to Path pattern matching">​</a></h3>
<p>Some research showed that it might be interesting to look into other path pattern matchers, as we use the (legacy) <a href="https://github.com/camunda/camunda/blob/main/dist/src/main/resources/application.properties#L17" target="_blank" rel="noopener noreferrer">ant path matcher</a> with <a href="https://github.com/camunda/camunda/blob/main/authentication/src/main/java/io/camunda/authentication/config/WebSecurityConfig.java#L86" target="_blank" rel="noopener noreferrer">regex</a>.</p>
<p><strong>Resources:</strong></p>
<ul>
<li>PathPattern - <a href="https://spring.io/blog/2020/06/30/url-matching-with-pathpattern-in-spring-mvc#pathpattern" target="_blank" rel="noopener noreferrer">https://spring.io/blog/2020/06/30/url-matching-with-pathpattern-in-spring-mvc#pathpattern</a></li>
<li><a href="https://github.com/spring-projects/spring-framework/issues/31098#issuecomment-1891737375" target="_blank" rel="noopener noreferrer">Results of using PathPattern and related discussion on GH</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gateway---broker-request-latency">Gateway - Broker request latency<a href="#gateway---broker-request-latency" class="hash-link" aria-label="Direct link to Gateway - Broker request latency" title="Direct link to Gateway - Broker request latency">​</a></h3>
<p>As we have such a high request-response latency, we have to find out where the time is spent. Ideally, we would have some sort of tracing (which we didn&#x27;t have yet), or we would look at metrics that cover sub-parts of the system and the request-response cycle.</p>
<p>The REST API request-response latency metric, we can take it as the complete round trip, accepting the request on the gateway edge, converting it to a Broker request, sending it to the Broker, the Broker processes, sends the response back, etc.</p>
<p>Luckily, we have a metric that is covering the part of sending the Broker request (from the other side of the Gateway) to the Broker and wait for the response. See related <a href="https://github.com/camunda/camunda/blob/main/zeebe/broker-client/src/main/java/io/camunda/zeebe/broker/client/impl/BrokerRequestManager.java#L153" target="_blank" rel="noopener noreferrer">code here</a>.</p>
<p>The difference shows us that there is not a small overhead, meaning that actually the Gateway to Broker request-response is slower with REST as well, which is unexpected.</p>
<p>This can either be because different data is sent, or a different API is used, or some other execution mechanics, etc.</p>
<p>Using the same cluster and enabling the REST API later, we can see the immediate effect on performance.</p>
<p><img decoding="async" loading="lazy" alt="rest-enabled" src="/zeebe-chaos/assets/images/rest-enabled-845502a54c3df66f99d1e87d99480221.png" width="2512" height="652" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="request-handling-execution-logic">Request handling execution logic<a href="#request-handling-execution-logic" class="hash-link" aria-label="Direct link to Request handling execution logic" title="Direct link to Request handling execution logic">​</a></h4>
<p>A difference we have spotted with REST API and gRPC is the usage of the BrokerClient.</p>
<p>While we use on the gRPC side the <a href="https://github.com/camunda/camunda/blob/main/zeebe/gateway-grpc/src/main/java/io/camunda/zeebe/gateway/EndpointManager.java#L457" target="_blank" rel="noopener noreferrer">BrokerClient with retries</a> and direct response handling, on the REST API we use no retries and <a href="https://github.com/camunda/camunda/blob/main/service/src/main/java/io/camunda/service/ApiServices.java#L55" target="_blank" rel="noopener noreferrer">handle the response async with the ForkJoinPool</a>.</p>
<p>As our benchmark clusters have two CPUs, <a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html" target="_blank" rel="noopener noreferrer">meaning 1 Thread for the common ForkJoin thread pool</a> we expected some contention on the thread.</p>
<p>For testing purposes, we increased the thread count by: <code>-Djava.util.concurrent.ForkJoinPool.common.parallelism=8</code></p>
<p>In a profile we can see that more threads are used, but it doesn&#x27;t change anything in the performance.</p>
<p><img decoding="async" loading="lazy" alt="profile-inc-fork-join" src="/zeebe-chaos/assets/images/profile-inc-fork-join-f567119ebd3858f2f5414425b7212228.png" width="2537" height="630" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="rest-gw-metrics-after-increaese-thread-pool" src="/zeebe-chaos/assets/images/rest-gw-metrics-after-increaese-thread-pool-f43f9d58ea3cc7f791d9e8a6aa85e32f.png" width="1907" height="761" class="img_ev3q"></p>
<p>The assumption was that we might not be able to handle the response in time with one thread, and this causes some contention also on the Gateway-Broker request-response cycle, but this is not the case.</p>
<p>We seem to spend time somewhere else or have a general resource contention issue. What we can see is that we have to work with more CPU throttling, then without REST API usage.</p>
<p><img decoding="async" loading="lazy" alt="rest-api-cpu-throttling.png" src="/zeebe-chaos/assets/images/rest-api-cpu-throttling-f4210b87ca53b8fcab9741cf12e76a60.png" width="2541" height="950" class="img_ev3q"></p>
<p>Increasing the CPU resolves the general performance problem, hinting even more that we might have some issues with threads competing with resources, etc.</p>
<p>In the following screenshot, you see the test with 6 CPUs per Camunda application.</p>
<p><img decoding="async" loading="lazy" alt="six-cpus" src="/zeebe-chaos/assets/images/six-cpus-fcbf79199e8adfeb2dd3f073ae0020fe.png" width="1896" height="933" class="img_ev3q"></p>
<p>Compared to the previous run with 2 CPUs per Camunda application, where it had to fight with a lot of CPU throttling. The request-response latency was five times higher on average.</p>
<p><img decoding="async" loading="lazy" alt="two-cpus" src="/zeebe-chaos/assets/images/two-cpus-a003a95c94a18bd11326736ffd163332.png" width="1901" height="925" class="img_ev3q"></p>
<p>We have to further investigate this based on this knowledge.</p>
<h1>Day 2: Profiling and Experimenting</h1>
<p>Yesterday I was taking profiles with 100s, to reduce the noise. Still, we can see that the filter chain is taking ~40% of the complete profile.</p>
<p><img decoding="async" loading="lazy" alt="jfr-10-minutes-filter-chain.png" src="/zeebe-chaos/assets/images/jfr-10-minutes-filter-chain-c3a4c75586a6c3ce023bad52889c5590.png" width="2540" height="665" class="img_ev3q"></p>
<p>When opening the JFR recording with JMC, we get some hints, related to context switches, CPU throttling (which we already know) and the inverted parallelism of GC (also mentioning high IO).</p>
<p><img decoding="async" loading="lazy" alt="locks-and-contention-context-switch.png" src="/zeebe-chaos/assets/images/locks-and-contention-context-switch-b3ebe8dc71f088fc16b4bf39f5defd2e.png" width="640" height="107" class="img_ev3q">
<img decoding="async" loading="lazy" alt="jfr-cpu-throttling-detection.png" src="/zeebe-chaos/assets/images/jfr-cpu-throttling-detection-5cfaa70f6b7e3b06c6ac745b8d317413.png" width="658" height="193" class="img_ev3q">
<img decoding="async" loading="lazy" alt="gc-ineffeciency-high-io.png" src="/zeebe-chaos/assets/images/gc-ineffeciency-high-io-8e03650f088004e21e0e549bf8cf985c.png" width="636" height="240" class="img_ev3q"></p>
<p>We have already seen in our metrics, for example, that we fight with high CPU throttling</p>
<p><img decoding="async" loading="lazy" alt="rest-base-cpu" src="/zeebe-chaos/assets/images/rest-base-cpu-c50ce9a1e9a0bb6715386fe81d11e5d8.png" width="2540" height="308" class="img_ev3q"></p>
<p>To better analyze (and circumvent that we have no tracing), I added some more metrics to understand where time is spent. Furthermore, I created a temporary dashboard to break down where time is spent.</p>
<p>When we look at the base with gRPC (taking our weekly benchmarks), we can see all latencies are low, and mostly under 5 ms.</p>
<p><img decoding="async" loading="lazy" alt="grpc-break-down.png" src="/zeebe-chaos/assets/images/grpc-break-down-5f9058a44a49e0fd5f38f87489eda3e7.png" width="2526" height="644" class="img_ev3q"></p>
<p>As soon as we enable the REST API, we can see the latencies go up. The most significant increase we see is in the job activations.</p>
<p><img decoding="async" loading="lazy" alt="rest-break-down" src="/zeebe-chaos/assets/images/rest-break-down-b8b23cb51fdc0b22bda557fd5f3d7836.png" width="2533" height="796" class="img_ev3q"></p>
<p>Fascinating is that the write to process latency, the time from acceptance by the CommandAPI until the processor processes this command, also increases.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="virtual-threads">Virtual threads<a href="#virtual-threads" class="hash-link" aria-label="Direct link to Virtual threads" title="Direct link to Virtual threads">​</a></h2>
<p>To remove some thoughts about potential IO and CPU contention, I experimented with virtual threads, which we can <a href="https://www.baeldung.com/spring-6-virtual-threads" target="_blank" rel="noopener noreferrer">easily enable for Spring</a>.</p>
<p>I set the following system property on the statefulset.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">-Dspring.threads.virtual.enabled=true</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Taking a new profile, we can see that all the http threads are gone, but still the filtering is prominent.</p>
<p><img decoding="async" loading="lazy" alt="jfr-virtual-threads.png" src="/zeebe-chaos/assets/images/jfr-virtual-threads-c97b70375d089d057d3edecdd0ad2ad5.png" width="2522" height="714" class="img_ev3q"></p>
<p>Checking our metrics break-down again we see there is no benefit here.</p>
<p><img decoding="async" loading="lazy" alt="virtual-threads-break-down.png" src="/zeebe-chaos/assets/images/virtual-threads-break-down-2d3df2e117f534eeb119f4614c377e90.png" width="2532" height="849" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="direct-handling">Direct handling<a href="#direct-handling" class="hash-link" aria-label="Direct link to Direct handling" title="Direct link to Direct handling">​</a></h2>
<p>Investigating the code basis, we saw several times <code>#handleAsync</code> without using an extra executor, causing to use of the ForkJoinPool (as mentioned the other day). One idea was to <a href="https://github.com/camunda/camunda/commit/265d7164f5384be8c443c30b20e432582df09c24" target="_blank" rel="noopener noreferrer">directly handle the future completions</a>, meaning the response handling, etc.</p>
<p>We didn&#x27;t observe any benefits with this.</p>
<p><img decoding="async" loading="lazy" alt="direct-handling-breakdown.png" src="/zeebe-chaos/assets/images/direct-handling-breakdown-f5af66c79a5d1064e3d1e603651d8797.png" width="2530" height="808" class="img_ev3q"></p>
<p>In the JFR recording, we can see that less Threads are used, but the Spring filter chain is also super prominent.
<img decoding="async" loading="lazy" alt="direct-handling-v2-profile-too-much-filtering.png" src="/zeebe-chaos/assets/images/direct-handling-v2-profile-too-much-filtering-d3a5da919b5220949e925bbb9042dbb4.png" width="2529" height="692" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spring-pathpattern-parser-for-mvc">Spring PathPattern parser for MVC<a href="#spring-pathpattern-parser-for-mvc" class="hash-link" aria-label="Direct link to Spring PathPattern parser for MVC" title="Direct link to Spring PathPattern parser for MVC">​</a></h2>
<p>At the end of the day I finally came to try the <code>PathPattern</code> parser. As mentioned the other day, it is recommended to use it over the legacy <code>AntPathMatcher</code>.</p>
<p>The migration was <a href="https://github.com/camunda/camunda/commit/357522d8355a624a1c07e1fb889561254b0305ba" target="_blank" rel="noopener noreferrer">rather simple</a>, we can replace the <code>spring.mvc.pathmatch.matching-strategy=ant_path_matcher</code> with
<code>spring.mvc.pathmatch.matching-strategy=path_pattern_parser</code>, we only had to fix some occurrences of regex combinations with <code>**</code>, as it is only allowed to have <code>**</code> at the end (no regex after).</p>
<p>See related branch <a href="https://github.com/camunda/camunda/commits/ck-pattern-path-parse/" target="_blank" rel="noopener noreferrer">ck-pattern-path-parse</a>.</p>
<p><img decoding="async" loading="lazy" alt="path-pattern-breakdown" src="/zeebe-chaos/assets/images/path-pattern-breakdown-9d9c15dc4c5d21d70472756d370ca533.png" width="2512" height="817" class="img_ev3q"></p>
<p>We were able to reduce the latencies by half, which also allowed us to bring back our throughput.</p>
<p><img decoding="async" loading="lazy" alt="path-pattern-general.png" src="/zeebe-chaos/assets/images/path-pattern-general-a9e20dc4a52c74b0488488aa78fa6e62.png" width="2528" height="667" class="img_ev3q"></p>
<p>I did a cross-check with the current SNAPSHOT, and weirdly the SNAPSHOT now behaved the same. I will run this for a while to see the results, as it might fail after certain period of time. As this might be also related to where the pods are scheduled (noisy neighbours etc.)</p>
<p><img decoding="async" loading="lazy" alt="rest-base-v2-breakdown.png" src="/zeebe-chaos/assets/images/rest-base-v2-breakdown-fe4ebdd50122d4c32e30517e8e66c2ce.png" width="2543" height="796" class="img_ev3q">
<img decoding="async" loading="lazy" alt="rest-base-v2-general.png" src="/zeebe-chaos/assets/images/rest-base-v2-general-b19aece8c6549aa08c4caef4ba45bf02.png" width="2527" height="679" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="combination-of-direct-handle-and-pathpattern">Combination of direct handle and PathPattern<a href="#combination-of-direct-handle-and-pathpattern" class="hash-link" aria-label="Direct link to Combination of direct handle and PathPattern" title="Direct link to Combination of direct handle and PathPattern">​</a></h2>
<p>On top of the above, I <a href="https://github.com/camunda/camunda/commits/ck-direct-handle/" target="_blank" rel="noopener noreferrer">combined the direct handling and PathPattern usage</a>, and this gave us the best results.</p>
<p>The latencies are only two times higher than gRPC vs before 5 times (and more).</p>
<p><img decoding="async" loading="lazy" alt="combination-of-all-breakdown.png" src="/zeebe-chaos/assets/images/combination-of-all-breakdown-3796dbecb61e2f55349b7e2b2fe9c58e.png" width="2502" height="907" class="img_ev3q"></p>
<p>The throttling of the CPU was reduced by half as well.</p>
<p><img decoding="async" loading="lazy" alt="combination-of-all-cpu.png" src="/zeebe-chaos/assets/images/combination-of-all-cpu-76b39f387018b794f171bb676b8b3832.png" width="2538" height="324" class="img_ev3q"></p>
<p>This gives a great stable throughput again.</p>
<p><img decoding="async" loading="lazy" alt="combination-of-all-general.png" src="/zeebe-chaos/assets/images/combination-of-all-general-285aa7b2d45b50d79f995ebffbbd697f.png" width="2531" height="684" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/06/30/Performance-of-REST-API">Performance of REST API</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-30T00:00:00.000Z">June 30, 2025</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>In today&#x27;s Chaos day we wanted to experiment with the new REST API (v2) as a replacement for our previous used gRPC API.</p>
<p>Per default, our load tests make use of the gRPC, but as we want to make REST API the default and release this fully with 8.8, we want to make sure to test this accordingly in regard to reliability.</p>
<p><strong>TL;DR;</strong> We observed severe performance regression when using the REST API, even when job streaming is in use by the job workers (over gRPC). Our client seems to have a higher memory consumption, which caused some instabilities in our tests as well. With the new API, we lack certain observability, which makes it harder to dive into certain details. We should investigate this further and find potential bottlenecks and improvements.</p>
<p><img decoding="async" loading="lazy" alt="general" src="/zeebe-chaos/assets/images/general-overview-a0c6c48be94b9a8aa8cbbd3b44830a67.png" width="1879" height="879" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Performance of REST API" href="/zeebe-chaos/2025/06/30/Performance-of-REST-API"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS">How does Zeebe behave with NFS</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-12T00:00:00.000Z">June 12, 2025</time> · <!-- -->13 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>This week, we (<a href="https://github.com/lenaschoenburg" target="_blank" rel="noopener noreferrer">Lena</a>, <a href="https://github.com/npepinpe" target="_blank" rel="noopener noreferrer">Nicolas</a>, <a href="https://github.com/romansmirnov" target="_blank" rel="noopener noreferrer">Roman</a>, and <a href="https://github.com/ChrisKujawa" target="_blank" rel="noopener noreferrer">I</a>) held a workshop where we looked into how Zeebe behaves with network file storage (NFS).</p>
<p>We ran several experiments with NFS and Zeebe, and messing around with connectivity.</p>
<p><strong>TL;DR;</strong> We were able to show that NFS can handle certain connectivity issues, just causing Zeebe to process slower. IF we completely lose the connection to the NFS server, several issues can arise, like IOExceptions on flush (where RAFT goes into inactive mode) or SIGBUS errors on reading (like replay), causing the JVM to crash.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about How does Zeebe behave with NFS" href="/zeebe-chaos/2025/06/12/How-does-Zeebe-behave-with-NFS"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment">Lower memory consumption of Camunda deployment</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-05T00:00:00.000Z">June 5, 2025</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>I&#x27;m back to finally do some load testing again.</p>
<p>In the past months, we have changed our architecture. This was to deploy instead all of our components as a separate deployment,
we now have one single statefulset. This statefulset is running our single Camunda standalone application,
combining all components together.</p>
<p><img decoding="async" loading="lazy" alt="simpler deployment" src="/zeebe-chaos/assets/images/simpler-deployment-a09568ddfb813f18a49e6b930b43cd02.png" width="1550" height="744" class="img_ev3q"></p>
<p>More details on this change we will share on a separate blog post. For simplicity, in our load tests (benchmark helm charts), we
combined all the resources we had split over multiple deployments together, see related PR <a href="https://github.com/camunda/zeebe-benchmark-helm/pull/213" target="_blank" rel="noopener noreferrer">#213</a>.</p>
<p>We are currently running our test with the following resources by default:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">Limits</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">cpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">     </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">memory</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">  12Gi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">Requests</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">cpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">      </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">memory</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">   6Gi</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In today&#x27;s Chaos day, I want to look into our resource consumption and whether we can reduce our used requests and limits.</p>
<p><strong>TL;DR;</strong> We have focused on experimenting with different memory resources, and were able to show that we can reduce the used memory by 75%, and our previous provisioned resources by more than 80% for our load tests.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Lower memory consumption of Camunda deployment" href="/zeebe-chaos/2025/06/05/Lower-memory-consumption-of-Camunda-deployment"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project">News from Camunda Exporter project</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-12-12T00:00:00.000Z">December 12, 2024</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>In this Chaos day, we want to verify the current state of the exporter project and run benchmarks with it. Comparing
with a previous version (v8.6.6) should give us a good hint on the current state and potential improvements.</p>
<p><strong>TL;DR;</strong> The latency of user data availability has improved due to our architecture change, but we still need to fix some bugs before our planned release of the Camunda Exporter. This experiment allows us to detect three new bugs, fixing this should allow us to make the system more stable.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about News from Camunda Exporter project" href="/zeebe-chaos/2024/12/12/News-from-Camunda-Exporter-project"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance">Impact of Camunda Exporter on processing performance</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-11-14T00:00:00.000Z">November 14, 2024</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>In our <a href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">last Chaos day</a> we experimented with the Camunda Exporter MVP. After our MVP we continued with Iteration 2, where we migrated the Archiver deployments and added a new Migration component (allows us to harmonize indices).</p>
<p><img decoding="async" loading="lazy" src="/zeebe-chaos/assets/images/it2-migration-1c91d1203aba0c2d454ce583eac8703c.png" width="1088" height="888" class="img_ev3q"></p>
<p>Additionally, <a href="https://github.com/camunda/zeebe-benchmark-helm/pull/202" target="_blank" rel="noopener noreferrer">some fixes and improvements</a> have been done to the realistic benchmarks that should allow us to better compare the general performance with a realistic good performing benchmark.</p>
<p>Actually, this is what we want to explore and experiment with today.</p>
<ul>
<li>Does the Camunda Exporter (since the last benchmark) impact performance of the overall system?<!-- -->
<ul>
<li>If so how?</li>
</ul>
</li>
<li>How can we potentially mitigate this?</li>
</ul>
<p><strong>TL;DR;</strong> Today&#x27;s, results showed that enabling the Camunda Exporter causes a 25% processing throughput drop. We identified the CPU as a bottleneck. It seems to be mitigated by either adjusting the CPU requests or removing the ES exporter. With these results, we are equipped to make further investigations and decisions.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Impact of Camunda Exporter on processing performance" href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">Camunda Exporter MVP</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-10-24T00:00:00.000Z">October 24, 2024</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>After a long pause, I come back with an interesting topic to share and experiment with. Right now we are re-architecture
Camunda 8. One important part (which I&#x27;m contributing to) is to get rid of Webapps Importer/Archivers and move
data aggregation closer to the engine (inside a Zeebe Exporter).</p>
<p>Today, I want to experiment with the first increment/iteration of our so-called MVP. The MVP targets green field installations where you simply deploy Camunda (with a new Camunda Exporter enabled) without Importers.</p>
<p><strong>TL;DR;</strong> All our experiments were successful. The MVP is a success, and we are looking forward to further improvements and additions. Next stop Iteration 2: Adding Archiving historic data and preparing for data migration (and polishing MVP).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="camunda-exporter">Camunda Exporter<a href="#camunda-exporter" class="hash-link" aria-label="Direct link to Camunda Exporter" title="Direct link to Camunda Exporter">​</a></h2>
<p>The <a href="https://github.com/camunda/product-hub/issues/2128" target="_blank" rel="noopener noreferrer">Camunda Exporter project</a> deserves a complete own blog post, here is just a short summary.</p>
<p>Our current Camunda architecture looks something like this (simplified).</p>
<p><img decoding="async" loading="lazy" alt="current" src="/zeebe-chaos/assets/images/current-miro-659b193b670b1b604ebb32ff30b067a4.png" width="1096" height="885" class="img_ev3q"></p>
<p>It has certain challenges, like:</p>
<ul>
<li>Space: duplication of data in ES</li>
<li>Maintenance: duplication of importer and archiver logic</li>
<li>Performance: Round trip (delay) of data visible to the user</li>
<li>Complexity: installation and operational complexity (we need separate pods to deploy)</li>
<li>Scalability: The Importer is not scalable in the same way as Zeebe or brokers (and workload) are.</li>
</ul>
<p>These challenges we obviously wanted to overcome and the plan (as mentioned earlier) is to get rid of the need of separate importers and archivers (and in general to have separate application; but this is a different topic).</p>
<p>The plan for this project looks something like this:</p>
<p><img decoding="async" loading="lazy" alt="plan" src="/zeebe-chaos/assets/images/how-brown-field-929f9a23e6dfee9ede15e76b1a134fdc.png" width="1228" height="904" class="img_ev3q"></p>
<p>We plan to:</p>
<ol>
<li>Harmonize the existing indices stored in Elasticsearch/Opensearch<!-- -->
<ul>
<li>Space: Reduce the unnecessary data duplication</li>
</ul>
</li>
<li>Move importer and archiver logic into a new Camunda exporter<!-- -->
<ul>
<li>Performance: This should allow us to reduce one additional hop (as we don&#x27;t need to use ES/OS as a queue)</li>
<li>Maintenance: Indices and business logic is maintained in one place</li>
<li>Scalability: With this approach, we can scale with partitions, as Camunda Exporters are executed for each partition separately (soon partition scaling will be introduced)</li>
<li>Complexity: The Camunda Exporter will be built-in and shipped with Zeebe/Camunda 8. No additional pod/application is needed.</li>
</ul>
</li>
</ol>
<p>Note: Optimize is right now out of scope (due to time), but will later be part of this as well.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mvp">MVP<a href="#mvp" class="hash-link" aria-label="Direct link to MVP" title="Direct link to MVP">​</a></h3>
<p>After we know what we want to achieve what is the Minimum viable product (MVP)?</p>
<p>We have divided the Camunda Exporter in 3-4 iterations. You can see and read more about this <a href="https://github.com/camunda/issues/issues/803" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p>The first iteration contains the MVP (the first breakthrough). Providing the Camunda Exporter with the basic functionality ported from the Operate and Tasklist importers, writing into harmonized indices.</p>
<p>The MVP is targeting green field installations (clean installations) of Camunda 8 with Camunda Exporter without running the old Importer (no data migration yet),</p>
<p><img decoding="async" loading="lazy" alt="mvp" src="/zeebe-chaos/assets/images/it1-mvp-421ca897b91c0d03c1d77adde73b48a7.png" width="1069" height="870" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Camunda Exporter MVP" href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark">Optimizing cluster sizing using a real world benchmark</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-10-14T00:00:00.000Z">October 14, 2024</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/rodrigo-lourenco-lopes.png" alt="Rodrigo Lopes"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Rodrigo Lopes</span></a></div><small class="authorTitle_nd0D" title="Associate Software Engineer @ Zeebe">Associate Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>Our first goal of this experiment is to use a benchmarks to
derive new optimized cluster configuration that can handle
at least 100 tasks per second, while maintaining low backpressure and low latency.</p>
<p>For our experiment, we use a newly defined realistic benchmark (with a more complex process model). More about this in a separate blog post.</p>
<p>The second goal is to scale out optimized cluster configuration
resources linearly and see if the performance scales accordingly.</p>
<p><strong>TL;DR;</strong></p>
<p>We used a realistic benchmark to derive a new
cluster configuration based on previous requirements.</p>
<p>When we scale this base configuration linearly we see that the performance
increases almost linearly as well, while maintaining low
backpressure and low latency.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="chaos-experiment">Chaos Experiment<a href="#chaos-experiment" class="hash-link" aria-label="Direct link to Chaos Experiment" title="Direct link to Chaos Experiment">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="expected">Expected<a href="#expected" class="hash-link" aria-label="Direct link to Expected" title="Direct link to Expected">​</a></h3>
<p>We expect that we can find a cluster configuration that can handle at 100
tasks second to be significantly reduced in resources in relation to our
smaller clusters (G3-S HA Plan) since these can process significantly above
our initial target.</p>
<p>We also expect that we can scale this base configuration linearly, and that
the processing tasks rate to grow initially a bit faster than linearly due to
the lower relative overhead, and if we keep expanding further to flatten due
to the partition count being a bottleneck.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="actual">Actual<a href="#actual" class="hash-link" aria-label="Direct link to Actual" title="Direct link to Actual">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="minimal-requirements-for-our-cluster">Minimal Requirements for our Cluster<a href="#minimal-requirements-for-our-cluster" class="hash-link" aria-label="Direct link to Minimal Requirements for our Cluster" title="Direct link to Minimal Requirements for our Cluster">​</a></h4>
<p>Based on known customer usage, and our own previous experiments, we
determined that the new cluster would need to create and complete a
baseline of 100 tasks per second, or about 8.6 million tasks per day.</p>
<p>Other metrics that we want to preserve and keep track are the backpressure
to preserve user experience, guarantee that exporting speed can keep up
with the processing speed, write-to-import latency which tells us how long
it takes for a record to be written to being imported by our other
apps such as the operator.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="reverse-engineering-the-cluster-configuration">Reverse Engineering the Cluster Configuration<a href="#reverse-engineering-the-cluster-configuration" class="hash-link" aria-label="Direct link to Reverse Engineering the Cluster Configuration" title="Direct link to Reverse Engineering the Cluster Configuration">​</a></h4>
<p>For our new configurations the only resources that we are going to change
are the ones relevant to the factors described above. These are the
resources allocated to our zeebe-brokers, gateway and elasticSearch.</p>
<p>Our starting point in resources was the configuration for our G3-S HA Plan
as this already had the capability to significantly outperform the current
goal of 100 tasks per second.</p>
<p>The next step was to deploy our realistic benchmark, with a payload of 5
customer disputes per instance and start 7 instances per second, this
generated approximately 120 tasks per second (some buffer was added to guarantee performance).</p>
<p>After this we reduced the resources iteratively until we saw any increase
in backpressure, given that no there was no backlog of records, and no
significant increase in the write to import latency.</p>
<p>The results for our new cluster are specified bellow in the tables, where
our starting cluster configuration is the G3-S HA Plan and the new
configuration cluster is the G3 - BasePackage HA.</p>
<table><thead><tr><th>G3-S HA</th><th>CPU Limit</th><th>Memory Limit in GB</th></tr></thead><tbody><tr><td>operate</td><td>2</td><td>2</td></tr><tr><td>operate.elasticsearch</td><td>6</td><td>6</td></tr><tr><td>optimize</td><td>2</td><td>2</td></tr><tr><td>tasklist</td><td>2</td><td>2</td></tr><tr><td>zeebe.broker</td><td>2.88</td><td>12</td></tr><tr><td>zeebe.gateway</td><td>0.9</td><td>0.8</td></tr><tr><td><strong>TOTAL</strong></td><td><strong>15.78</strong></td><td><strong>24.8</strong></td></tr></tbody></table>
<table><thead><tr><th>G3 - BasePackage HA</th><th>CPU Limit</th><th>Memory Limit in GB</th></tr></thead><tbody><tr><td>operate</td><td>1</td><td>1</td></tr><tr><td>operate.elasticsearch</td><td>3</td><td>4.5</td></tr><tr><td>optimize</td><td>1</td><td>1.6</td></tr><tr><td>tasklist</td><td>1</td><td>1</td></tr><tr><td>zeebe.broker</td><td>1.5</td><td>4.5</td></tr><tr><td>zeebe.gateway</td><td>0.6</td><td>1</td></tr><tr><td><strong>TOTAL</strong></td><td><strong>8.1</strong></td><td><strong>13.6</strong></td></tr></tbody></table>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="reduction-in-resources-for-our-optimized-cluster">Reduction in Resources for our Optimized Cluster<a href="#reduction-in-resources-for-our-optimized-cluster" class="hash-link" aria-label="Direct link to Reduction in Resources for our Optimized Cluster" title="Direct link to Reduction in Resources for our Optimized Cluster">​</a></h5>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:right">CPU Reduction (%)</th><th style="text-align:right">Memory Reduction (%)</th></tr></thead><tbody><tr><td style="text-align:left">zeebe.broker</td><td style="text-align:right">47.92</td><td style="text-align:right">62.5</td></tr><tr><td style="text-align:left">zeebe.gateway</td><td style="text-align:right">33.33</td><td style="text-align:right">-25.0</td></tr><tr><td style="text-align:left">operate.elasticsearch</td><td style="text-align:right">50.00</td><td style="text-align:right">25.0</td></tr></tbody></table>
<p>Total cluster reduction:</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:right">G3-S HA</th><th style="text-align:right">G3 - BasePackage HA</th><th style="text-align:right">Reduction (%)</th></tr></thead><tbody><tr><td style="text-align:left">CPU Limits</td><td style="text-align:right">15.78</td><td style="text-align:right">8.1</td><td style="text-align:right">49</td></tr><tr><td style="text-align:left">Memory Limits</td><td style="text-align:right">24.8</td><td style="text-align:right">13.6</td><td style="text-align:right">45</td></tr></tbody></table>
<p>The process of reducing the hardware requirements was donne initially by
scaling down the resources of the zeebe-broker, gateway and elasticSearch.
The other components were left untouched, as they had no impact in our key
metrics, and were scaled down later in separate experiences to maintain
user experience.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-out-the-cluster">Scaling out the Cluster<a href="#scaling-out-the-cluster" class="hash-link" aria-label="Direct link to Scaling out the Cluster" title="Direct link to Scaling out the Cluster">​</a></h4>
<p>Now for the scaling procedure we intend to see if we can linearly increase
the allocated resources and having a corresponding performance increase,
while keeping the backpressure low, low latency, and user experience.</p>
<p>For this we started with the G3 - BasePackage HA configuration and
incremented the load again until we saw any increase in backpressure,
capture our key metrics and repeated the process for the cluster
configuration resources respectively multiplied by 2x, 3x, and 4x.</p>
<p>This means that the resources allocated for our clusters were:</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:right">Base 1x</th><th style="text-align:right">Base 2x</th><th style="text-align:right">Base 3x</th><th style="text-align:right">Base 4x</th></tr></thead><tbody><tr><td style="text-align:left">CPU Limits</td><td style="text-align:right">8.7</td><td style="text-align:right">17.4</td><td style="text-align:right">26.1</td><td style="text-align:right">34.8</td></tr><tr><td style="text-align:left">Memory Limits</td><td style="text-align:right">14.9</td><td style="text-align:right">29.8</td><td style="text-align:right">44.7</td><td style="text-align:right">59.6</td></tr></tbody></table>
<p>The results in the table bellow show the performance of our several cluster
configurations:</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:right">Base 1x</th><th style="text-align:right">Base 2x</th><th style="text-align:right">Base 3x</th><th style="text-align:right">Base 4x</th></tr></thead><tbody><tr><td style="text-align:left">Process Instances/s</td><td style="text-align:right">7</td><td style="text-align:right">12</td><td style="text-align:right">23</td><td style="text-align:right">27</td></tr><tr><td style="text-align:left">Tasks/s</td><td style="text-align:right">125</td><td style="text-align:right">217</td><td style="text-align:right">414</td><td style="text-align:right">486</td></tr><tr><td style="text-align:left">Average Backpressure</td><td style="text-align:right">2%</td><td style="text-align:right">2%</td><td style="text-align:right">3%</td><td style="text-align:right">6%</td></tr><tr><td style="text-align:left">Write-to-Import Latency</td><td style="text-align:right">90s</td><td style="text-align:right">120s</td><td style="text-align:right">150s</td><td style="text-align:right">390s</td></tr><tr><td style="text-align:left">Write-to-Process Latency</td><td style="text-align:right">140ms</td><td style="text-align:right">89ms</td><td style="text-align:right">200ms</td><td style="text-align:right">160ms</td></tr><tr><td style="text-align:left">Records Processed Rate</td><td style="text-align:right">2500</td><td style="text-align:right">4700</td><td style="text-align:right">7800</td><td style="text-align:right">11400</td></tr><tr><td style="text-align:left">Records Exported Rate</td><td style="text-align:right">2100</td><td style="text-align:right">3900</td><td style="text-align:right">6500</td><td style="text-align:right">9200</td></tr></tbody></table>
<p>This first observations is that the performance scales particularly well by
just adding more resources to the cluster, particularly for a linear
increase of the resources the performance as measured by tasks completed
increases slightly less than linearly (comparing the 1x and 4x task/s we
get 388% the initial rate).</p>
<p>This a very good result as it means that we can scale our system linearly
(at least initially) to handle the expected increase in loads.</p>
<p>Importantly, the backpressure is kept low, and the write-to-import latency
only increases significantly if we leave the cluster running at max rate
for long periods of time. For slightly lower rates the write-to-import
latency is kept in the single digits of seconds or lower tens. This might
imply that a these sustained max rates, the amount records generated starts
to be too much for either ElasticSearch or our web apps that import these
records to handle. Some further investigation could be done here to
investigate the bottleneck.</p>
<p>Another metric also relevant but not shown in this table is the backlog of
records not exported, which kept at almost null through all the experiments
conducted.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-found">Bugs found<a href="#bugs-found" class="hash-link" aria-label="Direct link to Bugs found" title="Direct link to Bugs found">​</a></h3>
<p>During the initial tests, we had several OOM errors in the gateways pods.
After some investigation, we found that this was exclusive to the Camunda 8.
6.0 version, which consumes more memory in the gateway than the previous
versions. This explains why the gateway memory limits were the only
resource that was increased in the new reduced cluster configuration.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency">Improve Operate import latency</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-08-19T00:00:00.000Z">August 19, 2024</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><a href="/zeebe-chaos/2024/08/16/Operate-load-handling">In our last Chaos Day</a> we experimented with Operate and different load (Zeebe throughput). We observed that a higher load caused a lower import latency in Operate. The conclusion was that it might be related to Zeebe&#x27;s exporting configuration, which is affected by a higher load.</p>
<p>In today&#x27;s chaos day we want to verify how different export and import configurations can affect the importing latency.</p>
<p><strong>TL;DR;</strong> We were able to decrease the import latency by ~35% (from 5.7 to 3.7 seconds), by simply reducing the <code>bulk.delay</code> configuration. This worked on low load and even higher load, without significant issues.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Improve Operate import latency" href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/08/16/Operate-load-handling">Operate load handling</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-08-16T00:00:00.000Z">August 16, 2024</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/ChrisKujawa.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>🎉<!-- --> Happy to announce that we are broadening the scope of our Chaos days, to look holistically at the whole Camunda Platform, starting today.
In the past Chaos days we often had a close look (or concentrated mostly) at Zeebe performance and stability.</p>
<p>Today, we will look at the Operate import performance and how Zeebe processing throughput might affect (or not?) the throughput and latency of the Operate import. Is it decoupled as we thought?</p>
<p>The import time is an important metric, representing the time until data from Zeebe processing is
visible to the User (excluding Elasticsearch&#x27;s indexing). It is measured from when the record is written to the log, by the Zeebe processor, until Operate reads/imports it from Elasticsearch and converts it into its data model. We got much feedback (and experienced this on our own) that
Operate is often lagging behind or is too slow, and of course we want to tackle and investigate this further.</p>
<p>The results from this Chaos day and related benchmarks should allow us to better understand how the current importing
of Operate performs, and what its affects. Likely it will be a series of posts to investigate this further. In general,
the data will give us some guidance and comparable numbers for the future to improve the importing time. See also related GitHub issue <a href="https://github.com/camunda/camunda/issues/16912" target="_blank" rel="noopener noreferrer">#16912</a> which targets to improve such.</p>
<p><strong>TL;DR;</strong> We were not able to show that Zeebe throughput doesn&#x27;t affect Operate importing time. We have seen that Operate can be positively affected by the throughput of Zeebe. Surprisingly, Operate was faster to
import if Zeebe produced more data (with a higher throughput). One explanation of this might be that Operate was then less idle.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Operate load handling" href="/zeebe-chaos/2024/08/16/Operate-load-handling"><b>Read more</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/zeebe-chaos/page/2"><div class="pagination-nav__label">Older entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/zeebe" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://forum.camunda.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/Camunda" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/camunda/zeebe-chaos/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Zeebe Chaos. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>