<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.3">
<title data-rh="true">Blog | Zeebe Chaos</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://zeebe-io.github.io/zeebe-chaos/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | Zeebe Chaos"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/zeebe-chaos/img/zeebe-logo.png"><link data-rh="true" rel="canonical" href="https://zeebe-io.github.io/zeebe-chaos/"><link data-rh="true" rel="alternate" href="https://zeebe-io.github.io/zeebe-chaos/" hreflang="en"><link data-rh="true" rel="alternate" href="https://zeebe-io.github.io/zeebe-chaos/" hreflang="x-default"><script data-rh="true">function insertBanner(){var n=document.createElement("div");n.id="__docusaurus-base-url-issue-banner-container";n.innerHTML='\n<div id="__docusaurus-base-url-issue-banner" style="border: thick solid red; background-color: rgb(255, 230, 179); margin: 20px; padding: 20px; font-size: 20px;">\n   <p style="font-weight: bold; font-size: 30px;">Your Docusaurus site did not load properly.</p>\n   <p>A very common reason is a wrong site <a href="https://docusaurus.io/docs/docusaurus.config.js/#baseUrl" style="font-weight: bold;">baseUrl configuration</a>.</p>\n   <p>Current configured baseUrl = <span style="font-weight: bold; color: red;">/zeebe-chaos/</span> </p>\n   <p>We suggest trying baseUrl = <span id="__docusaurus-base-url-issue-banner-suggestion-container" style="font-weight: bold; color: green;"></span></p>\n</div>\n',document.body.prepend(n);var e=document.getElementById("__docusaurus-base-url-issue-banner-suggestion-container"),s=window.location.pathname,o="/"===s.substr(-1)?s:s+"/";e.innerHTML=o}document.addEventListener("DOMContentLoaded",(function(){void 0===window.docusaurus&&insertBanner()}))</script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"Blog","@id":"https://zeebe-io.github.io/zeebe-chaos/","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/","headline":"Blog","description":"Blog","blogPost":[{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance","url":"https://zeebe-io.github.io/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance","headline":"Impact of Camunda Exporter on processing performance","name":"Impact of Camunda Exporter on processing performance","description":"In our last Chaos day we experimented with the Camunda Exporter MVP. After our MVP we continued with Iteration 2, where we migrated the Archiver deployments and added a new Migration component (allows us to harmonize indices).","datePublished":"2024-11-14T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","image":"https://github.com/zelldon.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP","url":"https://zeebe-io.github.io/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP","headline":"Camunda Exporter MVP","name":"Camunda Exporter MVP","description":"After a long pause, I come back with an interesting topic to share and experiment with. Right now we are re-architecture","datePublished":"2024-10-24T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","image":"https://github.com/zelldon.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark","url":"https://zeebe-io.github.io/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark","headline":"Optimizing cluster sizing using a real world benchmark","name":"Optimizing cluster sizing using a real world benchmark","description":"Our first goal of this experiment is to use a benchmarks to","datePublished":"2024-10-14T00:00:00.000Z","author":{"@type":"Person","name":"Rodrigo Lopes","description":"Associate Software Engineer @ Zeebe","url":"https://github.com/rodrigo-lourenco-lopes","image":"https://github.com/rodrigo-lourenco-lopes.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2024/08/19/Operate-improve-import-latency","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2024/08/19/Operate-improve-import-latency","url":"https://zeebe-io.github.io/zeebe-chaos/2024/08/19/Operate-improve-import-latency","headline":"Improve Operate import latency","name":"Improve Operate import latency","description":"In our last Chaos Day we experimented with Operate and different load (Zeebe throughput). We observed that a higher load caused a lower import latency in Operate. The conclusion was that it might be related to Zeebe's exporting configuration, which is affected by a higher load.","datePublished":"2024-08-19T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","image":"https://github.com/zelldon.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2024/08/16/Operate-load-handling","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2024/08/16/Operate-load-handling","url":"https://zeebe-io.github.io/zeebe-chaos/2024/08/16/Operate-load-handling","headline":"Operate load handling","name":"Operate load handling","description":"Happy to announce that we are broadening the scope of our Chaos days, to look holistically at the whole Camunda Platform, starting today.","datePublished":"2024-08-16T00:00:00.000Z","author":{"@type":"Person","name":"Christopher Kujawa","description":"Chaos Engineer @ Zeebe","url":"https://github.com/zelldon","image":"https://github.com/zelldon.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting","url":"https://zeebe-io.github.io/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting","headline":"Using flow control to handle bottleneck on exporting","name":"Using flow control to handle bottleneck on exporting","description":"Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).","datePublished":"2024-07-25T00:00:00.000Z","author":{"@type":"Person","name":"Rodrigo Lopes","description":"Associate Software Engineer @ Zeebe","url":"https://github.com/rodrigo-lourenco-lopes","image":"https://github.com/rodrigo-lourenco-lopes.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops","url":"https://zeebe-io.github.io/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops","headline":"Using flow control to handle uncontrolled process loops","name":"Using flow control to handle uncontrolled process loops","description":"Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).","datePublished":"2024-07-25T00:00:00.000Z","author":{"@type":"Person","name":"Rodrigo Lopes","description":"Associate Software Engineer @ Zeebe","url":"https://github.com/rodrigo-lourenco-lopes","image":"https://github.com/rodrigo-lourenco-lopes.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2024/01/19/Job-Activation-Latency","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2024/01/19/Job-Activation-Latency","url":"https://zeebe-io.github.io/zeebe-chaos/2024/01/19/Job-Activation-Latency","headline":"Reducing the job activation delay","name":"Reducing the job activation delay","description":"With the addition of end-to-end job streaming capabilities in Zeebe, we wanted to measure the improvements in job activation latency:","datePublished":"2024-01-19T00:00:00.000Z","author":{"@type":"Person","name":"Nicolas Pepin-Perreault","description":"Senior Software Engineer @ Zeebe","url":"https://github.com/npepinpe","image":"https://github.com/npepinpe.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2023/12/20/Broker-scaling-performance","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2023/12/20/Broker-scaling-performance","url":"https://zeebe-io.github.io/zeebe-chaos/2023/12/20/Broker-scaling-performance","headline":"Broker Scaling and Performance","name":"Broker Scaling and Performance","description":"With Zeebe now supporting the addition and removal of brokers to a running cluster, we wanted to test three things:","datePublished":"2023-12-20T00:00:00.000Z","author":[{"@type":"Person","name":"Lena Schönburg","description":"Senior Software Engineer @ Zeebe","url":"https://github.com/lenaschoenburg","image":"https://github.com/lenaschoenburg.png"},{"@type":"Person","name":"Deepthi Akkoorath","description":"Senior Software Engineer @ Zeebe","url":"https://github.com/deepthidevaki","image":"https://github.com/deepthidevaki.png"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://zeebe-io.github.io/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss","mainEntityOfPage":"https://zeebe-io.github.io/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss","url":"https://zeebe-io.github.io/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss","headline":"Dynamic Scaling with Dataloss","name":"Dynamic Scaling with Dataloss","description":"We continue our previous experiments with dynamically scaling by now also testing whether","datePublished":"2023-12-19T00:00:00.000Z","author":{"@type":"Person","name":"Lena Schönburg","description":"Senior Software Engineer @ Zeebe","url":"https://github.com/lenaschoenburg","image":"https://github.com/lenaschoenburg.png"},"keywords":[]}]}</script><link rel="alternate" type="application/rss+xml" href="/zeebe-chaos/rss.xml" title="Zeebe Chaos RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zeebe-chaos/atom.xml" title="Zeebe Chaos Atom Feed"><link rel="stylesheet" href="/zeebe-chaos/assets/css/styles.09e3c82e.css">
<script src="/zeebe-chaos/assets/js/runtime~main.031f6a74.js" defer="defer"></script>
<script src="/zeebe-chaos/assets/js/main.be6d306f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zeebe-chaos/"><div class="navbar__logo"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/zeebe-chaos/img/zeebe-logo.png" alt="Zeebe" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Zeebe Chaos</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zeebe-chaos/">Chaos Summaries</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/zeebe-io/zeebe-chaos" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance">Impact of Camunda Exporter on processing performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">Camunda Exporter MVP</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark">Optimizing cluster sizing using a real world benchmark</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency">Improve Operate import latency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/08/16/Operate-load-handling">Operate load handling</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting">Using flow control to handle bottleneck on exporting</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops">Using flow control to handle uncontrolled process loops</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2024/01/19/Job-Activation-Latency">Reducing the job activation delay</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/20/Broker-scaling-performance">Broker Scaling and Performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss">Dynamic Scaling with Dataloss</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers">Dynamically scaling brokers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/12/06/Job-Push-resiliency">Job push resiliency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/30/Job-push-overloading">Job push overloading</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/11/07/Hot-backups-impact-on-processing">Hot backups impact on processing</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance">Using Large Multi-Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle">Continuing SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/05/15/SST-Partitioning-toggle">SST Partitioning toggle</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/04/06/gateway-termination">Gateway Termination</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2023/02/23/Recursive-call-activity">Recursive call activity</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2022</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/31/Message-Correlation-after-Network-Partition">Message Correlation after Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/08/02/deployment-distribution">Bring Deployment distribution experiment back</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/15/Standalone-Gateway-in-CCSaaS">Standalone Gateway in CCSaaS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/02/01/High-Snapshot-Frequency">High Snapshot Frequency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2022/01/19/big-variables">Handling of Big Variables</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2021</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/24/Worker-count-should-not-impact-performance">Worker count should not impact performance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/11/11/Not-produce-duplicate-Keys">Not produce duplicate Keys</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/29/Throughput-on-big-state">Throughput on big state</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/10/05/recovery-time">Recovery (Fail Over) time</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/09/23/Old-Clients">Old-Clients</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/07/06/Slow-Network">Slow Network</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/06/08/Full-Disk">Full Disk Recovery</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/05/25/Reset-Clock">Time travel Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/29/Corrupted-Snapshot">Corrupted Snapshot Experiment Investigation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/04/03/bpmn-meets-chaos-engineering">BPMN meets Chaos Engineering</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/30/set-file-immutable">Set file immutable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/23/camunda-cloud-network-partition">Camunda Cloud network partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/03/09/cont-workflow-instance">Fault-tolerant processing of process instances</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/02/23/automate-deployments-dist">Automating Deployment Distribution Chaos Experiment</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/26/deployments">Deployment Distribution</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/19/network-partition">Network partitions</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2021/01/07/disconnect-leader-and-follower">Disconnect Leader and one Follower</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2020</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/24/message-correlation-after-failover">Message Correlation after Failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/11/job-timeouts">Many Job Timeouts</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/11/03/investigate-failing-tests">Investigate failing Chaos Tests</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/20/non-graceful-shutdown">Non-graceful Shutdown Broker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/27/standalone-gw-memory">Gateway memory consumption</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/13/multiple-leader-changes">Multiple Leader Changes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/10/06/toxi-proxy">Play around with ToxiProxy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/20/experiment-with-camunda-cloud">Experiment with Camunda Cloud</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/08/06/low-load">Experiment with Low Load</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/30/experiment-without-exporters">Experiment without Exporters</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/16/big-multi-instance">Big Multi Instance</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/09/timer-and-huge-variables">Experiment with Timers and Huge Variables</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/07/02/extract-k8-resources">Extract K8 resources from namespace</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/25/gateway-network-partition">Gateway Network Partition</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/18/correlate-message-after-failover">Correlate Message after failover</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/11/high-cpu-gateway">High CPU load on Standalone Gateway</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zeebe-chaos/2020/06/04/first-chaos-day">First Chaos Day!</a></li></ul></div></nav></aside><main class="col col--7"><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance">Impact of Camunda Exporter on processing performance</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-11-14T00:00:00.000Z">November 14, 2024</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/zelldon.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>In our <a href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">last Chaos day</a> we experimented with the Camunda Exporter MVP. After our MVP we continued with Iteration 2, where we migrated the Archiver deployments and added a new Migration component (allows us to harmonize indices).</p>
<p><img decoding="async" loading="lazy" src="/zeebe-chaos/assets/images/it2-migration-1c91d1203aba0c2d454ce583eac8703c.png" width="1088" height="888" class="img_ev3q"></p>
<p>Additionally, <a href="https://github.com/zeebe-io/benchmark-helm/pull/202" target="_blank" rel="noopener noreferrer">some fixes and improvements</a> have been done to the realistic benchmarks that should allow us to better compare the general performance with a realistic good performing benchmark.</p>
<p>Actually, this is what we want to explore and experiment with today.</p>
<ul>
<li>Does the Camunda Exporter (since the last benchmark) impact performance of the overall system?<!-- -->
<ul>
<li>If so how?</li>
</ul>
</li>
<li>How can we potentially mitigate this?</li>
</ul>
<p><strong>TL;DR;</strong> Today&#x27;s, results showed that enabling the Camunda Exporter causes a 25% processing throughput drop. We identified the CPU as a bottleneck. It seems to be mitigated by either adjusting the CPU requests or removing the ES exporter. With these results, we are equipped to make further investigations and decisions.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Impact of Camunda Exporter on processing performance" href="/zeebe-chaos/2024/11/14/Impact-of-Camunda-Exporter-on-processing-performance"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP">Camunda Exporter MVP</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-10-24T00:00:00.000Z">October 24, 2024</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/zelldon.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>After a long pause, I come back with an interesting topic to share and experiment with. Right now we are re-architecture
Camunda 8. One important part (which I&#x27;m contributing to) is to get rid of Webapps Importer/Archivers and move
data aggregation closer to the engine (inside a Zeebe Exporter).</p>
<p>Today, I want to experiment with the first increment/iteration of our so-called MVP. The MVP targets green field installations where you simply deploy Camunda (with a new Camunda Exporter enabled) without Importers.</p>
<p><strong>TL;DR;</strong> All our experiments were successful. The MVP is a success, and we are looking forward to further improvements and additions. Next stop Iteration 2: Adding Archiving historic data and preparing for data migration (and polishing MVP).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="camunda-exporter">Camunda Exporter<a href="#camunda-exporter" class="hash-link" aria-label="Direct link to Camunda Exporter" title="Direct link to Camunda Exporter">​</a></h2>
<p>The <a href="https://github.com/camunda/product-hub/issues/2128" target="_blank" rel="noopener noreferrer">Camunda Exporter project</a> deserves a complete own blog post, here is just a short summary.</p>
<p>Our current Camunda architecture looks something like this (simplified).</p>
<p><img decoding="async" loading="lazy" alt="current" src="/zeebe-chaos/assets/images/current-miro-659b193b670b1b604ebb32ff30b067a4.png" width="1096" height="885" class="img_ev3q"></p>
<p>It has certain challenges, like:</p>
<ul>
<li>Space: duplication of data in ES</li>
<li>Maintenance: duplication of importer and archiver logic</li>
<li>Performance: Round trip (delay) of data visible to the user</li>
<li>Complexity: installation and operational complexity (we need separate pods to deploy)</li>
<li>Scalability: The Importer is not scalable in the same way as Zeebe or brokers (and workload) are.</li>
</ul>
<p>These challenges we obviously wanted to overcome and the plan (as mentioned earlier) is to get rid of the need of separate importers and archivers (and in general to have separate application; but this is a different topic).</p>
<p>The plan for this project looks something like this:</p>
<p><img decoding="async" loading="lazy" alt="plan" src="/zeebe-chaos/assets/images/how-brown-field-929f9a23e6dfee9ede15e76b1a134fdc.png" width="1228" height="904" class="img_ev3q"></p>
<p>We plan to:</p>
<ol>
<li>Harmonize the existing indices stored in Elasticsearch/Opensearch<!-- -->
<ul>
<li>Space: Reduce the unnecessary data duplication</li>
</ul>
</li>
<li>Move importer and archiver logic into a new Camunda exporter<!-- -->
<ul>
<li>Performance: This should allow us to reduce one additional hop (as we don&#x27;t need to use ES/OS as a queue)</li>
<li>Maintenance: Indices and business logic is maintained in one place</li>
<li>Scalability: With this approach, we can scale with partitions, as Camunda Exporters are executed for each partition separately (soon partition scaling will be introduced)</li>
<li>Complexity: The Camunda Exporter will be built-in and shipped with Zeebe/Camunda 8. No additional pod/application is needed.</li>
</ul>
</li>
</ol>
<p>Note: Optimize is right now out of scope (due to time), but will later be part of this as well.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mvp">MVP<a href="#mvp" class="hash-link" aria-label="Direct link to MVP" title="Direct link to MVP">​</a></h3>
<p>After we know what we want to achieve what is the Minimum viable product (MVP)?</p>
<p>We have divided the Camunda Exporter in 3-4 iterations. You can see and read more about this <a href="https://github.com/camunda/issues/issues/803" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p>The first iteration contains the MVP (the first breakthrough). Providing the Camunda Exporter with the basic functionality ported from the Operate and Tasklist importers, writing into harmonized indices.</p>
<p>The MVP is targeting green field installations (clean installations) of Camunda 8 with Camunda Exporter without running the old Importer (no data migration yet),</p>
<p><img decoding="async" loading="lazy" alt="mvp" src="/zeebe-chaos/assets/images/it1-mvp-421ca897b91c0d03c1d77adde73b48a7.png" width="1069" height="870" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Camunda Exporter MVP" href="/zeebe-chaos/2024/10/24/Camunda-Exporter-MVP"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/10/14/Optimizing-cluster-sizing-using-a-real-world-benchmark">Optimizing cluster sizing using a real world benchmark</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-10-14T00:00:00.000Z">October 14, 2024</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/rodrigo-lourenco-lopes.png" alt="Rodrigo Lopes"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Rodrigo Lopes</span></a></div><small class="authorTitle_nd0D" title="Associate Software Engineer @ Zeebe">Associate Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>Our first goal of this experiment is to use a benchmarks to
derive new optimized cluster configuration that can handle
at least 100 tasks per second, while maintaining low backpressure and low latency.</p>
<p>For our experiment, we use a newly defined realistic benchmark (with a more complex process model). More about this in a separate blog post.</p>
<p>The second goal is to scale out optimized cluster configuration
resources linearly and see if the performance scales accordingly.</p>
<p><strong>TL;DR;</strong></p>
<p>We used a realistic benchmark to derive a new
cluster configuration based on previous requirements.</p>
<p>When we scale this base configuration linearly we see that the performance
increases almost linearly as well, while maintaining low
backpressure and low latency.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="chaos-experiment">Chaos Experiment<a href="#chaos-experiment" class="hash-link" aria-label="Direct link to Chaos Experiment" title="Direct link to Chaos Experiment">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="expected">Expected<a href="#expected" class="hash-link" aria-label="Direct link to Expected" title="Direct link to Expected">​</a></h3>
<p>We expect that we can find a cluster configuration that can handle at 100
tasks second to be significantly reduced in resources in relation to our
smaller clusters (G3-S HA Plan) since these can process significantly above
our initial target.</p>
<p>We also expect that we can scale this base configuration linearly, and that
the processing tasks rate to grow initially a bit faster than linearly due to
the lower relative overhead, and if we keep expanding further to flatten due
to the partition count being a bottleneck.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="actual">Actual<a href="#actual" class="hash-link" aria-label="Direct link to Actual" title="Direct link to Actual">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="minimal-requirements-for-our-cluster">Minimal Requirements for our Cluster<a href="#minimal-requirements-for-our-cluster" class="hash-link" aria-label="Direct link to Minimal Requirements for our Cluster" title="Direct link to Minimal Requirements for our Cluster">​</a></h4>
<p>Based on known customer usage, and our own previous experiments, we
determined that the new cluster would need to create and complete a
baseline of 100 tasks per second, or about 8.6 million tasks per day.</p>
<p>Other metrics that we want to preserve and keep track are the backpressure
to preserve user experience, guarantee that exporting speed can keep up
with the processing speed, write-to-import latency which tells us how long
it takes for a record to be written to being imported by our other
apps such as the operator.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="reverse-engineering-the-cluster-configuration">Reverse Engineering the Cluster Configuration<a href="#reverse-engineering-the-cluster-configuration" class="hash-link" aria-label="Direct link to Reverse Engineering the Cluster Configuration" title="Direct link to Reverse Engineering the Cluster Configuration">​</a></h4>
<p>For our new configurations the only resources that we are going to change
are the ones relevant to the factors described above. These are the
resources allocated to our zeebe-brokers, gateway and elasticSearch.</p>
<p>Our starting point in resources was the configuration for our G3-S HA Plan
as this already had the capability to significantly outperform the current
goal of 100 tasks per second.</p>
<p>The next step was to deploy our realistic benchmark, with a payload of 5
customer disputes per instance and start 7 instances per second, this
generated approximately 120 tasks per second (some buffer was added to guarantee performance).</p>
<p>After this we reduced the resources iteratively until we saw any increase
in backpressure, given that no there was no backlog of records, and no
significant increase in the write to import latency.</p>
<p>The results for our new cluster are specified bellow in the tables, where
our starting cluster configuration is the G3-S HA Plan and the new
configuration cluster is the G3 - BasePackage HA.</p>
<table><thead><tr><th>G3-S HA</th><th>CPU Limit</th><th>Memory Limit in GB</th></tr></thead><tbody><tr><td>operate</td><td>2</td><td>2</td></tr><tr><td>operate.elasticsearch</td><td>6</td><td>6</td></tr><tr><td>optimize</td><td>2</td><td>2</td></tr><tr><td>tasklist</td><td>2</td><td>2</td></tr><tr><td>zeebe.broker</td><td>2.88</td><td>12</td></tr><tr><td>zeebe.gateway</td><td>0.9</td><td>0.8</td></tr><tr><td><strong>TOTAL</strong></td><td><strong>15.78</strong></td><td><strong>24.8</strong></td></tr></tbody></table>
<table><thead><tr><th>G3 - BasePackage HA</th><th>CPU Limit</th><th>Memory Limit in GB</th></tr></thead><tbody><tr><td>operate</td><td>1</td><td>1</td></tr><tr><td>operate.elasticsearch</td><td>3</td><td>4.5</td></tr><tr><td>optimize</td><td>1</td><td>1.6</td></tr><tr><td>tasklist</td><td>1</td><td>1</td></tr><tr><td>zeebe.broker</td><td>1.5</td><td>4.5</td></tr><tr><td>zeebe.gateway</td><td>0.6</td><td>1</td></tr><tr><td><strong>TOTAL</strong></td><td><strong>8.1</strong></td><td><strong>13.6</strong></td></tr></tbody></table>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="reduction-in-resources-for-our-optimized-cluster">Reduction in Resources for our Optimized Cluster<a href="#reduction-in-resources-for-our-optimized-cluster" class="hash-link" aria-label="Direct link to Reduction in Resources for our Optimized Cluster" title="Direct link to Reduction in Resources for our Optimized Cluster">​</a></h5>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:right">CPU Reduction (%)</th><th style="text-align:right">Memory Reduction (%)</th></tr></thead><tbody><tr><td style="text-align:left">zeebe.broker</td><td style="text-align:right">47.92</td><td style="text-align:right">62.5</td></tr><tr><td style="text-align:left">zeebe.gateway</td><td style="text-align:right">33.33</td><td style="text-align:right">-25.0</td></tr><tr><td style="text-align:left">operate.elasticsearch</td><td style="text-align:right">50.00</td><td style="text-align:right">25.0</td></tr></tbody></table>
<p>Total cluster reduction:</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:right">G3-S HA</th><th style="text-align:right">G3 - BasePackage HA</th><th style="text-align:right">Reduction (%)</th></tr></thead><tbody><tr><td style="text-align:left">CPU Limits</td><td style="text-align:right">15.78</td><td style="text-align:right">8.1</td><td style="text-align:right">49</td></tr><tr><td style="text-align:left">Memory Limits</td><td style="text-align:right">24.8</td><td style="text-align:right">13.6</td><td style="text-align:right">45</td></tr></tbody></table>
<p>The process of reducing the hardware requirements was donne initially by
scaling down the resources of the zeebe-broker, gateway and elasticSearch.
The other components were left untouched, as they had no impact in our key
metrics, and were scaled down later in separate experiences to maintain
user experience.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-out-the-cluster">Scaling out the Cluster<a href="#scaling-out-the-cluster" class="hash-link" aria-label="Direct link to Scaling out the Cluster" title="Direct link to Scaling out the Cluster">​</a></h4>
<p>Now for the scaling procedure we intend to see if we can linearly increase
the allocated resources and having a corresponding performance increase,
while keeping the backpressure low, low latency, and user experience.</p>
<p>For this we started with the G3 - BasePackage HA configuration and
incremented the load again until we saw any increase in backpressure,
capture our key metrics and repeated the process for the cluster
configuration resources respectively multiplied by 2x, 3x, and 4x.</p>
<p>This means that the resources allocated for our clusters were:</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:right">Base 1x</th><th style="text-align:right">Base 2x</th><th style="text-align:right">Base 3x</th><th style="text-align:right">Base 4x</th></tr></thead><tbody><tr><td style="text-align:left">CPU Limits</td><td style="text-align:right">8.7</td><td style="text-align:right">17.4</td><td style="text-align:right">26.1</td><td style="text-align:right">34.8</td></tr><tr><td style="text-align:left">Memory Limits</td><td style="text-align:right">14.9</td><td style="text-align:right">29.8</td><td style="text-align:right">44.7</td><td style="text-align:right">59.6</td></tr></tbody></table>
<p>The results in the table bellow show the performance of our several cluster
configurations:</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:right">Base 1x</th><th style="text-align:right">Base 2x</th><th style="text-align:right">Base 3x</th><th style="text-align:right">Base 4x</th></tr></thead><tbody><tr><td style="text-align:left">Process Instances/s</td><td style="text-align:right">7</td><td style="text-align:right">12</td><td style="text-align:right">23</td><td style="text-align:right">27</td></tr><tr><td style="text-align:left">Tasks/s</td><td style="text-align:right">125</td><td style="text-align:right">217</td><td style="text-align:right">414</td><td style="text-align:right">486</td></tr><tr><td style="text-align:left">Average Backpressure</td><td style="text-align:right">2%</td><td style="text-align:right">2%</td><td style="text-align:right">3%</td><td style="text-align:right">6%</td></tr><tr><td style="text-align:left">Write-to-Import Latency</td><td style="text-align:right">90s</td><td style="text-align:right">120s</td><td style="text-align:right">150s</td><td style="text-align:right">390s</td></tr><tr><td style="text-align:left">Write-to-Process Latency</td><td style="text-align:right">140ms</td><td style="text-align:right">89ms</td><td style="text-align:right">200ms</td><td style="text-align:right">160ms</td></tr><tr><td style="text-align:left">Records Processed Rate</td><td style="text-align:right">2500</td><td style="text-align:right">4700</td><td style="text-align:right">7800</td><td style="text-align:right">11400</td></tr><tr><td style="text-align:left">Records Exported Rate</td><td style="text-align:right">2100</td><td style="text-align:right">3900</td><td style="text-align:right">6500</td><td style="text-align:right">9200</td></tr></tbody></table>
<p>This first observations is that the performance scales particularly well by
just adding more resources to the cluster, particularly for a linear
increase of the resources the performance as measured by tasks completed
increases slightly less than linearly (comparing the 1x and 4x task/s we
get 388% the initial rate).</p>
<p>This a very good result as it means that we can scale our system linearly
(at least initially) to handle the expected increase in loads.</p>
<p>Importantly, the backpressure is kept low, and the write-to-import latency
only increases significantly if we leave the cluster running at max rate
for long periods of time. For slightly lower rates the write-to-import
latency is kept in the single digits of seconds or lower tens. This might
imply that a these sustained max rates, the amount records generated starts
to be too much for either ElasticSearch or our web apps that import these
records to handle. Some further investigation could be done here to
investigate the bottleneck.</p>
<p>Another metric also relevant but not shown in this table is the backlog of
records not exported, which kept at almost null through all the experiments
conducted.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-found">Bugs found<a href="#bugs-found" class="hash-link" aria-label="Direct link to Bugs found" title="Direct link to Bugs found">​</a></h3>
<p>During the initial tests, we had several OOM errors in the gateways pods.
After some investigation, we found that this was exclusive to the Camunda 8.
6.0 version, which consumes more memory in the gateway than the previous
versions. This explains why the gateway memory limits were the only
resource that was increased in the new reduced cluster configuration.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency">Improve Operate import latency</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-08-19T00:00:00.000Z">August 19, 2024</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/zelldon.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><a href="/zeebe-chaos/2024/08/16/Operate-load-handling">In our last Chaos Day</a> we experimented with Operate and different load (Zeebe throughput). We observed that a higher load caused a lower import latency in Operate. The conclusion was that it might be related to Zeebe&#x27;s exporting configuration, which is affected by a higher load.</p>
<p>In today&#x27;s chaos day we want to verify how different export and import configurations can affect the importing latency.</p>
<p><strong>TL;DR;</strong> We were able to decrease the import latency by ~35% (from 5.7 to 3.7 seconds), by simply reducing the <code>bulk.delay</code> configuration. This worked on low load and even higher load, without significant issues.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Improve Operate import latency" href="/zeebe-chaos/2024/08/19/Operate-improve-import-latency"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/08/16/Operate-load-handling">Operate load handling</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-08-16T00:00:00.000Z">August 16, 2024</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/zeebe-chaos/authors/zell"><img class="avatar__photo authorImage_XqGP" src="https://github.com/zelldon.png" alt="Christopher Kujawa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/zeebe-chaos/authors/zell"><span class="authorName_yefp">Christopher Kujawa</span></a></div><small class="authorTitle_nd0D" title="Chaos Engineer @ Zeebe">Chaos Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>🎉<!-- --> Happy to announce that we are broadening the scope of our Chaos days, to look holistically at the whole Camunda Platform, starting today.
In the past Chaos days we often had a close look (or concentrated mostly) at Zeebe performance and stability.</p>
<p>Today, we will look at the Operate import performance and how Zeebe processing throughput might affect (or not?) the throughput and latency of the Operate import. Is it decoupled as we thought?</p>
<p>The import time is an important metric, representing the time until data from Zeebe processing is
visible to the User (excluding Elasticsearch&#x27;s indexing). It is measured from when the record is written to the log, by the Zeebe processor, until Operate reads/imports it from Elasticsearch and converts it into its data model. We got much feedback (and experienced this on our own) that
Operate is often lagging behind or is too slow, and of course we want to tackle and investigate this further.</p>
<p>The results from this Chaos day and related benchmarks should allow us to better understand how the current importing
of Operate performs, and what its affects. Likely it will be a series of posts to investigate this further. In general,
the data will give us some guidance and comparable numbers for the future to improve the importing time. See also related GitHub issue <a href="https://github.com/camunda/camunda/issues/16912" target="_blank" rel="noopener noreferrer">#16912</a> which targets to improve such.</p>
<p><strong>TL;DR;</strong> We were not able to show that Zeebe throughput doesn&#x27;t affect Operate importing time. We have seen that Operate can be positively affected by the throughput of Zeebe. Surprisingly, Operate was faster to
import if Zeebe produced more data (with a higher throughput). One explanation of this might be that Operate was then less idle.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Operate load handling" href="/zeebe-chaos/2024/08/16/Operate-load-handling"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting">Using flow control to handle bottleneck on exporting</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-25T00:00:00.000Z">July 25, 2024</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/rodrigo-lourenco-lopes.png" alt="Rodrigo Lopes"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Rodrigo Lopes</span></a></div><small class="authorTitle_nd0D" title="Associate Software Engineer @ Zeebe">Associate Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).
Limiting the write rate is a new feature that can be used to prevent building up an excessive exporting backlog.
There are two ways to limit the write rate, either by setting a static limit or by enabling throttling that dynamically adjust the write rate based on the exporting backlog and rate.
In these experiments, we will test both ways of limiting the write rate and observe the effects on processing and exporting.</p>
<p><strong>TL;DR;</strong>
Both setting a static write rate limit and enabling throttling of the write rate can be used to prevent building up an excessive exporting backlog.
For users, this will be seen as backpressure because processing speed is limited by the rate at which it can write processing results.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Using flow control to handle bottleneck on exporting" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-bottlenecked-exporting"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops">Using flow control to handle uncontrolled process loops</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-25T00:00:00.000Z">July 25, 2024</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/rodrigo-lourenco-lopes.png" alt="Rodrigo Lopes"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/rodrigo-lourenco-lopes" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Rodrigo Lopes</span></a></div><small class="authorTitle_nd0D" title="Associate Software Engineer @ Zeebe">Associate Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>Zeebe 8.6 introduces a new unified flow control mechanism that is able to limit user commands (by default it tries to achieve 200ms response times) and rate limit writes of new records in general (disabled by default).</p>
<p>Limiting the write rate is a new feature that can be used to prevent building up an excessive exporting backlog.</p>
<p>In these experiments we will test what happens with the deployment of endless
loops that result in high processing load, and how we can use the new
flow control to keep the cluster stable.</p>
<p><strong>TL;DR;</strong></p>
<p>Enabling the write rate limiting can help mitigate the effects caused by
process instances that contain uncontrolled loops by preventing building up an
excessive exporting backlog.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Using flow control to handle uncontrolled process loops" href="/zeebe-chaos/2024/07/25/Using-flow-control-to-handle-uncontrolled-process-loops"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2024/01/19/Job-Activation-Latency">Reducing the job activation delay</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-01-19T00:00:00.000Z">January 19, 2024</time> · <!-- -->12 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/npepinpe" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/npepinpe.png" alt="Nicolas Pepin-Perreault"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/npepinpe" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Nicolas Pepin-Perreault</span></a></div><small class="authorTitle_nd0D" title="Senior Software Engineer @ Zeebe">Senior Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>With the addition of end-to-end job streaming capabilities in Zeebe, we wanted to measure the improvements in job activation latency:</p>
<ul>
<li>How much is a single job activation latency reduced?</li>
<li>How much is the activation latency reduced between each task of the same process instance?</li>
<li>How much is the activation latency reduced on large clusters with a high broker and partition count?</li>
</ul>
<p>Additionally, we wanted to guarantee that every component involved in streaming, including clients, would remain resilient in the face of load surges.</p>
<p><strong>TL;DR;</strong> Job activation latency is greatly reduced, with task based workloads seeing up to 50% reduced overall execution latency. Completing a task now immediately triggers pushing out the next one, meaning the latency to activate the next task in a sequence is bounded by how much time it takes to process its completion in Zeebe. Activation latency is unaffected by how many partitions or brokers there in a cluster, as opposed to job polling, thus ensuring scalability of the system. Finally, reuse of gRPC&#x27;s flow control mechanism ensure clients cannot be overloaded even in the face of load surges, without impacting other workloads in the cluster.</p>
<p><a href="https://docs.camunda.io/docs/components/concepts/job-workers/#job-streaming" target="_blank" rel="noopener noreferrer">Head over to the documentation to learn how to start using job push!</a></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Reducing the job activation delay" href="/zeebe-chaos/2024/01/19/Job-Activation-Latency"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2023/12/20/Broker-scaling-performance">Broker Scaling and Performance</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-20T00:00:00.000Z">December 20, 2023</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/lenaschoenburg" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/lenaschoenburg.png" alt="Lena Schönburg"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/lenaschoenburg" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Lena Schönburg</span></a></div><small class="authorTitle_nd0D" title="Senior Software Engineer @ Zeebe">Senior Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/deepthidevaki" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/deepthidevaki.png" alt="Deepthi Akkoorath"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/deepthidevaki" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Deepthi Akkoorath</span></a></div><small class="authorTitle_nd0D" title="Senior Software Engineer @ Zeebe">Senior Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>With Zeebe now supporting the addition and removal of brokers to a running cluster, we wanted to test three things:</p>
<ol>
<li>Is there an impact on processing performance while scaling?</li>
<li>Is scaling resilient to high processing load?</li>
<li>Can scaling up improve processing performance?</li>
</ol>
<p><strong>TL;DR;</strong> Scaling up works even under high load and has low impact on processing performance. After scaling is complete, processing performance improves in both throughput and latency.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/performance">performance</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Broker Scaling and Performance" href="/zeebe-chaos/2023/12/20/Broker-scaling-performance"><b>Read more</b></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss">Dynamic Scaling with Dataloss</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-12-19T00:00:00.000Z">December 19, 2023</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/lenaschoenburg" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/lenaschoenburg.png" alt="Lena Schönburg"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/lenaschoenburg" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Lena Schönburg</span></a></div><small class="authorTitle_nd0D" title="Senior Software Engineer @ Zeebe">Senior Software Engineer @ Zeebe</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>We continue our <a href="/zeebe-chaos/2023/12/18/Dynamically-scaling-brokers">previous experiments</a> with dynamically scaling by now also testing whether
the cluster survives dataloss during the process.</p>
<p>One goal is to verify that we haven&#x27;t accidentally introduced a single point of failure in the cluster.
Another is to ensure that data loss does not corrupt the cluster topology.</p>
<p><strong>TL;DR;</strong>
Even with dataloss, the scaling completes successfully and with the expected results.
We found that during scaling, a single broker of the previous cluster configuration can become a single point of failure by preventing a partition from electing a leader.
This is not exactly a bug, but something that we want to improve.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zeebe-chaos/tags/availability">availability</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Dynamic Scaling with Dataloss" href="/zeebe-chaos/2023/12/19/Dynamic-Scaling-with-Dataloss"><b>Read more</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/zeebe-chaos/page/2"><div class="pagination-nav__label">Older entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/zeebe" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://forum.camunda.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/Camunda" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/zeebe-io/zeebe-chaos/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Zeebe Chaos. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>